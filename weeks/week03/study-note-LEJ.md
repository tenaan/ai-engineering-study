# Chap 3. 평가 방법론

---
평가는 중요성과 복잡성 때문에 이 책에서 chap3, 4 두 장에 걸쳐 다룬다.
이번 장에서는 개방형 모델 평가에 사용되는 다양한 방법과 작동 원리, 한계를 다룬다.

---
# 파운데이션 모델 평가의 어려움
- ML 모델 평가는 항상 어려웠는데, 파운데이션 모델이 등장하면서 더욱 어려워졌다.
- 파운데이션 모델이 전통적인 ML보다 평가하기 어려운 이유
1. AI 모델이 똑똑해질수록 평가가 더 어려워진다.
2. 파운데이션 모델은 개방형 특성으로 인해, 정답을 기준으로 성능을 평가하는 기존 방식은 더 이상 유효하지 않다.
-> 개방형: 하나의 입력에 여러 개의 정답이 존재할 수 있다.
3. 대부분의 파운데이션 모델은 블랙박스로 취급된다.
-> 모델 제공업체가 모델의 세부사항을 공개하지 않거나, 애플리케이션 개발자가 이를 이해할 전문 지식이 부족하다.
4. 범용 모델의 평가 범위가 확장됐다.
-> 범용 모델에서 평가는 알려진 작업의 성능을 평가할 뿐만 아니라, 모델이 수행할 수 있는 새로운 작업을 발견하는 것도 포함된다.

- 이상적으로 평가 벤치마크는 모델의 모든 능력을 다뤄야 하고 AI가 발전하면서 계속 진화해야 한다.
- 평가에 대한 관심이 늘어났음에도 여전히 AI 엔지니어링 파이프라인의 다른 부분에 비해 관심이 뒤처져 있다.
![](https://velog.velcdn.com/images/dkan9634/post/99c45422-c871-4704-927f-95aa6d360613/image.png)
-> 평가 도구는 모델링, 학습, AI 오케스트레이션 도구에 비해 현저히 적은 수준

---
# 언어 모델링 지표 이해하기
- 언어 모델링은 클로드 섀넌이 1951년 논문 \<Prediction and Entropy of Printed English\> 에서 대중화한 이후 수십 년 동안 존재해왔지만, 그 이후로 언어 모델 개발을 이끄는 지표는 크게 변하지 않았다.
- 대부분의 자기회귀 언어 모델은 **교차 엔트로피**나 관련된 지표인 **퍼플렉시티**를 사용해 학습된다.(논문과 모델 보고서에서 문자당 비트(BPC)와 바이트당 비트(BPB)는 교차 엔트로피의 변형된 형태)
- 교차 엔트로피, 퍼플렉시티, 문자당 비트(BPC), 바이트당 비트(BPB) 이 네 가지 지표는 밀접하게 관련되어 있다.
    - 필요한 정보가 있다면 하나의 값으로 나머지 셋을 계산할 수 있다.
    - 언어 모델링 지표
    - 텍스트 뿐만 아니라 다른 종류의 토큰들로 이루어진 시퀀스를 생성하는 모델에서도 사용 가능
- **모델이 더 잘 학습할수록 학습 교차 엔트로피는 더 낮아진다.**
- 언어 모델을 학습 또는 파인튜닝하지 않더라도, 이런 지표를 이해하면 애플리케이션에 어떤 모델이 적합한지 찾아내는 데 도움이 되고 특정 평가나 데이터 중복 제거 기술에도 종종 사용된다.

## 엔트로피
- **토큰이 평균적으로 얼마나 많은 정보를 담고 있는지** 측정
- 엔트로피가 높을수록 각 토큰이 많은 정보를 담고 있고, 토큰을 표현하는 데 더 많은 비트가 필요
- 직관적으로 언어의 엔트로피가 낮다 == 언어의 토큰이 담고 있는 정보가 적다 == 다음에 올 것을 더 쉽게 예측할 수 있다

## 교차 엔트로피
- 데이터셋에서 언어 모델을 학습시킬 때, 우리의 목표는 모델이 학습 데이터의 분포를 배우게 하는 것 즉, 학습 데이터에서 다음에 무엇이 올지 예측하게 만드는 것
- 교차 엔트로피는 **언어 모델이 데이터셋의 내용을 얼마나 예측하기 어려워하는지를 보여주는 지표**
- 학습 데이터에 대한 모델의 교차 엔트로피는 **두 가지 특성**에 따라 달라진다.
	1. 학습 데이터의 예측 가능성(학습 데이터의 엔트로피로 측정)
    2. 언어 모델이 파악한 분포가 학습 데이터의 실제 분포와 얼마나 다른지
- 언어 모델은 학습 데이터에 대한 교차 엔트로피를 최소화하도록 학습된다.
(만약 언어 모델이 학습 데이터를 완벽히 학습하면, **모델의 교차 엔트로피 == 학습 데이터의 엔트로피**)

## 문자당 비트와 바이트당 비트
- **엔트로피, 교차 엔트로피의 단위: 비트**
(언어 모델의 교차 엔트로피가 6비트라면, 각 토큰을 표현하는 데 6비트가 필요하다는 뜻)
- 토큰당 비트 수가 6, 평균적으로 각 토큰이 2개의 문자로 이루어져 있다면, 문자당 비트(BPC) 6/2=3
- BPC의 문제점: 문자 인코딩 방식이 다양하다.(ASCII는 문자당 7비트, UTF-8은 문자당 8~32비트) 그래서 더 **표준화된 지표는 바이트당 비트(BPB) **
- BPB: 원본 학습 데이터의 1바이트를 표현하는 데 필요한 비트 수
- 교차 엔트로피는 언어 모델이 텍스트를 얼마나 효율적으로 압축할 수 있는지 알려준다. (언어 모델의 BPB가 3.43이면 원본 1바이트(8비트)를 3.43비트로 표현할 수 있다는 뜻, 이 언어 모델은 원본 학습 텍스트를 원래 크기의 절반 이하로 압축할 수 있다는 의미)

## 퍼플렉시티(PPL)
- 퍼플렉시티: 엔트로피와 교차 엔트로피의 지수 함수
- ** 언어 모델이 다음 토큰을 얼마나 헷갈려하는지(불확실성)를 나타내는 지표**
- 실제 분포 P에 대한 퍼플렉시티 정의
$$
PPL(P)=2^{H(P)}
$$
$H(P)$: 분포 P의 엔트로피
- 실제 분포 P와 모델 분포 Q의 퍼플렉시티
-> 언어 모델은 실제 분포 P를 모르니까 모델이 학습한 분포 Q로 예측한다.
$$
PPL(P,Q)=2^{H(P,Q)}
$$

> **교차 엔트로피가 모델이 다음 토큰을 예측하기 얼마나 어려운지를 측정한다면, 퍼플렉시티는 다음 토큰을 예측할 때의 불확실성을 측정한다. (불확실성이 높을수록 다음 토큰으로 가능한 선택지가 많다는 뜻)**

## 퍼플렉시티 해석과 활용 사례
- 모델이 텍스트를 더 정확학데 할수록 지표들(교차 엔트로피, 퍼플렉시티, BPC, BPB)의 값은 더 낮아진다.
WHY?
모델이 얼마나 헷갈리는지(불확실성)”를 수치화한 지표이기 때문이다. 
** 모델이 더 정확하게 예측할수록 → 덜 헷갈리니까 → 불확실성이 감소 → 지표 값이 낮아짐**
- 퍼플렉시티를 해석할 때 사용할 수 있는 규칙들
    - 구조화된 데이터일수록 퍼플렉시티가 낮다.
    - 어휘 크기가 클수록 퍼플렉시티가 높다.
    - 컨텍스트 길이가 길수록 퍼플렉시티가 낮다.
- 퍼플렉시티는 특이한 생각을 표현하는 텍스트나 의미 없는 텍스트처럼 예측하기 어려운 텍스트에서 가장 높게 나타난다. 
=> **퍼플렉시티를 통해 비정상적인 텍스트를 탐지하는 데 사용될 수 있다.**
---
# 정확한 평가
정확한 점수를 산출하는 두 가지 평가 방식을 다룬다.
1. 기능적 정확성
2. 참조 데이터의 유사도 측정

분류 같은 폐쇄형 응답이 아닌 임의의 텍스트를 생성하는 **개방형 응답의 평가에 중점**을 둔다.

## 기능적 정확성
- 시스템이 의도한 기능을 제대로 수행하는지 평가하는 것
- 애플리케이션이 의도한 대로 동작하는지를 측정하기 때문에 모든 애플리케이션의 성능을 평가하는 궁극적인 지표

## 참조 데이터 유사도 측정
- 기능적 정확성으로 자동 평가할 수 없는 작업이라면, AI의 출력을 참조 데이터와 비교해 평가하는 건 일반적인 방법이다.
- 참조 데이터의 각 예시는 (입력, 참조 응답) 형식
- 하나의 입력에 여러 개의 참조 응답이 있을 수 있다.
- 이 평가 방식은 참조 데이터가 필요하므로, 참조 데이터를 얼마나 빨리 많이 만들 수 있는지가 곧 성능의 척도이다.
- 두 개방형 텍스트 간의 유사도를 측정하는 방법에는 네 가지가 있다.
1. 비교
2. 정확한 일치
3. 어휘적 유사도 ex) n-gram
4. 의미적 유사도

## 임베딩 소개
- 임베딩을 만들기 위해 특별히 학습된 모델: BERT, CLIP, Sentence Transformers
![](https://velog.velcdn.com/images/dkan9634/post/7adeef0d-2c0c-422a-bd46-7a1aeaf024be/image.png)
- GPT나 라마를 포함한 많은 ML 모델은 보통 입력을 먼저 벡터로 표현해야 하기 때문에 임베딩을 생성하는 단계를 포함한다.
- 임베딩 알고리즘의 목표는 원본 데이터의 본질을 담아내는 임베딩을 만드는 것이다.
이를 어떻게 검증할 수 있을까?
큰 틀에서 보면, 더 비슷한 텍스트의 임베딩이 **코사인 유사도나 관련 지표로 측정했을 때 더 가까우면 좋은 임베딩 알고리즘이라고 본다.**
- 임베딩의 품질은 해당 작업의 유용성을 기준으로도 평가할 수 있다.
(임베딩은 분류, 주제 모델링, 추천 시스템, RAG 등 많은 작업에서 사용된다. MTEB는 여러 작업에서 임베딩 품질을 측정하는 벤치마크의 예시다.)
- 모든 데이터는 임베딩으로 표현될 수 있다.
![](https://velog.velcdn.com/images/dkan9634/post/1aa2f220-fb2c-47aa-a31b-fd31e8f9ea7d/image.png)
- 여러 유형의 데이터를 표현할 수 있는 통합 임베딩 공간은 멀티모달 임베딩 공간이라고 한다.

---
# AI 평가자
AI의 발전으로 어려운 작업들을 자동화할 수 있는데 AI가 평가도 자동화할 수 있을까?
AI를 사용해 AI를 평가하는 접근 방식을 **AI 평가자** 또는 **LLM 평가자**라고 한다.

## AI 평가자를 쓰는 이유
- 사람 평가자에 비해 빠르고, 사용하기 쉽고 비용도 상대적으로 저렴하다.
- 참조 데이터 없이도 작동할 수 있어서 참조 데이터가 없는 실제 서비스 환경에서도 사용할 수 있다.
- AI의 판단을 항상 신뢰할 수는 없지만 많은 사람의 의견을 바탕으로 만들어졌기 때문에, 대중적인 관점에서 판단을 내릴 수 있다.

## AI 평가자 사용법
- AI를 사용해 여러 가지 방식으로 평가할 수 있다.
ex) AI로 응답 자체의 품질 평가 or 응답을 참조 데이터와 비교 or 다른 응답과 비교
![](https://velog.velcdn.com/images/dkan9634/post/761905ac-2d21-43ba-aeba-436829b89e9f/image.png)
-> 위 접근 방식의 간단한 프롬프트 예시

- AI 평가의 기준은 표준화되어 있지 않다.
- AI 평가자를 프롬프트하는 방법은 다른 일반적인 AI 애플리케이션 프롬프트 방법과 유사하다. 일반적으로 평가자 프롬프트는 다음 상황을 명확하게 설명해야 한다.
> 1) **모델이 수행할 작업**(예: 생성된 응답과 질의 간의 관련성 평가)
2) **모델을 평가할 때 따라야 할 기준**(예: ‘주요 초점은 생성된 응답이 기존 응답 ground truth answer 에 따라 주어진 질의를 충분히 해결하는 정보를 포함하는지 판단하는 데 두어야 한다’), 지시가 더 자세할수록 좋다.
3) ** 점수 체계. 다음 중 하나의 방식으로 점수를 매길 수 있다.**
A. 분류: 좋음/나쁨, 관련됨/관련 없음/중립 등
B. 이산적인 숫자 값: 1~5점. 이는 각 클래스를 의미적으로 해석하는 대신 숫자로 구분하는 특수한 분류 방식이다.
C. 연속적인 숫자 값: 0과 1 사이의 값. 유사성 정도를 평가할 때 사용한다.**
- **언어 모델은** 일반적으로 숫자보다 **텍스트를 더 잘 다룬다.** **AI** 평가자는 수치 점수 체계보다 **분류에서 더 잘 작동**하는 것으로 보고됐다.
- 수치 점수 체계에서는 연속보다 이산 점수가 더 잘 작동하고 경험적으로 이산 점수의 범위가 넓을수록 모델의 성능이 더 나빠진다.

![](https://velog.velcdn.com/images/dkan9634/post/5487d5ae-921e-4053-bffb-aa37ff6c693c/image.png)
- **AI 평가자**는 단순히 모델만 있는게 아니라 **모델과 프롬프트를 모두 포함하는 시스템**이다. (모델, 프롬프트 또는 모델의 샘플링 파라미터를 변경하면 다른 평가자가 된다.)

## AI 평가자의 한계
- **비일관성**
    - 프롬프트에 평가 예시를 포함하면 GPT-4의 일관성이 65%에서 77.5%로 향상된다는 것을 증명
    - 하지만 높은 일관성 != 높은 정확도(평가자가 같은 실수를 일관되게 할 수도 있기 때문)
 - **평가 기준의 모호성**
    - AI 평가자의 지표는 표준화되지 않아서 잘못 해석하고 오용하기 쉽다.
    - ![](https://velog.velcdn.com/images/dkan9634/post/eea30bbb-19b8-455d-97de-4ca3c73bdb09/image.png)

    - 모델과 평가자에 사용된 프롬프트를 볼 수 없다면 어떤 AI 평가자도 신뢰하면 안된다.
- **비용과 지연 시간 증가**
    - 사용자에게 응답을 보내기 전에 평가한다면 위험은 줄지만 지연 시간이 늘어나는 트레이드오프가 발생한다.
- **AI 평가자의 편향**
    - 사람 평가자에게 편향이 있듯이 AI 평가자에도 편향이 있다.
    - AI 평가자의 편향을 이해하면 점수를 올바르게 해석하고 이런 편향을 완화하는 데도 도움이 된다.
    - AI 평가자는 **자기 편향(self-bias)** 경향이 있는데, 이는 다른 모델이 생성한 응답보다 자신의 응답을 선호하는 현상이다.
    - 많은 AI 모델이 첫 위치 편향을 보인다. 
    	- 여러 선택지 중 첫 번째 응답을 선호하는 것이다. 
       - 이는 순서를 바꿔가며 같은 테스트를 여러 번 반복하거나 세심하게 작성된 프롬프트를 사용해 완화할 수 있다.
    	- **사람**은 마지막에 본 응답을 선호하는 경향 => **최근성 편향(recency bias)**
    - 일부 AI 평가자는 장황성 편향(verbosity bias) 품질과 관계없이 더 긴 응답을 선호하는 것이다.
    
=> AI 평가자 방식은 이런 한계가 있지만, 많은 장점 덕분에 앞으로도 계속 도입될 것이라 생각한다. 하지만 AI 평가자 외에도 정확한 평가 방법이나 사람의 평가가 함께 필요하다.
    
## 평가자로 활용 가능한 모델
- 평가자는 평가받는 모델보다 더 강력 or 더 약함 or 비슷할 수 있다.
- **약한 평가자라면?** 비용과 지연 시간 측면에서 이득이다.
- **더 강력한 평가자라면?** 더 나은 판단, 더 나은 응답으로 성능향상에 도움을 줄 수 있다. 하지만 느릴 수도 있다.
    - 더 강력한 모델을 평가자로 사용했을 때 생기는 문제점 
    1. 가장 강력한 모델을 평가할 만한 평가자를 찾을 수 없다.
    2. 어떤 모델이 가장 강력한지 판단하기 위한 다른 평가 방법이 필요하다.

평가자의 예시를 살펴보자.
**1. 보상 모델**
- (프롬프트, 응답) 쌍을 입력 받고 주어진 프롬프트에 대해 그 응답이 얼마나 좋은지 점수를 매긴다.
- 수년간 RLHF에서 사용되어 왔다.
- ex) 구글의 캐피(cappy) - 3억 6천만 개의 파라미터를 가진 경량 평가 모델

**2. 참조 기반 평가자**
- 하나 이상의 참조 응답을 기준으로 생성된 응답을 평가한다.
- 유사도 점수나 품질 점수를 출력할 수 있다.
- ex) BLEURT, Prometheus

**3. 선호도 모델**
- (프롬프트, 응답1, 응답2)를 입력으로 받아 주어진 프롬프트에 대해 어느 응답이 더 나은지(사용자가 더 선호하는지) 출력한다.
- 아마 특화된 평가자 중에서 가장 기대되는 방향일 것이다.
- ex) PandaLM, JudgeLM
![](https://velog.velcdn.com/images/dkan9634/post/0c16b6d4-25c4-41af-bc8c-aafd1e53a080/image.png)
-> PandaLM이 어떻게 작동하는 보여주는 예시

---
# 비교 평가를 통해 모델 순위 정하기
- 개별 평가: 각 모델을 독립적으로 평가한 후 점수를 기준으로 순위를 매긴다.
- 비교 평가: 모델들을 서로 비교해 평가하고 비교 결과로 순위를 계산한다.
- 응답의 품질이 주관적일 때는 보통 개별 평가보다 비교 평가가 더 쉽다.
- AI 분야에서 비교 평가는 2021년 앤트로픽이 서로 다른 모델의 순위를 매기는 데 처음 사용했다. (LMSYS의 챗봇 아레나 순위표도 이 방식을 사용한다.)
![](https://velog.velcdn.com/images/dkan9634/post/2a68e7b7-1e3f-45f5-8a30-83e50b19ac64/image.png)

- 사용자에게 비교 의견을 수집할 때 가장 어려운 점은 어떤 질의가 선호도 투표에 적합한지 판단하는 것이다.
- 선호도 기반 투표는 평가자가 해당 분야를 잘 아는 경우에만 의미가 있다.

## 비교 평가의 과제들
- 점수 기반 평가에선 비교 결과를 얻기 위해 벤치마크와 지표 설계가 가장 어려운 부분이고, 점수로 모델의 순위를 매기는 건 쉽다.
- 반면 비교 평가에선 비교 결과를 수집하는 것과 모델 순위를 매기는 것 모두가 까다롭다.

비교 평가의 세 가지 주요 과제를 살펴보자.

**1. 확장성 병목**
- 기본적으로 비교 평가는 데이터가 많이 필요하다. 
- 비교할 모델 쌍의 수는 모델 수의 제곱에 비례해 증가한다.

**2. 표준화와 품질 관리의 부재**
- 비교 결과를 수집하는 한 가지 방법: LMSYS 챗봇 아레나처럼 커뮤니티에 비교를 맡기는 것(누구나 웹사이트에 접속해서 프롬프트를 입력하면 익명의 두 모델에서 나온 두 응답을 받아볼 수 있고, 더 나은 것에 투표할 수 있다. 모델의 이름은 투표가 끝난 후에만 공개된다.)
- 이 방식은 다양한 비교 결과를 얻고 조작도 어렵지 않지만 표준화와 품질 관리를 강제하기 어렵다.
- 인터넷에 접속할 수 있는 사람이라면 누구나 할 수 있기 때문에 어떤 응답이 더 나은지에 대한 기준이 없다.
- **크라우드 소싱 방식**의 비교는 사용자들이 실제 업무 환경이 아닌 곳에서 모델을 평가하게 된다. 실제 사용 환경에 대한 이해가 없다보니 테스트용 프롬프트가 현장에서 모델이 실제로 어떻게 쓰이는지 제대로 반영 못할 수도 있고 대부분의 사용자는 즉흥적으로 떠오르는 프롬프트를 사용할 것이다.
- 표준화를 강제하는 한 가지 방법: 사용자가 미리 정해진 프롬프트만 사용하도록 제한하는 것 하지만 이는 다양한 활용 사례에 대한 모델의 능력을 평가할 수 없게 된다.

**3. 비교에서 절대 성능으로**
- 많은 애플리케이션에서 반드시 최고의 모델이 필요한 것은 아니다. 필요한 수준의 성능을 내는 모델이면 충분하다.
- 비교 평는 어떤 모델이 더 나은지 알려주지만, 모델이 얼마나 좋은지 또는 우리 활용 사례에 충분히 좋은지는 알려주지 않는다.

## 비교 평가의 미래
- 비교 평가의 장점
    - 두 출력을 비교하는 게 각 출력에 구체적인 점수를 매기는 것보다 쉽다.
    - 비교 평가는 우리가 중요하게 여기는 품질인 사람의 선호도를 파악하는 것을 목표로 한다.
    - 비교 평가는 더 새롭고 강력한 모델이 계속 등장해도 포화 상태에 이르지 않는다.
    - 비교 평가는 참조 데이터로 모델을 학습시키는 것 같은 편법을 쓰기 어렵기 때문에 상대적으로 조작하기 어렵다. 이런 이유로 많은 사람이 다른 어떤 공개 순위표보다 공개 비교 순위표의 결과를 더 신뢰한다.
    - 비교 평가는 다른 방법으로는 알 수 없는 모델 간의 성능 차이를 보여줄 수 있다. 이는 오프라인 평가에서는 기존 벤치마크의 부족한 점을 채워주고, 오라인 평가에서는 A/B 테스트를 보완하는 역할을 한다.
---
# 마치며
- AI 모델이 강력해질수록 치명적인 오류가 발생할 가능성도 커져서 평가가 더욱 중요해진다.
- 강력하고 개방적인 모델을 평가하는 것은 쉽지 않기 때문에 많은 팀이 사람이 직접 평가하는 방식을 선택한다.

---
# Chap 4. AI 시스템 평가하기
이번 장은 chap3에서 배운 평가 방식들을 사용해 애플리케이션의 모델을 평가하는 방법을 설명한다.

---

# 평가 기준
애플리케이션을 만들기 위해 시간과 돈을 투자하기 전, 먼저 애플리케이션을 어떻게 평가할지 이해하는 것이 중요하다. 
=> **평가 주도 개발(evaluation-drive development)**
- 소프트웨어 공학에서 코드를 작성하기 전 테스트를 작성하는 방법을 일컫는 테스트 주도 개발에서 영감을 받았다.
- AI 엔지니어링에서 **평가 주도 개발은 개발하기 전에 평가 기준을 정의하는 것**이다.

## 도메인 특화 능력
- 코딩과 관련된 도메인 특화 능력은 보통 정확성 평가로 판단한다.(효율성과 비용도 고려해야 함)
- BIRDSQL 벤치마크는 생성된 쿼리의 정확성과 함꼐, 실제 SQL 쿼리의 실행 시간 비교를 통해 효율성까지 평가한다.
- MMLU 벤치마크의 객관식 문제 예시 
![](https://velog.velcdn.com/images/dkan9634/post/e2c1123b-11da-4a53-bf03-ca65fb1b7332/image.png)
- **객관식 문제(multiple-choice question, MCQ)**는 하나 이상의 정답이 있을 수 있다. 일반적인 지표는 정확도이다.
- 분류는 객관식의 특별한 경우로, 모든 문제에서 같은 선택지를 사용한다. 분류 작업의 지표는 정확도, F1 점수, 정밀도, 재현율이 있다.
- ** 객관식은 좋은 응답과 나쁜 응답을 구별하는 능력(분류)을 테스트**하는데, 이는 좋은 응답을 생성하는 능력과 다르다. **객관식은 지식("모델이 파리가 프랑스의 수도라는 걸 알고 있는가?")과 추론("모델이 비즈니스 비용 표에서 어느 부서가 가장 많이 지출하는지 추론할 수 있는가?")을 평가하는 데 가장 적합하다. ** 요약, 번역, 글 작성 같은 생성능력ㅇ을 평가하는데 적합하지 않다.

## 생성 능력
- 생성형 AI가 이슈가 되기 훨씬 전부트 AI는 개방형 출력을 생성하는 데 사용됐다. 
- 개방형 텍스트 생성을 연구하는 하위 분야를 자연어 생성(natural language generation, NLG)라고 한다. ex) 번역, 요약, 바꿔쓰기
- 당시 생성된 텍스트 품질을 평가하는 데 사용된 지표는 유창성, 일관성이 포함됐다. +) 번역에는 충실성, 요약에는 관련성
- **언어 모델의 생성 능력이 발전하면서** AI가 생성한 텍스트와 사람이 작성한 텍스트와 거의 구분하기 어려워졌다. **유창성과 일관성의 중요성이 낮아진 것이다**. (하지만 이런 평가 지표는 성능이 떨어지는 모델이나 창의적 글쓰기, 저자원 언어를 다루는 애플리케이션에는 여전히 유용)
+) 유창성과 일관성은 AI 평가자(AI 모델에게 텍스트가 얼마나 유창하고 일관성 있는지 물어보는 방식)를 활용하거나 퍼플렉시티를 사용해 평가할 수 있다.

### 사실 일관성
- **사실 일관성(factual inconsistency)**은 치명적인 결과를 초래할 수 있다.
- 명시적 사실에 대해 확인하는 것이 훨씬 쉽다. ex) 백신과 자폐증 사이에 입증된 연관성은 없다.
- 사실 일관성 검증에서 가장 어려운 부분은 주로 무엇이 사실인지 판단하는 것이다. (어떤 출처를 신뢰하느냐에 따라 달라지기 때문)
- 모델 출력의 사실 일관성은 두 가지 방식으로 검증할 수 있다.(명시적으로 제공된 사실 or 공개된 지식)

**1. 국소적 사실 일관성**
- 출력을 컨텍스트에 기반해 평가한다.
- 출력이 주어진 컨텍스트와 일치하면 사실 관계가 맞다고 본다.
- 작업이 제한된 영역에서 중요하다. ex) 요약문이 원문의 내용을 그대로 반영해야 하는 요약, 챗봇 응답이 회사 정책에 부합해야 하는 고객 지원 챗봇 등

**2. 전역적 사실 일관성**
- 출력을 공개된 지식에 기반해 평가한다.
- 일반 챗봇, 사실 확인, 시장 조사 등 광범위한 작업에서 중요하다.
![](https://velog.velcdn.com/images/dkan9634/post/d2054337-d069-4812-b8f6-fd30e42dee86/image.png)


 - 사실 일관성의 직관적인 평가 방법은 AI를 평가자로 활용하는 것이다. 
 - 리우 등의 연구(2023)에서 원본 문서와 관련해 요약의 사실 일관성을 평가하기 위해 사용한 프롬프트의 일부이다.
 ![](https://velog.velcdn.com/images/dkan9634/post/b9cdf746-c0a4-4ea5-88a6-77673dd5bc56/image.png)

- 사실 관계를 평가하기 위한 더 정교한 AI 판단 기법

**1. 자체 검증(self-verification)**
- SelfCheckGPT는 모델이 서로 일치하지 않는 여러 출력을 생성하는 경우, 원래의 출력이 환각일 가능성이 높다는 가정을 따른다.
- 평가할 응답 R이 주어지면, SelfCheckGPT는 N개의 새로운 응답을 생성하고, R이 이러한 N개의 새로운 응답과 얼마나 일치하는지 측정한다.
- 효과가 있지만 응답을 평가하는 데 많은 AI 질의가 필요해 비용이 많이 든다.

**2. 지식 강화 검증(knowledge-augmented verification)**
- 구글 딥마인드가 < Long-Form Factuality in Large Language Models> 라는 논문에서 소개한 검색 증강 사실성 평가기(SAFE)는 검색 엔진 결과를 활용해 응답을 검증한다.
1. AI 모델을 사용해 응답을 개별 문장으로 분리한다.
2. 각 문장이 독립적으로 이해될 수 있도록 수정한다. 예를 들어, “20세기에 개장했다”라는 문장에서 “개장했다”의 주어를 원래 주어로 바꾼다.
3. 각 문장에 대해 구글 검색 API에 보낼 사실 확인 질의를 제안한다.
4. AI를 사용해 문장이 검색 결과와 일치하는지 판단한다.

![](https://velog.velcdn.com/images/dkan9634/post/00323a4c-ff8b-41b7-b0c7-8549b2c245c3/image.png)

- 진술이 주어진 컨텍스트와 일치하는지 확인하는 것은 오랫동안 사용되어 온 NLP 작업인 텍스트 함의(textual entailment)로 표현할 수 있다.
- **텍스트 함의는 두 진술 간의 관계를 파악하는 작업**이다.
-> 전제(컨텍스트)가 주어지면, 가설(출력 or 출력의 일부)이 다음 범주 중 어디에 속하는지 결정한다.
   >함의 : 가설은 전제로부터 추론할 수 있다.
   모순 : 가설은 전제와 모순된다.
    중립 : 전제는 가설을 함의하지도, 모순되지도 않는다.
    
- **사실 일관성 예측에 특화된 평가 모델**도 있다.
- 이런 평가 모델은 (전제, 가설) 쌍을 입력으로 받아 함의, 모순, 중립 같은 미리 정의된 클래스 중 하나를 출력한다. 이렇게 하면 분류 작업으로 만들 수 있다.
ex) **DeBERTa-v3-base-mnli-fever-anli**는 함의를 예측하기 위해 764,000개의 주석이 달린 (가설, 전제) 쌍으로 학습된 1억 8,400만 개의 파라미터를 가진 모델
- 사실 일관성을 위한 벤치마크에는 TruthfulQA(사람이 잘못된 믿음이나 오해로 인해 부정확하게 응답할 수 있는 817개의 질의로 구성)
- 사실 일관성은 검색 증강 생성(RAG) 시스템의 중요한 평가 기준이다.

### 안정성
- 사실 일관성 외에도 모델이 생성하는 결과물이 해로울 수 있는 경우가 많다.
![](https://velog.velcdn.com/images/dkan9634/post/0fb908a1-c2a5-40e0-a2bf-16a891a495a4/image.png)
- 유해성을 측정하는 일반적인 벤치마크로는 `RealToxicityPrompts`(모델이 유해한 결과물을 생성하도록 유도할 가능성이 높은 10만 개의 자연스러운 프롬프트를 포함), `BOLD`가 있다.
- 아래 예시는 모델이 **유해한 결과물을 생성하도록 유도**하는 프롬프트이다.
![](https://velog.velcdn.com/images/dkan9634/post/a0e1cf2e-97d3-4395-af31-4f14f190c90b/image.png)


## 지시 수행 능력
- 지시 수행 능력 측정은 "이 모델이 주어진 지시를 얼마나 잘 따르는가?"라는 질문에서 시작한다.
- 지시를 따르는 능력은 파운데이션 모델의 핵심 요구사항이며, 대부분의 파운데이션 모델은 이를 위해 학습된다.
- ChatGPT의 전신인 InstructGPT는 지시를 따르도록 파인튜닝되었기 때문에 이런 이름이 되었다고 한다.
- 지시 수행 능력은 JSON이나 정규 표현식(regex) 같은 구조화된 출력이 필요한 애플리케이션에서 필수적이다.
- 지시 수행 능력은 도메인별 능력이나 생성 능력과 쉽게 혼동될 수 있어서 정의하거나 측정하기가 쉽지 않다.


### 지시 수행 기준
- 지시를 따르는 모델의 능력을 측정하는 벤치마크는 `IFEval` `INFOBench`가 있다. 이는 모델이 지시를 따르는 능력을 평가하는 방법에 대한 아이디어(어떤 기준, 평가 세트에 어떤 지시 포함, 어떤 평가 방법이 적절)를 제공한다.
![](https://velog.velcdn.com/images/dkan9634/post/c3349e90-7c91-45b7-9bce-4957b917a210/image.png)

### 역할 연기
- 모델에게 가상 캐릭터나 페르소나를 가정하도록 요청하는 것
- 두 가지 목적
1. 사용자가 상호작용할 수 있도록 캐릭터를 연기하는 것이다. 이는 게임이나 대화형 스토리텔링 같은 엔터테인먼트를 위한 경우가 많다.
2. 모델의 출력 품질을 향상시키기 위해 프롬프트 엔지니어링 기법으로 역할 연기를 활용하는 것이다.
![](https://velog.velcdn.com/images/dkan9634/post/4802972c-6082-4fbb-93b5-192aadec88c8/image.png)
-> 특히 게임 내 AI 기반 NPC, AI 동반자, 글쓰기 도우미에서 역할 연기가 중요하다.
- 역할 연기 능력을 평가하는 벤치마크에는 `RoleLLM`(신중하게 만든 유사도 점수와 AI 평가자를 모두 사용해 모델이 페르소나를 얼마나 잘 모방하는지 평가), `CharacterEval`(사람 평가자를 사용하고 각 역할 연기 측면을 5점 척도로 평가하는 보상 모델을 학습)이 있다.
- 애플리케이션의 AI가 특정 역할을 맡아야 한다면 모델이 캐릭터를 유지하는지도 평가해야 한다. 
    - 역할에 따라 모델의 출력을 평가하는 휴리스틱을 만들 수 있다. ex) 말을 많이 안하는 역할: 모델 출력의 평균 길이를 휴리스틱으로 설정
(그 외에는 AI를 평가자로 사용하는 게 가장 쉬운 자동 평가 방법)

## 비용과 지연 시간
- 파레토 최적화: 여러 목표를 최적화하는 연구
- 지연 시간 지표: 첫 토큰까지 걸리는 시간, 토큰당 시간, 토큰 간 시간, 질의당 시간
- 모델 API를 사용하면 규모가 커져도 토큰당 비용은 크게 변하지 않는다. 하지만 자체 호스팅을 하면 규모가 커질수록 토큰당 비용을 크게 줄일 수 있다.
- 따라서 기업은 규모에 따라 모델 API를 사용할지, 자체 모델을 호스팅할지 결정해야 한다.
- 아래 예시는 모델을 평가할 떄 쓸 수 있는 기준을 보여준다.(규모 항목이 중요한데, 모델 API를 평가할 때는 해당 서비스가 필요한 규모를 지원할 수 있는지 확인해야 하기 때문)
![](https://velog.velcdn.com/images/dkan9634/post/0aa0f42a-1aac-4d20-bde5-d9e4ef465627/image.png)

---
# 모델 선택
- 애플리케이션을 개발하는 동안 여러 모델 조정 기법을 사용하면서 모델을 반복적으로 선택해야 한다. 
    - 프롬프트 엔지니어링은 실현 가능성을 펴가하기 위해 전반적으로 가장 성능이 좋은 모델로 시작하고, 그 다음에 더 작은 모델로도 가능한지 확인한다. 
    - 파인튜닝은 코드를 테스트하기 위해 작은 모델로 시작해서 하드웨어 제약 조건에 맞는 가장 큰 모델로 확장할 수 있다.

각 기법에 대한 선택 과정은 보통 두 단계
1. 달성할 수 있는 최고 성능 파악하기
2. 비용-성능 축에 모델을 배치하고 투자 대비 최고의 성능을 내는 모델 선택하기
-> 하지만 실제 선택 과정은 이보다 훨씬 복잡하다.

## 모델 선택 과정
하드 속성
- 모델 제공업체의 결정(라이선스, 학습 데이터, 모델 크기) 또는 자체 정책(개인 정보 보호, 제어)의 결과인 경우가 많다.

소프트 속성
- 정확도, 유해성, 사실 일관성과 같이 개선할 수 있는 속성

하드 속성과 소프트 속성을 어떻게 정의할지는 모델과 활용 사례에 따라 다르다. 모델을 최적화해서 더 빠르게 실행할 수 있다면 지연 시간은 소프트 속성이지만 다른 사람이 호스팅하는 모델을 사용한다면 이는 하드 속성이 된다.

전반적으로 평가 과정은 네 단계로 구성된다.
1. **하드 속성이 적합하지 않은 모델을 걸러낸다. ** 하드 속성 목록은 자체 내부 정책과 상용 API를 사용할지 자체 모델을 호스팅할지에 따라 크게 달라진다.
2. ** 공개된 정보(예: 벤치마크 성능과 리더보드 순위)를 활용해 실행해 볼 가장 유망한 모델을 추려내되**, 모델 품질, 지연 시간, 비용 등 여러 목표를 **균형 있게 고려**한다.
3. **자체 평가 파이프라인으로 실험을 수행해 최적의 모델**을 찾되, 마찬가지로 모든 목표를 균형 있게 고려한다.
4. 운영 환경에서 모델을 **지속적으로 모니터링**하여 실패를 감지하고 애플리케이션 개선을 위한 피드백을 수집한다.

![](https://velog.velcdn.com/images/dkan9634/post/1e8c9db3-6452-4f4e-8c72-08644c81ed72/image.png)
- 이 네 단계는 순환적이다.(현재 단계에서 얻은 새로운 정보로 이전 단계의 결정을 변경할 수 있다.)

## 모델 자체 개발 VS 상용 모델 구매
- 기술을 활용할 떄 기업들이 늘 마주치는 고민은 직접 개발할지 구매할 것인가다.
- 데이터 공개 여부를 구분하기 위해, 데이터 없이 공개된 모델은 **오픈 웨이트**, 데이터와 함께 공개된 모델은 **오픈 모델**


### 오픈 소스 모델과 모델 API 비교
- 모델을 호스팅하고 사용자의 질의를 받아서, 모델을 실행해 질의에 대한 응답을 생성하고, 이 응답을 사용자에게 반환하는 서비스를 추론 서비스라고 한다.
- 사용자가 상호작용하는 인터페이스를 모델 API라고 한다.
![](https://velog.velcdn.com/images/dkan9634/post/7d3627e9-474c-4d1d-b961-60541a8622df/image.png)
- 상용 모델은 모델 개발사가 라이선스에 따라 제공하는 독점 API를 통해서만 접근할 수 있다.
- 반면, 오픈 소스 모델은 여러 API 제공업체에서 지원하므로 사용자는 가장 적합한 업체를 선택할 수 있다.
- 큰 모델을 위한 확장 가능한 추론 서비스를 개발하는 것이 쉽지 않기 때문에, 많은 기업이 직접 개발하기를 원하지 않는다. 이로 인해 오픈 소스 모델 위에서 동작하는 많은 서드파티 추론 및 파인튜닝 서비스가 생겨났다.


**데이터 프라이버시**
- 엄격한 데이터 프라이버시 정책으로 조직 외부에 데이터를 전송할 수 없는 기업은 외부 호스팅 모델 API 사용이 불가능하다.

**데이터 계보와 저작권**
- 데이터 계보(data lineage)와 저작권 문제는 기업을 오픈 소스 모델로 가거나, 독점 모델을 선택하거나 아니면 둘 다 피하는 방향으로 이끌 수 있다.
- AI 응용 프로그램의 특허 가능성은 **혁신에 대한 사람의 기여도가 특허를 받기에 충분한지**에 달려 있다.
- 게임과 영화 스튜디오처럼 지적 재산권에 생존이 달린 많은 기업은 AI 관련 지적재산권법이 명확해질 때까지는 제품 제작에 AI를 사용하길 꺼린다.
- 많은 기업은 상용 모델을 선택한다. 오픈 소스 모델은 상용 모델에 비해 법적 자원이 제한적이다. **상용 모델을 사용하면 모델 제공 업체와 맺은 계약으로 데이터 계보 위험을 어느 정도 막을 수 있다.**

**성능**
- 현재 다양한 벤치마크에서 오픈 소스 모델과 독점 모델 간의 격차가 많이 좁혀진 것을 알 수 있다.
![](https://velog.velcdn.com/images/dkan9634/post/4ce4dae8-5d7d-409b-9d81-6dec0c8e7d0a/image.png)
-> 이러한 추세를 보며 오픈 소스 모델이 결국엔 가장 강력한 독점 모델을 따라잡거나, 어쩌면 넘어설 수도 있다.
- 오픈 소스 모델이 뒤처질 수 있는 이유 중 하나는 오픈 소스 개발자들이 상용 모델만큼 사용자 피드백을 받아 모델을 개선할 수 없다는 점이다. 모델이 오픈 소스로 공개되면 개발자들이 어떻게 그 모델을 사용하고 있고, 실제로 어떤 작업에서 잘 작동하는지 파악할 수 없다.

**기능**
- 특정 화용 사례에 맞게 모델을 작동시키려면 여러 기능이 필요하다.
    - **확장성**: 추론 서비스가 원하는 지연 시간과 비용을 유지하며 애플리케이션 트래픽을 감당하도록 하는 것
    - **함수 호출**: 6장에서 다룰 RAG와 에이전트 활용 사례에 필요한 외부 도구를 사용할 수 있는 모델의 능력
    - **출력 구조**: JSON 형식으로 출력을 생성하도록 모델에 요청하는 것과 같은 구조화된 출력
    - **출력 가드레일**: 응답이 인종차별적이거나 성차별적이지 않도록 하는 등 생성된 응답의 위험을 줄이는 것
- 이런 기능들은 실제로 구현하기 어렵고 시간이 많이 걸려서 많은 기업은 이런 기능을 바로 사용할 수 있는 모델 API를 사용한다.
- 하지만 API가 제공하는 기능만 사용할 수 있다는 게 단점이다.

**제어, 접근성, 투명성**
- 기업들이 오픈 소스 모델에 관심을 갖는 두 가지 주요 이유: 제어, 커스터마이징 가능성
![](https://velog.velcdn.com/images/dkan9634/post/a0ea0239-4728-4eb8-96aa-29c1d2037451/image.png)

**온디바이스 배포**
- 디바이스 자체에서 직접 모델을 실행하고 싶다면 서드파티 API는 사용할 수 없다.
- 실제로 모델을 로컬에서 실행하는 게 필요한 경우가 많다. ex) 인터넷이 불안정한 지역을 위한 서비스
- 다음은 모델 API 사용과 자체 호스팅의 장단점을 요약한 것
![](https://velog.velcdn.com/images/dkan9634/post/13a908eb-2302-4f6d-b2c7-0af36501a5ce/image.png)

## 공개 벤치마크 탐색하기
- 구글의 BIG-Bench(2022)는 214개의 벤치마크를 가지고 있다.
- AI 모델이 개선됨에 따라 기존 벤치마크는 포화 상태가 되어 새로운 벤치마크의 도입이 필요하게 된다.
- 여러 벤치마크에서 모델을 평가하는 데 도움이 되는 도구는** 평가 하네스(evaluation harness)**이다.
    - 여러 테스트(벤치마크, 질문, 데이터셋)를 한 번에 돌려서 모델 성능을 측정하고 비교할 수 있게 해주는 자동화 시스템

### 벤치마크 선택 및 집계 ###
- 벤치마크 결과는 모델을 찾는 데 도움이 되고 집계해서 모델의 순위를 매기면 리더보드가 만들어진다. 리더보드를 만들 떄 다음 두 가지 질문을 고려해야 한다.
1. 리더 보드에 어떤 벤치마크를 포함할 것인가?
2. 모델 순위를 매기기 위해 이런 벤치마크 결과를 어떻게 집계할 것인가?
-  공개 벤치마크에서 자신만의 리더보드를 만드는 방법에 대한 영감을 얻으려면, 기존 공개 리더보드가 어떻게 만들어졌는지 살펴보는 것이 유용하다.

### 공개 리더보드
- 일부 벤치마크의 **종합 성능을 기반으로 모델 순위**를 매긴다.
- 컴퓨팅 제약으로 소수의 벤치마크만 포함한다.
- 2023년 후반에 허깅 페이스는 오픈 LLM 리더보드를 업데이트해서 다음 **6개 벤치마크의 평균**으로 모델 순위를 매겼다.
1. **ARC-C**(Clark et al., 2018): 복잡한 초등학교 수준의 과학 문제를 풀 수 있는 능력을 측정
2. **MMLU**(Hendrycks et al., 2020): 초중 수학, 미국 역사, 컴퓨터 과학, 법학 등 57개 과목에서 지식과 추론 능력을 측정
3. **HellaSwag**(Zellers et al., 2019): 이야기나 영상에서 문장이나 장면의 완성을 예측하는 능력을 측정하며 상식과 일상적인 활동에 대한 이해를 테스트하는 것이 목표
4. **TruthfulQA**(Lin et al., 2021): 정확할 뿐만 아니라 진실하고 오해의 소지가 없는 응답을 생성하는 능력을 측정하며, 모델의 사실 이해에 초점
5. **WinoGrande**(Sakaguchi et al., 2019): 언어 모델에게 어렵게 설계된, 정교한 상식적 추론이 필요한 대명사 해결 문제를 풀 수 있는 능력을 측정
6. **GSM-8K**(오픈 AI, 2021): 초등학교 교과과정에서 일반적으로 접하는 다양한 수학 문제를 풀 수 있는 능력을 측정
=> 허깅페이스는 ** 다양한 분야에 걸쳐 추론과 일반 지식을 테스트하기 때문에** 이런 벤치마크를 선택

- 같은 시기에 스탠퍼드의 HLEM 리더보드는 10개의 벤치마크를 사용했는데, 이 중 허깅페이스와 겹치는 건 MMLU와 GSM-8K 뿐이었다.

- 리더보드 개발자들도 자신들의 벤치마크 선정 기준을 명확히 설명하지 못하는데, 이는 그만큼 설명하기 어려운 작업이기 때문일 것이다.
![](https://velog.velcdn.com/images/dkan9634/post/9c19ed8f-65b6-4d9a-9213-5fdb34ee2b8a/image.png)

- 허깅페이스는 모델의 최종 순위를 매기기 위해 모든 벤치마크의 점수를 평균 낸다. 즉, 모든 벤치마크 점수를 동등하게 취급한다는 뜻이다.
- HLEM 연구자들은 평균을 내지 않고 평균 승률을 사용한다. **한 모델이 다른 모델보다 더 좋은 점수를 얻는 비율을 시나리오별로 평균 낸 것**이다.

### 공개 벤치마크의 데이터 오염
- 데이터 오염(data contamination)은 매우 흔해서 데이터 유출, 테스트 세트로 학습하기, 부정행위 등 여러 이름으로 불린다.
- 모델이 테스트 데이터로 학습했을 때 발생한다.

### **데이터 오염이 일어나는 방식**
- 오늘날 많은 모델이 인터넷에서 수집한 데이터로 학습되는데, 이 과정에서 공개된 벤치마크의 데이터가 **실수로 포함**될 수 있다.
- train, test 데이터가 모두 **동일한 출처**에서 오는 경우와 같이 간접적으로 발생할 수도 있다.

반면에 데이터 오염이 **좋은 의도로 발생하는 경우**도 있다.
- 처음엔 모델의 학습 데이터에서 벤치마크 데이터를 제외하고 이런 벤치마크를 기반으로 최고의 모델을 선택한다. 그러나 고품질의 벤치마크 데이터가 모델의 성능을 향상시킬 수 있기 때문에, **사용자가 공개하기 전에 벤치마크 데이터로 최고의 모델을 계속 학습**시킨다.

### 데이터 오염 다루기
다음 방법과 같은 휴리스틱 방법으로 오염을 감지할 수 있다.
1. **n-gram 중복**
- test 샘플에 13개 토큰으로 이루어진 특정 시퀀스가 train 데이터에도 있으면 오염일 수도
- 정확도가 높지만 시간, 비용이 많이 소모될 수 있다.

2. **퍼플렉시티**
- 모델이 주어진 텍스트를 얼마나 예측하기 어려워하는지를 측정하는 지표
- 모델이 평가 데이터에 대해 퍼플렉시티가 유난히 낮다면, 즉 쉽게 예측한다면 오염일 수도 
- 정확도는 떨어지지만, 훨씬 적은 자원으로 가능하다.

---
# 평가 파이프라인 설계하기
AI 애플리케이션의 성공 여부는 종종 좋은 결과, 나쁜 결과를 구분하는 능력에 달려있다. 이를 위해선 신뢰할 수 있는 **평가 파이프라인**이 필요하다.

## 1단계: 시스템의 모든 구성 요소 평가하기
- 엔드투엔드 출력과 각 구성 요소의 중간 출력을 독립적으로 평가해야 한다.
- 각 구성 요소를 독립적으로 평가하지 않으면 시스템이 정확히 어디서 실패하는지 알 수 없다.
- 가능하다면, 애플리케이션을 턴별로 그리고 작업별로 모두 평가해야 한다.
    - 하나의 턴은 여러 단계와 메시지로 구성될 수 있다. 
    - 시스템이 결과물을 생성하기 위해 여러 단계를 거치더라도 이는 여전히 하나의 턴으로 간주된다.
- **턴 기반 평가** : 각 출력물의 품질을 평가한다. 
- **작업 기반 평가** : 시스템이 작업을 완료했는지를 평가한다.

## 2단계: 평가 가이드라인 만들기
- 가장 중요한 단계이다.
- 평가 가이드라인을 만들 떄, 애플리케이션이 해야 할 일뿐만 아니라 하면 안 되는 일도 정의하는 것이 중요하다.

### 평가 기준 정의하기
- 평가에서 가장 어려운 부분은 출력이 좋은지 여부 판단하는 게 아니라 **오히려 '좋다'는 것이 무엇을 의미하는지 정하는 것**이다.
- 정확한 응답이 항상 좋은 응답은 아니다.

### 예시와 함께 평가 기준표 만들기
- 각 기준에 대해 평가 시스템을 선택해야 한다.(이진법, 1~5, 0~1 등)
- 점수가 1인 응답은 어떤 모습이며 왜 1점을 받을 자격이 있는가? 기준표를 여러 사람과 함꼐 검증해야 한다. 이 과정은 많은 논의가 필요할 수 있지만, 필수적이다.

### 평가 지표를 비즈니스 지표와 연결하기
- 비즈니스 내에서 애플리케이션은 비즈니스 목표를 달성해야 한다.
- 따라서 애플리케이션의 지표는 해결하고자 하는 비즈니스 문제의 컨텍스트에서 고려되어야 한다.
- 평가 지표가 비즈니스 지표에 미치는 영향을 이해하는 것은 계획 수립에 도움이 된다. 특정 지표를 개선해서 얻을 수 있는 이득이 얼마인지 알면, 그 지표를 개선하는 데 자원을 투자할 때 더 확신을 가질 수 있다.
- 유용성 임계값을 결정하는 데도 도움이 된다.

## 3단계: 평가 방법과 데이터 정의하기
이제 애플리케이션을 평가하는 데 사용할 방법과 데이터를 정의해보자.
### 평가 방법 선택하기
- 서로 다른 기준에는 서로 다른 평가 방법이 필요하다.
	- 
    - 유해성 감지 - 작고 특화된 유해성 분류기
    - 응답과 사용자의 원래 질의 간의 관련성 측정 - 의미적 유사도
    - 응답과 전체 컨텍스트 간의 사실 일관성을 측정 - AI 평가자
- 동일한 기준에 대해 여러 평가 방법을 혼합해 사용할 수도 있다.
- 로그프롭을 사용할 수 있다면 사용하는 게 좋은데, 모델이 생성된 토큰에 대해 얼마나 확신하는지 측정하는 데 사용할 수 있다. 분류에 특히 유용하다.
ex) 세 클래스에 대한 모델의 로그프롭이 모두 30~40% 사이라면 이는 모델이 이 예측에 확신이 없다는 뜻이다. 한 클래스에 대한 모델의 확률이 95%라면 매우 확신한다는 뜻
- 로그프롭은 생성한 텍스트에 대한 모델의 퍼플렉시티를 평가할 때 사용될 수 있다.
>가능한 한 자동 지표를 사용하되, 운영 환경에서도 사람 평가에 의존하는 것을 두려워하지 말자. 사람 전문가가 수도으로 모델의 품질을 평가하는 건 AI에서 오랫동안 이어져 온 관행이다.

### 평가 데이터 주석 달기
- 애플리케이션을 평가하기 위한 주석이 달린 예시 세트를 선별하자.
- 턴 기반 평가, 작업 기반 평가, 둘 다 시스템의 각 구성 요소와 각 기준을 평가하기 위해 주석이 달린 데이터가 필요하다.
- 가능하다면 실제 운영 환경의 데이터를 사용하자.
- 이 단계의 성공 여부는 **평가 기준표의 명확성**에 달려 있다. 
- 시스템에 대한 더 세분화된 이해를 얻기 위해 **데이터를 슬라이스 **할 수 있다. => 데이터를 하위 집합으로 나누고 각 하위 집합에 대한 시스템 성능을 개별적으로 살펴보는 것을 의미한다.
- 좋은 평가를 위해서는 다양한 데이터 슬라이스를 나타내는 여러 평가 세트를 갖춰야 한다.
- 데이터는 등급, 트래픽 출처, 사용량 등을 기준으로 슬라이스할 수 있다.
> 1) 모델 성능을 정확하게 평가하려면 다양한 데이터 슬라이스를 반영한 평가 세트가 필요하다.
2) **평가 안정성을 위해 부트스트랩으로 점수 일관성을 확인**하고, 차이를 검증할 때는 신뢰도에 따른 표본 크기를 계산한다.
3) 작은 개선도 정확하게 잡고 싶다면 더 큰 평가 세트가 필요하다.

### 평가 파이프라인 평가하기
- **평가 파이프라인을 평가하면 파이프라인의 신뢰성을 향상시키고 평가 파이프라인을 더 효율적으로 만드는 방법을 찾는 데 도움이 될 수 있다.**
- 신뢰성은 AI 평가자가 같은 주관적인 평가 방법에서 특히 중요하다.
- 다음은 평가 파이프라인의 품질에 대해 물어봐야 할 몇 가지 질문이다.
    - 평가 파이프라인이 올바른 신호를 제공하고 있는가?
    - 평가 파이프라인은 얼마나 신뢰할 수 있는가?
    - 지표 간 상관관계는 어떠한가?
    - 평가 파이프라인이 애플리케이션에 얼마나 많은 비용과 지연 시간을 추가하는가?

### 반복
- 요구사항과 사용자 행동이 변화하면서, 평가 기준도 진화할 것이고 평가 파이프라인을 반복적으로 개선해야 할 것이다.
- 평가 파이프라인을 반복적으로 개선하면서, 적절한 실험 추적을 수행해야 한다. 평가 데이터, 기준표, AI 평가자에 사용된 프롬프트 및 샘플링 구성을 포함해 평가 프로세스에서 변경될 수 있는 모든 변수를 기록하자.

---
[chap3 정리](https://velog.io/@dkan9634/AI-Engineering-Chap-3.-%ED%8F%89%EA%B0%80-%EB%B0%A9%EB%B2%95%EB%A1%A0)


[chap4 정리](https://velog.io/@dkan9634/AI-Engineering-Chap-4.-AI-%EC%8B%9C%EC%8A%A4%ED%85%9C-%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0)
