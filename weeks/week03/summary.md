# 통합 정리

**정리자**: 김성언
**날짜**: 2025년 12월 4일

---

# 3장. 평가 방법론

## 개요

AI 애플리케이션을 개발하는 데 있어 가장 큰 난관은 **평가(Evaluation)**이다. 평가의 목적은 **위험을 줄이고 기회를 발견**하는 것이다.

- 평가 시에는 시스템의 컨텍스트를 고려해야 하며, 시스템의 취약점을 파악하고 그에 맞춰 평가 설계를 진행해야 한다.
- 체계적인 평가에 투자하지 않고 입소문이나 대충 결과를 확인하는 접근 방식은 위험을 증폭시키고 개발 속도를 늦춘다.

---

## 3.1 파운데이션 모델 평가의 어려움

파운데이션 모델이 전통적인 ML 모델보다 평가하기 어려운 이유:

### 1. AI 모델이 똑똑해질수록 평가가 더 어려워진다
- AI는 그럴듯한 대답을 잘 내놓는다.
- 사실 확인, 추론, 전문 지식까지 활용해서 정보가 맞는지 확인해야 한다.

### 2. 개방형 특성
- 파운데이션 모델은 개방형 특성으로 인해, 정답을 기준으로 성능을 평가하는 기존 방식이 더 이상 유효하지 않다.
- 하나의 입력에 대해 여러 개의 정답이 있을 수 있어 완벽한 정답 목록을 만드는 것이 불가능하다.

### 3. 블랙박스 특성
- 대부분의 파운데이션 모델은 블랙박스로 취급된다.
- 모델 제공업체가 모델의 세부 사항(아키텍처, 학습 데이터, 학습 과정)을 공개하지 않거나, 개발자가 이해할 전문 지식이 부족하다.
- 출력 결과만 보고 평가를 진행해야 한다.

### 4. 벤치마크의 한계
- 파운데이션 모델의 성능이 빠르게 향상되면서 기존 벤치마크들(GLUE, SuperGLUE, MMLU 등)의 최고 점수에 도달하게 되었다.
- AI의 발전에 맞춰 벤치마크도 진화해야 한다. (예: MMLU → MMLU-Pro로 대체)

### 5. 범용 모델의 평가 범위 확장
- 범용 모델 평가는 알려진 작업의 성능뿐만 아니라 모델이 수행할 수 있는 새로운 작업을 발견하는 것도 포함된다.
- 사람의 능력을 넘어서는 작업도 포함될 수 있다.

> **현황**: 평가에 대한 관심이 늘어났음에도 AI 엔지니어링 파이프라인의 다른 부분에 비해 여전히 뒤처져 있다. 불충분한 투자는 불충분한 인프라로 이어지며 체계적인 평가를 수행하기 어렵게 만든다.

---

## 3.2 언어 모델링 지표 이해하기

파운데이션 모델은 언어 모델에서 발전했고 여전히 핵심 구성 요소로 활용되므로, 언어 모델링 지표를 이해하면 애플리케이션 성능을 이해하는 데 도움이 된다.

대부분의 자기회귀 언어 모델은 **교차 엔트로피**나 **퍼플렉시티**를 사용해 학습된다.

> **핵심 지표**: 교차 엔트로피, 퍼플렉시티, 문자당 비트(BPC), 바이트당 비트(BPB)
> - 이 4가지 지표는 밀접하게 관련되어 있어, 필요한 정보가 있다면 하나의 값으로 나머지 셋을 계산할 수 있다.

### 3.2.1 엔트로피 (Entropy)

**엔트로피**는 토큰이 평균적으로 얼마나 많은 정보를 담고 있는지 측정한다.

$$H(P) = -\sum_x P(x)\log P(x)$$

- 엔트로피가 높을수록 각 토큰이 더 많은 정보를 담고 있으며, 토큰을 표현하는 데 더 많은 비트가 필요하다.
- **직관적 해석**: 엔트로피가 낮다는 것은 다음에 올 것을 더 쉽게 예측할 수 있다는 뜻이다.

### 3.2.2 교차 엔트로피 (Cross Entropy)

**교차 엔트로피**는 언어 모델이 데이터셋의 내용을 얼마나 예측하기 어려워하는지를 보여주는 지표이다.

$$H(P, Q) = H(P) + D_{KL}(P \| Q)$$

- $P$: 학습 데이터의 실제 분포
- $Q$: 언어 모델이 학습한 분포
- $D_{KL}(P \| Q)$: 쿨백-라이블러(KL) 발산

**교차 엔트로피를 결정하는 두 가지 특성:**
1. 학습 데이터의 예측 가능성 (학습 데이터의 엔트로피로 측정)
2. 언어 모델이 파악한 분포가 학습 데이터의 실제 분포와 얼마나 다른지

- 언어 모델은 학습 데이터에 대한 교차 엔트로피를 최소화하도록 학습된다.
- 모델이 완벽하게 학습하면 **모델의 교차 엔트로피 = 학습 데이터의 엔트로피** (이때 KL 발산 = 0)
- 교차 엔트로피는 **비대칭적**이다: $H(P, Q) \neq H(Q, P)$

### 3.2.3 문자당 비트(BPC)와 바이트당 비트(BPB)

엔트로피와 교차 엔트로피의 단위는 **비트(bit)**이다.

- **토큰당 비트**: 교차 엔트로피가 6비트 → 각 토큰을 표현하는 데 6비트가 필요
- **BPC (Bits Per Character)**: 문자당 필요한 비트 수
  - 문제점: 문자 인코딩 방식이 다양함 (ASCII는 문자당 7비트, UTF-8은 8~32비트)
- **BPB (Bits Per Byte)**: 원본 학습 데이터의 1바이트를 표현하는 데 필요한 비트 수 (더 표준화된 지표)

$$\text{BPB} = \frac{\text{Bits Per Token}}{\text{Chars Per Token} \times \text{Bytes Per Char}}$$

> **예시**: BPB가 3.43이면 원본 1바이트(8비트)를 3.43비트로 표현할 수 있다는 뜻으로, 원본 학습 텍스트를 절반 이하로 압축할 수 있다는 의미이다.

### 3.2.4 퍼플렉시티 (Perplexity, PPL)

**퍼플렉시티**는 엔트로피와 교차 엔트로피의 지수 함수이다.

$$PPL(P) = 2^{H(P)}$$
$$PPL(P, Q) = 2^{H(P, Q)}$$

- 퍼플렉시티는 **다음 토큰을 예측할 때의 불확실성**을 측정한다.
- 불확실성이 높을수록 다음 토큰으로 가능한 선택지가 많다.

> **nat 단위**: 텐서플로우, 파이토치 등에서는 자연로그를 사용하여 $PPL(P, Q) = e^{H(P, Q)}$로 계산

### 3.2.5 퍼플렉시티 해석과 활용 사례

**퍼플렉시티 해석 규칙:**
1. **구조화된 데이터일수록 퍼플렉시티가 낮다** (예: HTML 코드 < 일상 텍스트)
2. **어휘 크기가 클수록 퍼플렉시티가 높다** (선택지가 많을수록 예측이 어려움)
3. **컨텍스트 길이가 길수록 퍼플렉시티가 낮다** (더 많은 정보로 불확실성 감소)

**활용 사례:**
- **학습 데이터 탐지**: 모델이 본 적 있고 기억한 텍스트에서는 퍼플렉시티가 가장 낮음
- **데이터 오염 탐지**: 특정 벤치마크에 대해 낮은 퍼플렉시티 → 학습 데이터에 포함되었을 가능성
- **비정상 텍스트 탐지**: 특이한 생각이나 의미 없는 텍스트에서 퍼플렉시티가 높음
- **데이터 중복 제거**

> **주의**: 퍼플렉시티는 SFT, RLHF 같은 기법으로 사후 학습된 모델을 평가하는 데는 적절한 지표가 아닐 수 있다. 특정 작업을 잘 수행하도록 가르치면 다음 토큰 예측 능력이 떨어질 수 있어, 보통 사후 학습 후에 퍼플렉시티가 높아진다.

---

## 3.3 정확한 평가

폐쇄형 응답이 아닌 **개방형 응답**의 평가에 중점을 둔 두 가지 평가 방식:

### 3.3.1 기능적 정확성 (Functional Correctness)

시스템이 의도한 기능을 제대로 수행하는지 평가하는 것으로, 모든 애플리케이션 성능을 평가하는 궁극적인 지표이다.

**자동화 가능한 작업 예시:**
1. **코드 생성**: 단위 테스트로 검증 (HumanEval, MBPP, BIRD-SQL, WikiSQL 등)
2. **게임 봇**: 승리/점수로 측정
3. **측정 가능한 목표가 있는 작업**: (예: 에너지 소비 최적화)

> **한계**: 복잡한 작업에서 AI가 일부분만 수행할 때 전체 결과보다 일부분을 평가하는 것이 더 어려울 수 있다.

### 3.3.2 참조 데이터를 사용하는 측정

기능적 정확성으로 자동 평가할 수 없다면 AI의 출력을 참조 데이터와 비교해 평가한다.

**참조 데이터 형식**: (입력, 참조 응답) - 하나의 입력에 여러 개의 참조 응답이 있을 수 있다.

**유사도 측정 방법 4가지:**

| 방법 | 설명 | 특징 |
|------|------|------|
| **비교** | 평가자에게 두 텍스트가 같은지 판단 요청 | 사람/AI 평가자 활용 |
| **정확한 일치** | 생성된 응답이 참조 응답 중 하나와 정확히 일치하는지 | 짧고 정확한 답변에 적합 (퀴즈, 수학) |
| **어휘적 유사도** | 생성된 응답이 참조 응답과 얼마나 비슷해 보이는지 | 의미는 측정하지 않음 |
| **의미적 유사도** | 생성된 응답이 의미에서 참조 응답과 얼마나 가까운지 | 임베딩 기반 |

**어휘적 유사도 측정 방법:**
- **근사 문자열 매칭 (퍼지 매칭)**: 편집 거리 (삭제, 삽입, 대체 연산)
- **n-gram 유사도**: 연속된 시퀀스의 겹침 기준
- **대표 지표**: BLEU, ROUGE, METEOR++, TER, CIDEr
- **관련 벤치마크**: WMT, COCO Captions, GEMv2

> **단점**: 포괄적인 참조 응답 세트를 만들어야 하며, 비슷한 응답이 참조 세트에 없으면 좋은 응답도 낮은 점수를 받는다.

### 3.3.3 임베딩 소개

**임베딩(Embedding)**은 원본 데이터의 의미를 담으려는 숫자 표현(벡터)이다.

- 일반적으로 임베딩 벡터 크기는 100~10,000 사이
- 임베딩 생성 전용 모델: BERT, CLIP, Sentence Transformers

**임베딩 품질 검증:**
- 비슷한 텍스트의 임베딩이 **코사인 유사도**로 측정했을 때 더 가까우면 좋은 임베딩
- 해당 작업(분류, 추천, RAG 등)의 유용성 기준으로 평가
- **벤치마크**: MTEB (Massive Text Embedding Benchmark)

**의미적 유사도 계산:**

$$\text{코사인 유사도} = \frac{A \cdot B}{\|A\| \|B\|}$$

- $A$: 생성된 응답의 임베딩
- $B$: 참조 응답의 임베딩
- 유사도 점수: -1 ~ 1 (1에 가까울수록 유사)

**멀티모달 임베딩:**
- **CLIP**: 이미지와 텍스트를 하나의 통합 임베딩 공간으로 매핑
- **ImageBind**: 텍스트, 이미지, 오디오 등 6가지 유형의 데이터 통합 임베딩
- **ULIP**: 텍스트, 이미지, 포인트 클라우드의 통합 표현

---

## 3.4 AI 평가자 (AI as a Judge)

AI를 사용해 AI를 평가하는 접근 방식 (LLM 평가자라고도 함)

### 3.4.1 AI 평가자를 쓰는 이유

- 사람 평가자에 비해 **빠르고, 사용하기 쉽고, 비용이 저렴**
- **참조 데이터 없이도 작동** 가능 → 실제 서비스 환경에서도 사용 가능
- 대중적인 관점에서 판단 가능 (많은 사람의 의견을 바탕으로 학습)
- 사람 평가자와 **높은 상관관계** (연구에 따르면 85% 일치도, 0.98 상관관계)
- 자신의 **결정을 설명** 가능

### 3.4.2 AI 평가자 사용법

**평가 방식:**
- 응답 자체의 품질 평가
- 응답을 참조 데이터와 비교
- 다른 응답과 비교

**평가자 프롬프트에서 명확히 설명해야 할 사항:**

1. **모델이 수행할 작업** (예: 생성된 응답과 질의 간의 관련성 평가)
2. **평가할 때 따라야 할 기준** (지시가 자세할수록 좋음)
3. **점수 체계:**
   - **A. 분류**: 좋음/나쁨, 관련됨/관련 없음/중립 등
   - **B. 이산적 숫자 값**: 1~5점
   - **C. 연속적 숫자 값**: 0과 1 사이의 값

> **팁**: 언어 모델은 숫자보다 텍스트를 더 잘 다루므로, 수치 점수 체계보다 분류에서 더 잘 작동한다. 이산 점수의 범위가 넓을수록 모델 성능이 나빠진다.

> **중요**: AI 평가자는 단순히 모델만 있는 것이 아니라 **모델과 프롬프트를 모두 포함하는 시스템**이다. 모델, 프롬프트, 샘플링 파라미터를 변경하면 다른 평가자가 된다.

### 3.4.3 AI 평가자의 한계

| 한계 | 설명 | 완화 방법 |
|------|------|----------|
| **비일관성** | 같은 입력에도 다른 점수 출력 가능 | 예시 포함, 샘플링 변수 조정 (단, 높은 일관성 ≠ 높은 정확도) |
| **평가 기준의 모호성** | 표준화되지 않아 잘못 해석/오용 위험 | 명확한 기준 정의 필요 |
| **비용과 지연 시간 증가** | 생성 + 평가 = 호출 2배, 비용 2배 | 약한 모델 사용, 표본 검사 |
| **AI 평가자의 편향** | 자기 편향, 첫 위치 편향, 장황성 편향 | 순서 변경 반복, 프롬프트 세심 작성 |

**편향 종류:**
- **자기 편향 (Self-bias)**: 다른 모델이 생성한 응답보다 자신의 응답을 선호
- **첫 위치 편향**: 여러 선택지 중 첫 번째 응답을 선호 (↔ 사람의 최근성 편향)
- **장황성 편향 (Verbosity bias)**: 품질과 관계없이 더 긴 응답을 선호

> **주의**: 모델과 평가자에 사용된 프롬프트를 볼 수 없다면 어떤 AI 평가자도 신뢰할 수 없다.

### 3.4.4 평가자로 활용 가능한 모델

평가자는 평가받는 모델보다 **더 강력하거나, 약하거나, 비슷할 수 있다**.

**강력한 평가자의 문제점:**
1. 가장 강력한 모델을 평가할 평가자를 찾을 수 없음
2. 어떤 모델이 가장 강력한지 판단하기 위한 다른 평가 방법 필요

**특화된 평가자 유형:**

| 유형 | 입력 | 출력 | 설명 |
|------|------|------|------|
| **보상 모델** | (프롬프트, 응답) | 품질 점수 | RLHF에서 사용, 예: 구글의 Cappy |
| **참조 기반 평가자** | (프롬프트, 응답, 참조 응답) | 유사도/품질 점수 | 예: BLEURT, Prometheus |
| **선호도 모델** | (프롬프트, 응답1, 응답2) | 어느 응답이 나은지 | 예: PandaLM, JudgeLM |

---

## 3.5 비교 평가를 통해 모델 순위 정하기

- **개별 평가**: 각 모델을 독립적으로 평가한 후 점수를 기준으로 순위
- **비교 평가**: 모델들을 서로 비교해 평가하고 비교 결과로 순위 계산

> 응답의 품질이 주관적일 때는 보통 개별 평가보다 **비교 평가가 더 쉽다**.

**예시**: LMSYS 챗봇 아레나 - 누구나 웹사이트에 접속해 프롬프트를 입력하면 익명의 두 모델에서 나온 응답을 받고, 더 나은 것에 투표

### 3.5.1 비교 평가의 과제들

| 과제 | 설명 |
|------|------|
| **확장성 병목** | 비교할 모델 쌍의 수는 모델 수의 제곱에 비례해 증가, 전이성 가정이 성립하는지 불분명 |
| **표준화와 품질 관리의 부재** | 누구나 참여 가능 → 기준 없음, 사실 확인 어려움 |
| **비교 성능 → 절대 성능 변환 어려움** | 어떤 모델이 더 나은지는 알려주지만, 얼마나 좋은지는 알려주지 않음 |

### 3.5.2 비교 평가의 장점과 미래

- **두 출력을 비교**하는 것이 각 출력에 구체적인 점수를 매기는 것보다 쉽다.
- 사람의 **선호도를 파악**하는 것을 목표로 하므로 새롭고 강력한 모델이 등장해도 포화 상태에 도달하지 않는다.
- 참조 데이터로 모델을 학습시키는 편법을 쓰기 어려워 상대적으로 **조작하기 어렵다**.
- AI의 끊임없이 확장되는 능력을 따라잡기 위해 계속 새로운 벤치마크를 만들어야 하는 부담을 줄여준다.

---

# 4장. AI 시스템 평가하기

## 개요

4장은 3장에서 배운 평가 방식들을 사용해 **모델이 아닌 애플리케이션 전체**를 평가하는 방법을 다룬다.

---

## 4.1 평가 기준

### 평가 주도 개발 (Evaluation-Driven Development)

애플리케이션을 만들기 위해 시간과 돈을 투자하기 전에, **먼저 애플리케이션을 어떻게 평가할지 이해하는 것**이 중요하다. (테스트 주도 개발에서 영감)

| 활용 분야 | 평가 기준 |
|----------|----------|
| 추천 시스템 | 참여도, 구매 전환율 |
| 사기 탐지 시스템 | 예방한 사기로 절약한 금액 |
| 코딩 | 기능적 정확성 |
| 파운데이션 모델 | 폐쇄형 작업에서 주로 평가 (분류, 다음 행동 예측) |

### 4.1.1 도메인 특화 능력 (Domain-Specific Capability)

특정 분야의 지식/규칙을 이해하고 수행하는 능력을 평가한다.

- 도메인 특화 벤치마크를 활용해 모델이 애플리케이션에 필요한 능력을 갖췄는지 평가
- **코딩**: 정확성, 효율성, 코드 가독성
- **객관식 문제(MCQ)**: 지식과 추론 능력 테스트에 적합 (생성 능력 평가에는 부적합)

### 4.1.2 생성 능력 (Generation Capability)

자연어 생성(NLG) 작업의 품질 평가 지표:

| 지표 | 설명 | 현재 중요도 |
|------|------|------------|
| **유창성** | 텍스트가 자연스럽게 읽히는지 | 중요도 감소 (AI 생성 텍스트가 사람과 구분 어려움) |
| **일관성** | 논리적으로 일관성 있는지 | 중요도 감소 |
| **충실성** | 번역에서 원문에 충실한지 | 여전히 중요 |
| **관련성** | 요약에서 핵심 내용 포함 여부 | 여전히 중요 |
| **사실 일관성** | 사실과 일치하는지 | **매우 중요** |

#### 사실 일관성 (Factual Consistency)

**검증 방식 두 가지:**

1. **국소적 사실 일관성**: 출력을 **컨텍스트**에 기반해 평가
   - 예: 요약문이 원문 내용을 반영하는지, 챗봇 응답이 회사 정책에 부합하는지

2. **전역적 사실 일관성**: 출력을 **공개된 지식**에 기반해 평가
   - 예: 일반 챗봇, 사실 확인, 시장 조사

**정교한 AI 판단 기법:**

| 기법 | 설명 | 장단점 |
|------|------|--------|
| **자체 검증 (SelfCheckGPT)** | 모델이 일치하지 않는 여러 출력 생성 시 원래 출력이 환각일 가능성 높다고 가정 | 효과적이나 비용 많이 듦 |
| **지식 강화 검증 (SAFE)** | 검색 엔진 결과를 활용해 응답 검증 (구글 딥마인드) | 더 신뢰성 있음 |

**텍스트 함의 (Textual Entailment):**
- 함의: 가설은 전제로부터 추론할 수 있다 (사실 일관성)
- 모순: 가설은 전제와 모순된다 (사실 비일관성)
- 중립: 전제는 가설을 함의하지도, 모순되지도 않는다

**관련 벤치마크**: TruthfulQA (817개 질의)

#### 안전성 (Safety)

모델이 생성하는 결과물이 해로운지 아닌지를 판단한다.

**위험한 콘텐츠 분류:**
1. 부적절한 언어 (욕설, 노골적 내용)
2. 유해한 추천과 지침
3. 혐오 발언
4. 폭력
5. 고정관념
6. 정치적/종교적 편향

**벤치마크**: RealToxicityPrompts (10만 개 프롬프트), BOLD

### 4.1.3 지시 수행 능력 (Instruction Following)

"이 모델이 주어진 지시를 얼마나 잘 따르는가?"

- JSON, 정규 표현식 같은 **구조화된 출력**이 필요한 애플리케이션에서 필수
- **벤치마크**: IFEval, INFOBench

**지시 유형:**
- 키워드 포함/제외
- 길이 제한
- 구두점/대소문자 규칙
- 출력 형식
- 언어
- 역할 연기

**역할 연기 (Role-playing):**
- 모델에게 가상 캐릭터나 페르소나를 가정하도록 요청
- 엔터테인먼트 목적 (게임 NPC, AI 동반자)
- 프롬프트 엔지니어링 기법으로 출력 품질 향상

### 4.1.4 비용과 지연 시간

고품질 결과물을 생성하지만 너무 느리고 비용이 많이 드는 모델은 쓸모가 없다.

**지연 시간 지표:**
- 첫 토큰까지 걸리는 시간 (TTFT)
- 토큰당 시간
- 토큰 간 시간
- 질의당 시간

**비용 고려사항:**
- 모델 API 사용: 규모가 커져도 토큰당 비용은 크게 변하지 않음
- 자체 호스팅: 규모가 커질수록 토큰당 비용을 크게 줄일 수 있음

> **파레토 최적화**: 품질, 지연 시간, 비용의 균형을 맞추는 연구

---

## 4.2 모델 선택

### 4.2.1 모델 선택 과정

**속성 구분:**
- **하드 속성**: 변경이 불가능하거나 비현실적인 속성 (라이선스, 학습 데이터, 모델 크기 등)
- **소프트 속성**: 변경할 수 있고 개선 가능한 속성 (정확도, 유해성, 사실 일관성 등)

**전반적인 평가 과정 (4단계, 순환적):**

```
1. 하드 속성이 적합하지 않은 모델 걸러내기
    ↓
2. 공개 정보(벤치마크, 리더보드)로 유망한 모델 추리기
    ↓
3. 자체 평가 파이프라인으로 실험 수행
    ↓
4. 운영 환경에서 지속적으로 모니터링
    ↓
   (피드백 반영하여 1번으로)
```

### 4.2.2 모델 자체 개발 vs 상용 모델 구매

**용어 정리:**
- **오픈 소스**: 모델 + 학습 데이터 모두 공개
- **오픈 웨이트**: 모델만 공개 (대다수가 해당)

**고려 사항 7가지:**

| 고려 사항 | 모델 API 사용 | 자체 호스팅 |
|----------|--------------|-------------|
| **데이터 프라이버시** | 데이터를 외부로 전송해야 함 | 데이터를 외부로 보낼 필요 없음 |
| **데이터 계보/저작권** | 계약으로 일부 보호 가능 | 투명성 낮음 |
| **성능** | 일반적으로 더 높음 | 조금 뒤처질 수 있음 |
| **기능** | 확장성, 함수 호출, 구조화 출력 지원 | 제한적일 수 있음 |
| **비용** | 사용량에 따른 요금 | 인재, 시간, 인프라 비용 |
| **제어/투명성** | 제한적 (요청 제한, 버전 관리 불투명) | 높음 (동결, 커스터마이징 가능) |
| **온디바이스 배포** | 불가능 | 가능 |

> **기업들이 오픈 소스에 관심을 갖는 이유**: 제어와 커스터마이징 가능성

### 4.2.3 공개 벤치마크 탐색하기

**평가 하네스(Evaluation Harness)**: 여러 벤치마크에서 모델을 평가하는 데 도움이 되는 도구

**공개 리더보드:**
- 일부 벤치마크의 종합 성능을 기반으로 모델 순위를 매긴다.
- 컴퓨팅 제약이나 비용 문제로 일부 벤치마크만 포함
- **허깅페이스 오픈 LLM 리더보드**: ARC-C, MMLU, HellaSwag, TruthfulQA, WinoGrande, GSM-8K 등 6개 벤치마크 평균

> **주의**: 공개 리더보드에서 높은 순위를 기록했다고 해서 애플리케이션에서도 잘 작동한다는 보장은 없다. **맞춤형 리더보드**를 만들어야 한다.

**데이터 오염 (Data Contamination):**
- 모델이 테스트 데이터로 학습했을 때 발생
- 인터넷에서 수집한 데이터에 공개된 벤치마크 데이터가 실수로 포함될 수 있음

**오염 감지 방법:**
- **n-gram 중복**: 테스트 샘플의 특정 시퀀스가 훈련 데이터에도 있는지 확인
- **퍼플렉시티**: 평가 데이터에 대해 유난히 낮으면 오염 의심

---

## 4.3 평가 파이프라인 설계하기

AI 애플리케이션의 성공 여부는 좋은 결과와 나쁜 결과를 구분하는 능력에 달려 있다. 이를 위해 **신뢰할 수 있는 평가 파이프라인**이 필요하다.

### 4.3.1 1단계: 시스템의 모든 구성 요소 평가하기

- 실제 AI 애플리케이션은 복잡하며, 여러 구성 요소와 단계를 거쳐 완료된다.
- **엔드투엔드 출력**과 각 구성 요소의 **중간 출력**을 독립적으로 평가해야 한다.

**평가 수준:**
- **턴 기반 평가 (Turn-based)**: 각 출력물의 품질 평가
- **작업 기반 평가 (Task-based)**: 시스템이 작업을 완료했는지 평가

### 4.3.2 2단계: 평가 가이드라인 만들기

명확한 **평가 가이드라인**을 만드는 것은 평가 파이프라인에서 가장 중요한 단계 중 하나이다.

**평가 기준 정의 예시 (고객 지원 애플리케이션):**
- **관련성**: 응답이 사용자의 질의와 관련이 있는가?
- **사실 일관성**: 응답이 컨텍스트와 사실적으로 일치하는가?
- **안전성**: 응답이 유해하지 않은가?

**가이드라인 작성 시 주의사항:**
- 애플리케이션이 **해야 할 일**뿐만 아니라 **하면 안 되는 일**도 정의
- 각 기준에 대해 평가 시스템 선택 (이진값, 1~5점 척도, 0~1 연속값)
- **예시와 함께 평가 기준표** 만들기
- **평가 지표를 비즈니스 지표와 연결**하기

### 4.3.3 3단계: 평가 방법과 데이터 정의하기

**평가 방법 선택:**
- 서로 다른 기준에는 서로 다른 평가 방법 필요
- 동일한 기준에 대해 여러 평가 방법을 혼합 사용 가능
- 로그프롭(logprob) 활용: 모델의 예측 확신도 측정

**데이터 슬라이싱:**
- 데이터를 하위 집합으로 나누고 각 하위 집합에 대한 시스템 성능을 개별적으로 분석
- **용도**: 편향 축소, 디버깅, 개선 영역 발굴, 심슨의 역설 회피

> **심슨의 역설**: 모델 A가 집계된 데이터에서는 모델 B보다 성능이 좋지만, 데이터의 모든 하위 집합에서는 성능이 떨어지는 현상

**평가 파이프라인 평가 질문:**
1. 평가 파이프라인이 **올바른 신호**를 제공하고 있는가?
2. 평가 파이프라인을 **얼마나 신뢰**할 수 있는가?
3. 지표 간 **상관관계**는 어떠한가?
4. 평가 파이프라인이 애플리케이션에 **얼마나 많은 비용과 지연 시간**을 추가하는가?

### 4.3.4 반복 (Iteration)

- 요구사항과 사용자 행동이 변화하면서 **평가 기준도 진화**해야 한다.
- 평가 파이프라인을 **반복적으로 개선**하면서 적절한 **실험 추적**을 수행해야 한다.
- 기록해야 할 변수: 평가 데이터, 기준표, AI 평가자에 사용된 프롬프트 및 샘플링 구성

---

## 핵심 요약

### 3장 핵심 포인트
1. 파운데이션 모델은 개방형, 블랙박스 특성으로 평가가 어렵다.
2. 언어 모델링 지표(엔트로피, 교차 엔트로피, 퍼플렉시티)를 이해하면 성능 파악에 도움이 된다.
3. 정확한 평가는 기능적 정확성과 참조 데이터 유사도 측정으로 수행한다.
4. AI 평가자는 빠르고 저렴하지만 비일관성, 편향 등의 한계가 있다.
5. 비교 평가는 주관적 품질 평가에서 개별 평가보다 쉽다.

### 4장 핵심 포인트
1. **평가 주도 개발**: 개발 전에 평가 기준을 정의한다.
2. 도메인 특화 능력, 생성 능력, 지시 수행 능력, 비용/지연 시간을 기준으로 평가한다.
3. 모델 선택 시 하드 속성과 소프트 속성을 구분하고, 4단계 순환 과정을 따른다.
4. API 사용 vs 자체 호스팅은 7가지 요소를 고려하여 결정한다.
5. 평가 파이프라인은 시스템 구성 요소 평가 → 가이드라인 작성 → 방법/데이터 정의 → 반복 개선의 단계를 거친다.

---

## 참고 자료

- ATH - study-note-ATH.md (HTML 형식 정리)
- KSE - study-note-KSE.md (마크다운 형식 정리)
- [HCY - 3장 velog](https://velog.io/@algorithm_cell/AI-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-3%EC%9E%A5.-%ED%8F%89%EA%B0%80%EB%B0%A9%EB%B2%95%EB%A1%A0)
- [HCY - 4장 velog](https://velog.io/@algorithm_cell/AI-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-4%EC%9E%A5.-AI-%EC%8B%9C%EC%8A%A4%ED%85%9C-%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0)
- [LEJ - chap3 정리](https://velog.io/@dkan9634/AI-Engineering-Chap-3.-%ED%8F%89%EA%B0%80-%EB%B0%A9%EB%B2%95%EB%A1%A0)
- [LEJ - chap4 정리](https://velog.io/@dkan9634/AI-Engineering-Chap-4.-AI-%EC%8B%9C%EC%8A%A4%ED%85%9C-%ED%8F%89%EA%B0%80%ED%95%98%EA%B8%B0)
