[8장 velog 링크](https://velog.io/@algorithm_cell/AI-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-8%EC%9E%A5.-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81)

[9-1장 velog 링크](https://velog.io/@algorithm_cell/AI-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-9-1%EC%9E%A5.-%EC%B6%94%EB%A1%A0-%EC%B5%9C%EC%A0%81%ED%99%94-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0)


<br>

모델의 품질은 **학습 데이터의 품질**에 달려 있다.

무한한 컴퓨팅 자원을 가졌더라도 데이터가 없으면 좋은 모델을 파인튜닝할 수 없다. 

**데이터셋 엔지니어링**의 목표는 **할당된 예산 내에서** 최고의 모델을 학습할 수 있는 **데이터셋**을 구축하는 것이다.


- **모델 중심 AI** : **모델 자체를 개선해 성능을 올리는 방식**으로, 새로운 아키텍처를 설계하거나, 모델을 더 크게 만들하거나, 새로운 학습 기법을 개발하는 방식이다.

- **데이터 중심 AI** : **데이터를 개선해 성능을 올리는 방식**으로, 새로운 데이터 처리 기법을 개발하거나, 고품질 데이터셋을 만드는 등의 방식이다.

딥러닝 초기에는 AI벤치마크 대부분은 **모델 중심**이었지만, 최근에는 **데이터 중심 벤치마크**가 늘고 있다.


# 1) 데이터 큐레이션

**데이터 큐레이션**은 필요에 따라 데이터에 접근/재사용할 수 있도록 데이터셋을 생성하고 관리하는 프로세스이다.

모델이 새로운 행동을 학습하도록 돕는 **새로운 데이터를 만드는 것**뿐만 아니라, 모델이 **학습하지 않아야 할 오류 행동을 줄이기 위해 기존 데이터에서 일부 샘플을 제거하거나 비중을 조정**하는 작업까지 포함한다.


애플리케이션마다 필요한 데이터 특성이 다를 수 있다.

- **자기 지도 학습 파인튜닝** : 데이터 sequence

- **선호도 파인튜닝** : (지시, 선호 응답, 비선호 응답) / ((지시,응답), 점수)

※ 특히, **멀티 턴 데이터, CoT, 도구 사용에 대한 데이터**는 만들기 어렵다.

따라서, 단순히 데이터를 모으는 것을 넘어, 목표 작업에 맞는 데이터셋을 체계적으로 구성하는 과정이다.

일반적으로, **데이터 품질, 데이터 커버리지, 데이터 양**이라는 3가지를 기준을 따른다.

## 데이터 품질

고품질의 데이터를 정의하는 기준은 너무 많고, 작업에 따라 다르다.

일반적으로는 다음 6가지를 기준으로 고품질 데이터를 판단한다.

### 1. 관련성

- 학습 예시는 모델이 학습하려는 작업과 관련이 있어야한다.


### 2. 작업 요구사항 부합

- 주석은 작업 요구사항에 부합해야한다.
※ 주석 : 데이터에 붙이는 라벨, 정답 등


### 3. 일관성

- 주석은 예시들끼리, 주석자들 간에 일관되어야 한다.
- 작업 요구사항과 부합하면서도 일관된 주석을 만들려면 **주석 가이드라인**이 필요하다.

### 4. 올바른 형식

- 모든 예시는 모델이 기대하는 형식을 따라야한다.
- ex) 웹사이트에서 제품 리뷰를 스크래핑한다면 html 태그를 제거해야한다.


### 5. 충분한 고유성

- 데이터에서 고유한 예시를 말한다.
- 모델 학습에서의 중복은 **편향**을 만들고 **데이터 오염**을 일으킬 수 있다.

### 6. 규정 준수

- 데이터는 모든 관련 내부 및 외부 정책을 준수해야한다.


## 데이터 커버리지

좋은 커버리지를 확보하려면 데이터의 다양성이 필수적이므로, **데이터 다양성**이라고도 한다.

애플리케이션마다 **필요한 다양성의 종류가 다르다.**

> `<The Data Addition Dilemma>`에서는 경우에 따라 이질적인 데이터를 추가하면 오히려 성능이 나빠질 수 있다


> Llama 3 성능 향상은 모델 구조 변경보다도
>
> - 데이터 품질 개선
> - 데이터 다양성 개선
> - 학습 규모 증가
> 에 있다.




## 데이터 양

※ 수백만 개의 예시를 처음부터 파인튜닝 하는 것이 효율적이지 않을 수 있다?

- 사전 학습이 모델 가중치를 **경화**시켜서 파인튜닝 데이터에 잘 적응하지 못하게 할 수 있다.
- 학습 데이터가 많을 때 특히 그렇다.
- 작은 모델일수록 더 취약하다.


**학습에 필요한 데이터 양을 결정하는 3가지 요인이 있다.**

### 1. 파인튜닝 기법

- 전체 파인튜닝 vs PEFP

    - (지시, 응답) 쌍이 수만 개 ~ 수백만 개 **전체 파인튜닝**
    - (지시, 응답) 쌍이 수백 개~ 수천 개 **PEFT**

### 2. 과제 복잡성

- 과제가 복잡할 수록 더 많은 데이터가 필요하다.

### 3. 기본 모델의 성능

- 기본 모델이 원하는 성능에 가까울 수록 목표에 도달하는 데 필요한 데이터가 더 적다.



![](https://velog.velcdn.com/images/algorithm_cell/post/84670ab0-26c9-4832-9d38-1bb8a21b852f/image.png)

- **데이터가 적다면 고급 모델에 PEFT방법**을 , **데이터가 많다면 작은 모델에 전체 파인튜닝**을 사용하는 것이 좋다.

![](https://velog.velcdn.com/images/algorithm_cell/post/37ad09f9-4b53-4f56-a758-d0f609a84c26/image.png)

- 대규모 데이터셋 구축을 시작하기보다, 먼저 **잘 만들어진 소규모 데이터셋**으로 시작해서 파인튜닝이 모델 개선에 효과가 있는지 확인해야한다.

- 데이터셋 크기별 성능 증가 곡선을 보면 추가학습 예시가 모델 성능에 어떤 영향을 주는지 추정할 수 있다.

![](https://velog.velcdn.com/images/algorithm_cell/post/fe91880b-bf28-474c-bc78-0b0e37858f5d/image.png)

- 예시의 개수 뿐만 아니라 **예시의 다양성도 중요하다.**
- 과제가 9개에서 282로 증가했을 때 모델의 성능도 크게 향상 되었다.


### 고품질 데이터의 양을 줄이는 접근법

1. **자기 지도 학습 → 지도 학습**

2. **관련성 낮은 데이터 → 관련성 높은 데이터**

3. **합성 데이터 → 실제 데이터**

- 서로 다른 두 번의 파인튜닝 작업을 하면서 전환을 잘 조율해야한다.



## 데이터 수집 및 주석

데이터 수집의 목표는 사용자 프라이버시를 존중하고 규정을 지키면서 필요한 품질과 다양성을 갖춘 충분한 크기의 데이터셋을 만드는 것이다. 


### 가장 중요한 데이터 소스

**사용자가 만든 데이터를 활용해 제품을 지속적으로 개선하는 데이터 플라이휠**을 구축할 방법을 찾아낸다면 상당한 이점을 얻을 것이다.

애플리케이션 자체 데이터는 관련성이 높고 원하는 데이터 분포와 잘 맞아서 최고의 데이터가 된다.


### 공개 데이터셋 

직접 데이터셋을 만들기 전에 사용 가능한 데이터셋이 있는지 먼저 확인해야 한다.

**데이터 마켓플레이스**는 굉장히 다양하고 오픈소스 데이터, 독점 데이터 모두 제공한다.

- 이상적으로는 필요한 데이터셋을 정확히 찾지만, 데이터는 다양한 획득 경로를 거쳐 만들어지기 때문에 대부분의 경우 여러 데이터셋을 조합해서 구성해야 한다.

**공개 데이터셋**도 있다.

- 단, 기존 데이터를 맹신하면 안 되고 반드시 직접 검증해야 한다.

- ex) HuggingFace, Kaggle, Google Dataset Search, Data.gov, ICPSR 등


### 주석

파인튜닝을 위해서는 자체 데이터에 주석을 달아야하는 경우가 많다.

주석이 어려운 이유는 **명확한 주석 가이드라인을 만드는 것 자체가 복잡**하기 때문이다.


---


# 2) 데이터 증강 및 합성


- 데이터 증강 : 기존 데이터에서 새로운 데이터를 만든다.
- 데이터 합성 : 실제 데이터의 특성을 모방하는 데이터를 생성한다.

- ex) 마우스 웹페이지에서 어떻게 움직이는지 시뮬레이션해서 봇의 움직임 패턴에 대한 데이터를 생성할 수 있다.



## 합성 이유

### 1. 데이터 양 늘리기

- 대규모 데이터를 만들어 AI 모델 학습과 테스트에 풍부한 데이터를 공급할 수 있다는 점이다.

- **실제 데이터를 구하기 어렵거나 부족한 경우**에 특히 유용하다.

- ex) 심해 탐사 데이터, 자율주행차 사고 데이터 등 

### 2. 데이터 커버리지 늘리기

- 특정 특성을 가진 데이터를 생성해 모델 성능을 개선하거나, 특정 행동을 하도록 만들 수 있다.

- 기존 데이터가 부족한 부분을 채우는 맞춤형 데이터를 만드는 것이다.

- ex) AI를 사용해 적대적 예시를 만든다.


### 3. 데이터 품질 향상


- 사람과 AI는 근본적으로 다른 방식으로 작동하고 다른 도구를 선호하기 때문에, 사람 전문가가 생각해내는 것보다 더 복잡한 문제를 생각해낼 수 있다.



### 4. 프라이버시 문제 해결


- 의료 분야에서는 법적 규제로 인해 실제 환자 기록으로 모델을 학습시킬 수 없다. 
- 민감한 정보가 없는 합성 환자 기록을 생성해 모델에 학습할 수 있는 데이터셋을 만든다.


### 5. 모델 증류


작은 모델이 큰 모델을 **모방하도록** 학습시키는 방법이다. 

큰 모델의 지식이 작은 모델로 증류되어 들어간다는 의미이다.


큰 모델을 배포하려면 자원이 많이 들기 때문에 **증류를 통해 teacher 모델과 비슷한 성능을 내면서도 더 작고 빠른 student 모델**을 만들 수 있다.




이외에도 명백한 강점이 있다. 

이러한 장점 덕분에 더 많은 모델이 합성 데이터로 학습되고 있고, 합성하는 기법 역시 계속해서 개발되고 있다.



## 전통적인 데이터 생성 기법

알고리즘으로 데이터를 생성하는 것으로, **절차적 생성**이라고도 한다.

### 1. 규칙 기반

1. **규칙과 템플릿 활용**

    - 미리 정해둔 규칙과 템플릿을 사용하는 방식이다.

    - ex) 신용카드 거래 내역 : 민감 정보
    - ex) 청구서, 이력서, 세금 신고서 등
    - ex) 수학 방정식처럼 특정 규칙과 형식을 따르는 데이터


2. **기존 데이터에 변형**

  - 기존 데이터에 변형을 가해서 새로운 데이터를 절차적으로 생성할 수 있다.

  - ex) 이미지

      - 무작위로 회전, 자르기, 크기 변경 등을 할 수 있다. 

      - ImageNet 데이터셋이 이 증강 기법을 활용해 만들어진 데이터셋이다.

  - ex) 텍스트

      - 뜻이나 감정이 바뀌지 않는 선에서 유의어 사전이나, 임베딩을 찾아 유의어로 바꿀 수 있다.
      
      - 이 방식은 데이터의 편향을 줄이는 방식에도 활용될 수 있다.

    ![](https://velog.velcdn.com/images/algorithm_cell/post/aee74966-6637-432f-ba0e-55af9423afbe/image.png)
    - nurse가 여성과 많이, doctor가 남성과 많이 쓰인다면, 증강 데이터에서는 doctor를 여성과 nurse를 남성과 함께 사용해 **편향**을 줄인다

  - **섭동** : 기존 데이터에 노이즈를 넣어서 새로운 데이터를 생성

    - 초기 연구에서 **데이터 샘플을 아주 조금만 건드려도 모델을 속여 잘못 분류**하게 만들 수 있다는 사실이 발견됨
    
    - **공격 방어, 강건성 향상 학습에 사용**
    
    - **이미지** : 밝기 조정, 대비 변경 등 15가지 흔한 시각적 왜곡을 ImageNet 이미지에 적용해 ImageNet-C, ImageNet-P를 제작
    
    - **텍스트** : BERT 학습 시 토큰의 1.5%를 무작위 단어로 교체





### 2. 시뮬레이션

실제 세계에서 실험을 통해 데이터를 모으는 것은 비용도 많이 들고 위험할 수 있기 때문에, 실험을 통해 가상으로 시뮬레이션할 수 있다.

**Ex1) 자율주행**
- 자율주행차의 위험 상황을 실제상황에서 테스트하기 어려우므로 가상 환경에서 그 상황을 시뮬레이션한다.
- 시뮬레이션 엔진 : CARLA, SimulationCity, 테슬라의 샌프란시스코 시뮬레이션 등

**Ex2) 로보틱스**

- 로봇 걷기 학습 : 다양한 관절 움직임 시나리오를 시뮬레이션하고 제대로 움직이는 시나리오를 골라 그 결과를 기반으로 로봇을 학습시킬 수 있음

**Ex3) 도구 사용 학습**

- 시뮬레이션은 모델에게 도구 사용법을 가르치는 데이터를 생성하는 데도 많이 사용된다.

- 사람이 만든 행동이 항상 AI 에이전트에게 효율적이지 않을 수 있기 때문에 시뮬레이션은 사람이 놓치는 행동을 찾는 데 도움을 준다.

- 다양한 행동 순서를 시뮬레이션하고 실행한 결과를 바탕으로 가장 효율적인 행동 순서를 골라 학습시킨다.


**시뮬레이션의 장점**

- 시뮬레이션을 사용하면 사고나 물리적 손상 없이 최소 비용으로 여러 실험을 수행 가능하다.

- 시뮬레이션에서 실패한다면 실제 세계에서도 실패할 가능성이 높다.

- 현실에서 일어나기 힘든 사건의 데이터를 생성하는 데 특히 유용하다.

	
    - 지구 시스템 시뮬레이션, 이상탐지 등
  
    - 이런 합성 데이터를 모델에 넣으면 더 다양한 가능성들로부터 학습할 수 있게 해준다.
    

**시뮬레이션의 한계**

- 시뮬레이션에서 잘 작동한 것이 현실에서는 작동하지 않을 수도 있음

- 아무리 정교한 시뮬레이션도 결국 실제 세계를 단순화한 것에 불과함
	
    - **Sim2Real** : 시뮬레이션에서 학습한 알고리즘을 실제 세계에 적용하는데 초점을 맞춘 연구도 등장하고 있다.


## AI 기반 데이터 


사람이 데이터 만드는 방식이 거의 무한하듯, AI도 다양한 방법으로 합성 데이터를 만들 수 있다.

1. **시뮬레이션을 통한 데이터 생성**

    - StableToolBench

        - 실제 API를 호출하지 않고도 과정을 시뮬레이션해서 결과를 예측한 데이터를 만들 수 있다.

    - 즉, 현실에서 하기 힘들거나 비싼 실험을 AI가 대신 실행해 데이터를 만든다.

2. **셀프 플레이**

    - AI는 결과뿐 아니라 사람의 행동 자체도 시뮬레이션 가능하다.

    - 예시로, 체스는 사람끼리 두면 시간이 오래걸리지만, AI끼리 두면 훨신 빠르게 데이터를 수집할 수 있다.

3. **바꿔쓰기**

	- AI는 기존 질문을 다른 형태로 바꿔 데이터 다양성을 늘릴 수 있다.

4. **번역**

	- 온라인에 데이터가 많은 언어의 데이터를 저자원 언어로 번역해 저자원 언어 모델을 구축할 수 있다.

5. **역변역**

    - 번역된 결과의 품질을 점검하는 방법이다.

    - 원문 x와 번역문을 다시 번역한 x'을 비교하여 합성 번역 데이터 품질 확인에 활용한다.

6. **프로그래밍 언어**

	- 코드 조각을 바탕으로 설명과 문서를 생성하고, 설명과 문서를 바탕으로 코드 조각을 다시 생성해 원본과 비교 후 제대로 구현했다고 여겨질 때만 파인튜닝에 사용한다.


이러한 합성 데이터는 **사후학습에서 훨씬 자주 사용된다.**

- AI는 기존 지식을 다른 형태로 바꾸는 것은 쉽게 하지만, **새로운 지식을 만들어 내는 합성 자체는 어렵기 때문이다.**

- 지시 데이터, 선호 데이터는 만들기 어렵지만, 여러 응답 중 좋은 응답을 선택하는 것은 비교적 쉽기 때문이다.

인터넷 데이터에 의존해 학습하는 모델들은 **이미 합성 데이터로 사전학습 되고 있을 가능성도 높다.**


### 1. 지시 데이터 합성

지시 파인튜닝 데이터(SFT) 는 **지시(instruction) + 응답(response)** 으로 구성되어 있다.


>그리고 이 데이터는 다음 방식으로 만들 수 있다.
>
>- AI가 지시 생성 + 사람이 응답 작성
>
>- 사람이 지시 작성 + AI가 응답 생성
>
>- AI가 지시,응답 둘 다 생성


1. **지시 생성**

    - 좋은 지시 데이터를 만들기 위해서는 주제, 키워드, 지시 유형 목록을 만드록 각 항목별 일정 수의 지시를 만든다.

    - 템플릿 세트를 만들고 템플릿당 예시를 생성할 수 있다.
    
   
   	- ex) LLama 3 코딩 지시 데이터 생성
    
 		
        > 1. AI로 문제 설명 생성
        > 2. AI로 해결 코드를 다양한 프로그래밍 언어로 생성
        > 3. AI로 생성 코드 테스트할 단위 test 생성
        > 4. AI에게 합성된 코드의 오류 수정시키기
        > 5. AI로 코드를 다른 프로그래밍 언어로 번역하고, **역번역** 검증을 통과하지 못하는 설명과 문서를 제외시킨다.
        
       - 이 파이프라인을 통해 코딩 관련 예시를 생성했다.

    
 - **역지시**

    - AI는 응답이 길수록 환각 위험이 증가하기 때문에 이미 존자하는 긴 고품질 콘텐츠를 가져와서 그 콘텐츠를 유도할 수 있는 **프롬프트를 AI가 생성하게 한다.**

        - 환각을 줄이면서 **고품질 지시 데이터를 확보**
    
    - **수동 주석을 추가하지 않고** 강한 모델을 만들 수 있다.

        - 과정

        > 1. 소수의 초기 예시로 약한 모델을 학습한다.
        > 2. 약한 모델로 기존 고품질 콘텐츠에 대한 **지시를 생성한다**.
        > 3. 이 데이터로 약한 모델을 **파인튜닝**한다.
        > 4. 원하는 성능이 될 때까지 반복한다.



2. **응답 생성**

    - 지시 하나당 응답 1개만 만들 수도 있고 응답을 여러 개 만들 수도 있다.
    - long-context 파인튜닝 데이터

        - 모델이 처리 가능한 범위보다 더 큰 범위까지 이해하도록 만들고 싶을 때 활용하는 방식이다.

        - 과정

        > 1. 긴 문서를 짧은 문서로 나눈다.
        > 2. 각 덩어리마다 여러 (질의,응답) 쌍 생성
        > 3. 각 (질의, 응답) 쌍에 원래 긴 문서를 컨텍스트로 제공
        >
        > **입력 = 긴 문서 + (질의, 응답) 쌍**

        - 입력을 줬을 때 **해당 출력을 생성하도록 학습시키는 것**이다.

   

### 2. 데이터 검증

모델 성능은 데이터 품질에 크게 좌우되므로, 데이터 품질을 검증하는 방법을 갖추는 것이 중요하다.

1. **기능적 정확성**

    - **모델이 만든 출력이 의도한 기능을 실제로 수행하는가?** 를 기준으로 평가하는 방법이다.

    - ex) 코드 생성, 함수 작성

2. **AI 평가자 활용**

    - 기능적 정확성으로 검증할 수 없는 합성 데이터는 보통 **AI 검증기**를 사용한다.

    - 범용 AI 평가자일 수도 있고, 특정 과제에 특화된 채점기일 수도 있다.

      - ex) 점수 매기기, 좋음과 나쁨으로 분류, 요구사항을 명시하고 이를 만족하는지 판정하게 하기

    - 데이터에 사실적 일관성이 중요하다면 환각 가능성이 높은 예시를 걸러낼 수 있다.
    - 합성 데이터 품질은 활용 사례와 생성 데이터 특성에 따라 창의적으로 검증할 수도 있다.




### 3. AI 생성 데이터의 한계

AI가 생성한 데이터가 사람이 생성한 데이터를 완전히 대체하기는 어렵다.

그 이유는 여러 가지가 있지만, 대표적으로 네 가지를 짚어볼 수 있다.

1. **품질 관리**

    - AI가 생성한 데이터는 품질이 낮을 수 있고, 합성 데이터의 품질을 평가할 지표를 개발해야 한다.


2. **피상적 모방**

    - 모방을 통해 얻은 성능은 겉으로만 좋아보일 수 있다.

    - student 모델이 teacher 모델의 **스타일(말투/표현 방식)**을 흉내 내는 데는 뛰어나지만, **사실적 정확성**이나 학습 데이터 범위를 벗어난 과제에 대한 일반화 능력은 오히려 어려움을 겪을 수 있다.

3. **성능 저하 가능성**

    - AI가 생성한 데이터를 모델이 얼마나 잘 학습할 수 있는지 확실하지 않다.

    - 일부 연구에 따르면, AI 생성 데이터를 반복적으로 학습에 사용하면 모델에 되돌릴 수 없는 결함이 생기고 시간이 갈수록 성능이 저하될 수 있다고 한다.

    > **모델 붕괴 *model collapse***
    >
    > - VAE, 가우시안 혼합 모델, LLM을 포함한 모델들에서 발생할 수 있음을 보였다.
    >
    > - 사전학습(pre-training), 사후학습(post-training) 모두에서 일어날 수 있다.
    >
    >  AI는 원래도 확률이 높은 사건을 더 자주 생성하고, 희귀한 사건은 잘 생성하지 못할 수 있다.
    >
    > 이를 반복하면 결과적으로 모델이 더 일반적인 사건만 출력하고 희귀하지만 중요한 사건을 잊게 되므로, **데이터 분포가 점점 좁아지면서 다양성이 붕괴된다.**
    >
    >

4. **불분명한 데이터 계보**

    - AI 모델은 **학습 데이터의 영향을 받고 때로는 사용자도 모르게 그 내용을 그대로 출력**할 수 있다.

    - ex) 모델 X를 사용해서 자신의 모델을 학습시킬 데이터를 만든다고 할 때, 모델 X가 저작권을 위반한 데이터로 학습했다면, 자신의 모델 역시 저작권을 위반할 수 있다.














































---

# 3) 데이터 처리

## 데이터 처리 팁

- 데이터 처리 단계는 시간과 컴퓨팅 자원을 절약할 수 있는 순서라면 어떤 순서로든 진행해도 된다.

- 모든 데이터에 스크립트를 적용하기 전에 처리 스크립트가 제대로 작동하는지 확인하기 위해 향상 테스트를 진행하자.

- 데이터 원본에서 바로 수정하지 말자.

## 검사


데이터를 확인하고 품질을 파악해야한다. 

1. **먼저 기본 질문들을 통해 데이터의 정체를 확인해야 한다.**

    - 데이터는 어디서 왔는가?

    - 데이터는 어떻게 처리(preprocess)되었는가?
    


2. **데이터셋의 통계적 특성을 시각화하여 분석한다.**

  - **토큰 분포**
  <p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/6013be14-5499-456c-a638-60fd3cd5cd28/image.png">
    GPT-4는 동사-명사 조합이 더 넓고 다양하게 나타남</p>

  - **입력 길이 분포, 응답 길이 분포**

  <p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/34f35fe2-6289-4591-abf1-c661344f783e/image.png">
    GPT-4는 GPT-3보다 더 긴 응답을 생성하는 경향이 있음</p>





## 중복 제거

중복된 데이터는 **데이터의 분포를 왜곡해고 편향**을 만들 수 있다.

### 1. 쌍대 비교

- 데이터셋의 각 예시를 다른 모든 예시와 비교해서 유사도 점수를 계산
	
    - 정확한 일치, n-gram 유사도, 퍼지 매칭, 의미적 유사도 사용

- 데이터가 클수록 비용이 매우 큼


### 2. 해싱

- 예시를 여러 버킷으로 해시해서 묶은 뒤 같은 버킷 안의 데이터끼리만 확인
	
    - MinHash, Bloom filter

- 대규모 데이터에서 효율적으로 중복 제거


### 3. 차원 축소

- 먼저 데이터의 차원을 줄이고 그 다음 쌍대 비교 또는 벡터 검색을 수행

뿐만 아니라, 다양한 중복 제거 **라이브러리**를 활용 가능하다.

## 정리 및 필터링

- 불필요한 형식 토큰 제거

- 유해한 데이터, 저작권이 있는 데이터 등 정책에 맞지 않는 데이터 제거

- 저품질 데이터 제거 

  - 데이터 검증 기법으로 저품질 데이터를 찾고 제거할 수도 있지만, 이 단계에서는 특히 데이터를 직접 보는 것 또한 매우 중요

- 데이터가 많거나 컴퓨팅 예산 문제로 데이터를 모두 쓰기 어렵다면, 데이터를 선별

  - 능동 학습 : 모델 학습에 가장 도움이 되는 예시를 선택

  - 중요도 샘플링 : 과제에 가장 중요한 예시를 찾아 선택


## 형식 맞추기


데이터를 중복 제거하고 정리했다면, **파인튜닝할 모델이 기대하는 형식**에 맞춰야한다.

파인튜닝 데이터 형식이 다르면 모델의 성능에 영향을 줄 수 있고, **어떤 형식이 가장 좋은 지는 실험을 통해 알아내야한다.**



---


데이터셋 제작은 복잡하지만 원칙은 목표 행동을 정의하고, 품질,커버리지,양 기준으로 설계하는 것이다.

데이터셋 설계는 여전히 창의적이고 핵심적인 작업이다.


---

모델의 성능이 아무리 좋더라도, 지연 시간이 길거나 비용이 크다면 가치는 크게 떨어진다.

이번 장에서는 AI 추론 과정에서 발생하는 병목 현상을 분석하고, 이를 해결하기 위한 최적화 기법들을 다룬다. 

추론 최적화는 모델, 하드웨어, 서비스 수준의 세 가지 관점에서 접근할 수 있다.


# 1) 개요

AI 모델의 생명주기는 **학습**과 **추론**으로 이루어진다.

![](https://velog.velcdn.com/images/algorithm_cell/post/5a24d6c3-5b20-4269-839a-8ff615385061/image.png)


- **추론 서비스** 

  - 사용자의 요청을 받아 적절한 곳(모델/서버)으로 보낸다.
  
  - 추론 서버 도달 전 전처리
  
  - ex) **모델 API** 

- **추론 서버**

  - 운영 환경에서 모델 추론을 실행하는 구성 요소 
  
  - 여러 모델을 호스팅하고 필요한 하드웨어를 사용할 수 있다.
  
  - 애플리케이션에서 요청이 들어오면 리소스를 할당해서 적절한 모델을 실행하고 결과를 사용자에게 반환한다.


## 연산 병목

최적화는 병목을 찾아서 해결하는 일이다. 추론 서버도 추론 작업의 연산 병목을 해결하도록 설계해야 한다.


### 1. 연산 제약

- 작업을 끝내는 데 걸리는 시간이 연산량에 따라 결정되는 경우를 의미한다.

- ex) 암호 해독 : 암호화 알고리즘을 뚫기 위해 복잡한 수학 연산을 많이 해야 하므로 연산 제약을 받는다.

- 작업을 더 많은 칩에 **분산**시키거나, **연산 성능**이 더 좋은 칩을 활용해 속도를 높일 수 있다.

### 2. 메모리 대역폭 제약

- 시스템 내부의 데이터 전송 속도에 제약을 받는 경우를 의미한다.

- 메모리와 프로세서 간의 데이터 전송 속도가 핵심적인 병목 지점이 된다.

- ex) CPU 메모리에 데이터를 저장하고, GPU에서 모델을 학습하는 경우에는 메모리 제약을 받는다.

- 대역폭이 더 넓은 칩을 활용해 속도를 높일 수 있다. 

※ **메모리 부족 오류** 역시 실제로는 메모리 대역폭 문제로 볼 수 있다.

- GPU 메모리가 부족해서 모델 전체를 GPU에 올릴 수 없다면 GPU와 CPU에 나눠 저장할 수 있지만, 메모리 대역폭 제약을 받는다.


> 모델 구조와 작업 종류에 따라 연산 병목도 다르게 나타난다.
>
> **이미지 생성 모델 추론**은 보통 **연산 제약**이고, **자기회귀 언어 모델 추론**은 보통 **메모리 대역폭 제약**이다.

**언어 모델 추론**

![](https://velog.velcdn.com/images/algorithm_cell/post/d7f53125-1177-42df-832b-a6e5de5cff0d/image.png)

1. **프리필**

- 모델이 입력 토큰들을 병렬로 처리한다.

- 한번에 처리할 수 있는 토큰 개수는 하드웨어가 정해진 시간에 실행할 수 있는 연산량에 따라 달라진다.

 따라서, **연산 제약**이다.

2. **디코딩**

- 모델이 출력 토큰을 한 번에 하나씩 생성한다.

- 모델 가중치와 같은 행렬을 GPU에서 불러오는 작업을 포함하므로, 하드웨어가 데이터를 메모리로 불러오는 속도에 따라 제한된다.

 따라서, **메모리 대역폭 제약**이다.


### 산술 강도 

- **메모리 1바이트에 접근할 때 몇 번의 산술을 하는 지**를 의미한다.

- 산술강도로 **연산이 어느 쪽의 제약을 받는지** 구분할 수 있다. 

- 엔비디아 엔사이트 *NVIDIA Nsight*같은 프로파일링 도구를 쓰면 **루프라인 차트**로 작업이 어떤 제약인지 확인 할 수 있다. 

![](https://velog.velcdn.com/images/algorithm_cell/post/93bb584f-a148-4920-a3c7-9fcf9fc1025e/image.png)


### API

많은 모델 제공 업체가 추론 API를 제공한다. 

1. **온라인 API**
	
    - 온라인 API는 **지연 시간을 최적화**한다.
    
    - 요청이 들어오면 바로 처리한다.
    - ex) 챗봇, 코드 자동 생성 같은 고객 대면 서비스 

2. **배치 API**
	
    - 배치 API는 **비용을 최적화**한다.
    
    - 지연 시간이 중요하지 않은 작업에 대해 배치 API로 보내 더 효율적으로 처리한다.
	
    - 요청을 한번에 모아서 처리하거나, 저렴한 하드웨어를 쓰는 등의 최적화 기법을 사용한다.
	- 배치 API는 구글 제미나이, 오픈AI 모두 50% 할인된 가격이지만, 처리 시간은 훨씬 길다. (초, 분 단위 X)

	- ex) 합성 데이터 생성, 정기 보고서 생성, 회사 데이터를 다시 정리하는 지식 베이스 업데이트
    

---


# 2) 추론 성능 지표

## 지연시간 

지연 시간은 **사용자가 질의를 보낸 시점부터 완전한 응답을 받기까지 걸리는 시간**을 의미한다.

1. **첫 토큰까지 걸리는 시간 *Time to first token(TTFT)***

- 사용자가 질의를 보낸 후 첫번째 토큰이 나오기까지 걸리는 시간이다.
- 프리필 단계에 해당하고, 입력 길이에 따라 달라진다.
- 애플리케이션마다 TTFT에 대한 사용자 기대치가 다를 수 있는데, 대화형 챗봇이라면 TTFT는 즉시 나와야하지만, 긴 문서를 요약할 때는 좀 더 기다려도 괜찮을 수 있다.


2. **출력 토큰 당 시간 *Time per output token(TPOT)***

- 첫 토큰 이후 각 토큰이 생성되는 시간을 측정한다.
- 매우 빠른 독자는 토큰당 120ms로 읽을 수 있으므로, 초당 6~8개의 토큰이면 대부분 충분하다.

3. **토큰 간 시간 *Time between tokens(TBT)* 토큰 간 지연시간*(ITL)***

- 출력 토큰 사이 걸리는 시간을 측정한다.


전체 지연 시간은 **TTFT + TPOT * 출력 토큰 수**이다.

전체 지연 시간이 같아도 TTFT와 TPOT에 따른 최적의 사용자 경험은 다르다.

지연 시간은 여러값들의 분포로 나타나기 때문에 **평균**만 보면 잘못 판단할 수 있다.
따라서, **백분위수로 보는 것이 더 도움이 된다.**


## 공개까지의 시간 *Time to publish*

사용자가 실제로 보는 첫 토큰까지의 시간을 의미한다.

> 사용자가 질의를 보낸 후 모델이 다음과 같은 과정을 거친다고 가정해보자.
>
> 1. 행동 순서로 이뤄진 계획을 생성한다. 이 계획은 사용자에게 공개하지 않는다.
> 2. 행동을 실행하고 그 결과를 기록한다. 이 결과들도 사용자에게 보여주지 않는다.
> 3. 이 결과들을 바탕으로 사용자에게 보여줄 **최종 응답**을 생성한다.

모델 입장에서는 1단계에서 첫 토큰이 나오지만, 사용자는 3단계에서 생성된 최종 출력의 첫 토큰만 보기 때문에 **사용자가 느끼는 TTFT는 훨씬 길다.**



## 처리량 *Throughput*

- 추론 서비스가 모든 사용자와 요청을 통틀어서 초당 몇 개의 출력 토큰을 만들어낼 수 있는지를 측정한다.
	
    - tokens/s (TPS)
  
    - 여러 사용자에게 서비스 할 시 : tokens/s/user
 
    - 정해진 시간 동안 완료한 요청 개수로 측정할 수도 있다. **(RPS)**
    
    - 파운데이션 모델 기반 애플리케이션에서는 **분당 완료 요청수(RPM)**을 더 많이 사용한다.

- 입력 토큰 처리(프리필), 출력 토큰 처리(디코딩)은 병목 지점이 다르고, 추론 서버는 둘을 분리해서 처리하는 경우가 많기 때문에 따로 연산해야한다.

- 보통의 경우, **처리량은 출력 토큰**을 의미한다.
- 처리량이 높을수록 보통 **연산비용이 낮다.**

- 작은 모델과 고성능 칩일수록 **처리량이 높다.**

- 입/출력 길이가 일정한 작업이 최적화하기 쉽기 때문에 처리량이 높다.
	
    - 배치 처리에서는 보통 길이를 맞추기 위해 padding을 넣는다.
  
    - 길이가 일정하면 패딩이 거의 없어져서 GPU가 실제 토큰만 계산하기 때문에 throughput이 증가한다.
   
   
## 굿풋 *goodput*

일반적으로, **지연 시간과 처리량 사이에는 Trade-off**가 있기 때문에, 추론 서비스를 처리량과 비용만으로 판단하면 사용자 경험이 나빠질 수 있다.

- 굿풋은 **소프트웨어 수준 목표(SLO)** 를 만족하는 초당 요청 개수를 측정한다.


    
![](https://velog.velcdn.com/images/algorithm_cell/post/016716a0-fc4f-43b2-b417-2dc30237bb0a/image.png)

- 추론 서비스가 초당 10개 요청을 처리하지만, 그 중 TPOT가 100ms, TTFT가 200ms를 넘지 않는 작업은 3개 뿐이므로 goodput은 3RPS가 된다.


## 활용률

리소스가 얼마나 효율적으로 사용하고 있는지 측정한다. 

보통 **전체 사용 가능한 용량 중에서 실제로 쓰이고 있는 비율을 의미한다.**

- GPU 사용량 모니터링 **nvidia-smi** (※ SMI : System Management Interface)

활용률이 높다고 해서 효율적으로 작업을 처리하고 있다는 것은 아니다.

- ex) 초당 100개의 연산이 가능한 GPU가 초당 1개의 연산만 하고 있어도, 활용률은 100%로 나타날 수 있다.

## MFU *model FLOP/s utilization*

할 수 있는 모든 연산 중에서 정해진 시간에 실제로 몇 개를 하는 지, 즉 **연산 효율 지표**이다.

- 최고 FLOP/s로 동작할 때 달성할 수 있는 이론상 최대 처리량 대비, 실제 처리량이 어느 정도인지를 나타낸다.


## MBU *model bandwidth utilization*

사용 가능한 메모리 대역폭 중 실제 쓰이는 비율을 측정한다.

- LM 추론에서 사용되는 메모리 대역폭

$$
= \text{파라미터 수} \times \text{파라미터 당 byte} \times \text{tokens/s}
$$

- **MBU**

$$
=\frac{\text{파라미터 수} \times \text{파라미터 당 byte} \times \text{tokens/s}}{\text{이론적 대역폭}}$$

- 파라미터당 byte 수가 줄어들면 모델이 메모리 대역폭을 덜 소모하므로, 양자화가 중요하다.



**학습 단계**에서는

추론 작업보다 작업 패턴이 **예측 가능하기 때문에** 더 효율적인 최적화를 적용할 수 있다. 따라서, 학습 MFU가 보통 추론 MFU보다 높다.


**추론 단계**에서는

- 프리필(프롬프트 처리) : 연산 중심 → MFU 높음

- 디코딩(토큰 생성 반복) : 메모리 대역폭 중심 → MFU 낮음

보통 Prefill MFU > Decoding MFU


![](https://velog.velcdn.com/images/algorithm_cell/post/86b0ba4c-9938-4dce-97e2-43baa2239fc1/image.png)

![](https://velog.velcdn.com/images/algorithm_cell/post/4cb7bac6-69d5-4b3f-b6c4-9e6e2c3884b4/image.png)

- 예시 모델들의 MBU가 50% 미만인 것을 확인할 수 있는데, 이는 사용자가 늘어나면 **연산 부하가 커져서 병목이 생긴 것**으로 보인다.
	
    - 동시 사용자 수가 증가하면 동시에 처리해야 하는 요청 시퀀스 수가 증가하게 된다.
    
    	- 디코딩 단계에서 한 스텝마다 필요한 연산 : QKᵀ, Softmax, MLP ...
       
       - 토큰 생성을 여러 사용자에 대해 반복하니까, 총 연산량(FLOPs/s) 증가
       
       
       
---


# 3) AI 가속기 

소프트웨어가 얼마나 빠르고 저렴하게 돌아가는지는 어떤 하드웨어에서 실행되느냐에 달려 있다.

## 정의


**가속기란 특정 종류의 연산 작업을 빠르게 처리하도록 만든 칩이다.**

AI 가속기는 AI 작업 전용으로 설계된다.

현재 가장 널리 쓰이는 AI 가속기는 GPU이며, 엔비디아가 중심이다.


## CPU vs GPU 

CPU와 GPU의 핵심 차이는 설계 목적이 다르다는 것이다.

1. **CPU**

- **강력한 코어** 몇 개로 구성

- 코어가 멀티스레드 작업을 효과적으로 처리할 수 있지만, 특히 단일 스레드 성능이 중요한 작업에 뛰어남
	
    - ex) 운영체제 실행, I/O 작업 관리, 복잡한 순차 프로세스 처리
 
2. **GPU**

- **약한 코어** 수천 개로 구성

- 그래픽 렌더링, ML처럼 작은 연산을 많이 나눠 병렬로 처리할 수 있는 작업에 최적화

- 대부분의 ML 연산은 병렬화가 쉬운 행렬곱 중심이므로 GPU로 처리


병렬 처리 효율을 극대화할수록 연산 능력은 늘지만, **메모리 설계가 어려워지고 전력 소비 문제도 커진다.**

![](https://velog.velcdn.com/images/algorithm_cell/post/d5c4037e-ed01-4bab-a44d-5b398242a098/image.png)

- 하드웨어 아키텍처마다 메모리 레이아웃과 특화된 **연산 유닛**은 다르다.
- **유닛**은 스칼라, 벡터, 텐서와 같은 특정 데이터 유형에 최적화되어 있다.



## 칩 평가요소

칩에는 자세한 정보가 많지만, 어떤 용도든 중요한 핵심 특성은 다음 3가지이다. 


### 1.연산 성능

연산 성능은 보통 **정해진 시간 동안 수행 가능한 연산 수**로 측정한다.

가장 대표 지표는 **FLOP/s**이지만 현실적으로 애플리케이션이 이론적인 FLOP/s를 달성하기는 거의 어렵다.

칩이 1초에 수행할 수 있는 연산 횟수는 **수치 정밀도(precision)**에 따라 달라진다.

정밀도가 높을수록 연산량이 증가하여 처리 가능 연산은 감소

![](https://velog.velcdn.com/images/algorithm_cell/post/90abdad8-2d13-4cec-837b-ef4780049367/image.png)



### 2. 메모리 크기 및 대역폭

- GPU는 코어가 병렬로 돌아가므로 메모리에서 코어로 데이터를 지속적으로 공급해야 한다.
- 특히 가중치/학습 데이터처럼 대량 데이터 전송이 필요한 AI 모델에서는 **데이터 전송 속도(대역폭)** 이 매우 중요하다.

GPU 메모리는 CPU 메모리보다 **넓은 대역폭, 낮은 지연시간, 고급 메모리 기술 필요**하다.


![](https://velog.velcdn.com/images/algorithm_cell/post/de9b6ae3-6ec2-4864-95d6-9e10e2bd412b/image.png)

GPU는 3단계 메모리와 상호작용한다.

1. **CPU DRAM**

    - 대역폭이 가장 낮다.

    - 속도는 약 25~50 GB/s으로 느린 편이다.

    - 용량은 다양하다.(일반 노트북 16~64GB / 워크스테이션 1TB 이상 가능)


2. **GPU HBM**
	
    - GPU 전용 메모리로 더 빠르게 접근하기 위해 GPU 가까이에 둔다.

    - 매우 높은 대역폭을 제공한다.

    - 속도는 약 256 GB/s ~ 1.5TB/s 이상으로 대량 데이터 전송과 높은 처리량 작업을 효율적으로 처리하는 데 꼭 필요하다.

    - 소비자용 GPU는 보통 24~80GB HBM 범위

3. **GPU 온칩 SRAM**

    - 칩 안에 들어 있는 메모리로, 자주 쓰는 데이터/지시를 저장하여 즉시 접근 가능하다.

    - L1/L2 캐시 포함, 일부는 L3도 포함

    - 속도는 10TB/s 이상으로 매우 빠르다.

    - 크기는 보통 40MB 이하로 매우 작다.


GPU 최적화의 상당 부분은 이 메모리 계층을 어떻게 최대한 활용하느냐에 달려 있다.

다만, 아직은 PyTorch/TensorFlow 같은 프레임워크 메모리 접근을 세밀하게 완전히 제어하기 어렵다.

그래서 많은 연구자/엔지니어가 NVIDIA CUDA, OpenAI Triton, AMD ROCm, 같은 GPU 프로그래밍 도구에 관심을 가지게 됐다.


### 3. 전력 소비

칩은 연산할 때 트랜지스터를 사용한다.

각 연산은 트랜지스터가 켜졌다 꺼졌다를 반복하며 이뤄지는데, 이 과정에서 **에너지가 필요하다.**
GPU에는 수십억 개의 트랜지스터가 들어있다. ex) A100에는 540억개, H100에는 800억개의 트랜지스터가 들어있다.

가속기를 제대로 활용하면 **수십억 개의 트랜지스터가 빠르게 상태를 바꾸며서 엄청난 양의 에너지를 소비하고 상당한 열을 발생시킨다.**

이를 냉각하는 냉각 시스템에도 전기가 필요하므로 데이터센터 전체 에너지 소비가 늘어난다.

**열 선계 전력 *Termal Design Power (TDP)***

- **최대 전략 소모량** : 칩이 최대 부하 상태에서 뽑아낼 수 있는 최대 전력을 의미한다.

- TDP : 칩이 일반적인 작업을 할 때 냉각 시스템이 방출해야하는 최대 열을 의미한다. 
	
    - 최대 전략 소모량은 일반적으로 **TDP의 1.1배 ~ 1.5배**이다.
