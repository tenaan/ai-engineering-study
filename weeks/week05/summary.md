# 통합 정리

**정리자**: 허채연

**날짜**: 26.01.20

---

## 주요 내용
모델이 작업을 처리하려면, 수행 방법에 대한 지시와 정보 모두 필요하다. 

사람이 정보가 부족할 때 잘못된  응답을 하기 쉬운 것 처럼, **AI 모델도 컨텍스트가 부족할 때 실수를 하거나 환각을 일으킬 가능성이 높다.** 

따라서, 이번 장에서는 각 질의에 적절한 컨텍스트를 구성하는 방법에 대해 다루고자 한다.


# 1) RAG





## 검색 알고리즘

**검색 *retrieval***은 일반적으로 하나의 데이터베이스나 시스템에 국한되는 반면, **search**는 다양한 시스템을 걸친 검색을 포함한다. 이 장에서는 검색과 서치를 구분없이 표현한다.


### 1. 용어 기반 검색 

**용어 기반 검색 방법**

1. **TF-IDF**

- TF (Term Frequency): 특정 용어가 문서 D에 얼마나 자주 등장하는가

- IDF (Inverse Document Frequency): 해당 용어가 전체 문서 집합에서 얼마나 희귀한가

$$
\text{IDF}(t) = \log \frac{N}{C(t)}
$$

$$
\text{Score}(D, Q) = \sum_{i=1}^{l} \text{IDF}(t_i) \times f(t_i, D)
$$



2. **BM25**

BM25는 **TF-IDF를 개선한 방식**으로 문서 길이를 고려해 정규화를 진행하는 방식이다.

문서가 길수록 특정 단어를 포함할 확률이 자연히 높아지기 때문에, 이를 보정하지 않으면 긴 문서가 과대평가될 수 있다. 


3. **엘라스틱서치**

**역색인 *Inverted Index***이라는 데이터 구조를 활용하는 방식이다.

역색인은 **용어를 문서로 매핑하는 사전**이다. 

따라서 이 사전 덕분에 어떤 용어가 주어졌을 때 관련 문서를 빠르게 찾을 수 있다.**용어 빈도나 해당 용어 포함 문서 개수 같은 추가 정보도 저장**할 수 있다. 

![](https://velog.velcdn.com/images/algorithm_cell/post/33758177-a8e9-4d60-b348-9cb0bfa32e65/image.png)

4. **토큰화**

- uni-gram : 개별 단어로 분리되면서 본래의 의미가 사라질 수 있다.

- n-gram : 같이 등장하는 토큰들이 있다면 묶어서 하나의 용어로 취급하는 방식이다.

- 토큰화 도구 : NLTK, spaCy, CoreNLP 등


**용어 기반 검색의 장점**

- 색인화와 질의 모두에서 일반적으로 임베딩 기반 검색보다 훨씬 빠르다. 용어 추출은 임베딩 생성보다 빠르고, 용어에서 그 용어를 포함하는 문서로의 매핑은 최근접 이웃 검색보다 계산 비용이 적을 수 있다.
- 별도의 설정 없이도 잘 작동한다. 

**용어 기반 검색의 단점**
- 단순하다. 단순함은 성능 향상을 위해 조정할 수 있는 구성 요소가 적다는 것을 의미하기도 한다.

### 임베딩 기반 검색

- 용어 기반 검색은 의미가 아닌 단어의 형태만 가지고 관련성을 계산한다.
  - 의미를 제대로 반영하지 못할 수 있기 때문에 사용자의 의도와 무관한 문서가 반환될 수 있다.
- **임베딩 기반 검색기**는 **문서의 의미가 질의와 얼마나 가까운지를 기준으로 순위를 매긴다.**


**의미 기반 검색(semantic retrieval)**
- 임베딩 기반 검색에서 색인화는 하는 일이 하나 더 있는데, 원본 데이터 청크를 임베딩으로 변환하는 것이다.
  
    - 이렇게 만들어진 임베딩을 저장하는 데이터베이스를 **벡터 데이터베이스**라고 부른다.

- 질의 과정은 두 단계로 이루어진다.
  
1. 임베딩 모델: 색인화에 사용된 것과 동일한 임베딩 모델을 사용해 **질의를 임베딩으로 변환**
2. 검색기: 질의 임베딩과 가장 가까운 k개의 데이터 청크를 가져온다. k값은 활용 사례, 생성 모델, 질의에 따라 달라진다.
![](https://velog.velcdn.com/images/dkan9634/post/dc03d3ea-1c90-44c0-8744-39052561dfc8/image.png)

- 이 임베딩 기반 검색 과정은 간소화된 것.
- 실제 의미 기반 검색 시스템에는 검색된 후보들의 순위를 다시 매기는 재순위 모듈이나 지연 시간을 줄이기 위한 캐시 같은 다른 구성 요소들도 포함될 수 있다.


> **임베딩**: 원본 데이터의 중요한 특성을 보존하는 것을 목표로 하는 벡터
>**벡터 데이터베이스**: 말 그대로 벡터를 저장하는 공간

- 단순히 벡터를 저장하는 것은 쉬운일이고 어려운 부분은 **벡터 검색**이다.
- 질의가 임베딩으로 변환되면, 벡터 데이터베이스는 이 질의 벡터와 매우 유사한 벡터들을 데이터베이스에서 찾아내야 한다.
- **그래서 벡터들은 빠르고 효율적인 검색이 가능한 방식으로 색인화되고 저장돼야 한다.**

  
- 벡터 검색은 보통 최근접 이웃 검색 문제로 접근한다.
    - **k-최근접 이웃(k-NN)** 알고리즘: 주어진 질의에 대해 k개의 가장 가까운 벡터를 찾는 것이다.
    1. 질의 임베딩과 데이터베이스의 모든 벡터 간의 유사도 점수를 코사인 유사도와 같은 지표를 사용해 계산
    2. 모든 벡터를 유사도 점수에 따라 순위를 매긴다.
    3. 높은 유사도 점수를 가진 상위 k개의 벡터를 반환한다.
    => 결과가 정확하지만 계산이 많이 필요하고 느림
    - 큰 데이터셋에서는 보통 **근사 최근접 이웃(ANN)** 알고리즘으로 벡터 검색을 수행한다.
    	- 벡터 검색 라이브러리 `FAISS`, `ScaNN`, `Annoy`, `Hnswlib`
      
- 벡터 DB는 벡터를 버킷, 트리 또는 그래프로 구성한다.
- 벡터 검색 알고리즘은 각각 다른 휴리스틱을 사용해 비슷한 벡터들이 저장 공간에서 서로 가까이 위치하도록 만든다.
  
- **벡터 검색 알고리즘**
    - 지역 민감 해싱(LSH)
    - 계층적 탐색이 가능한 소규모 세계(HNSW)
    - 제품 양자화
    - 역파일 색인(IVF)
    - 근사 최근접 이웃탐색(Annoy)

**임베딩 기반 검색의 장점**

- 용어 기반 검색보다 더 좋은 성능을 낼 수 있다. 

- 임베딩 모델과 검색기를 각각 따로 파인튜닝하거나, 둘을 함께 파인튜닝하거나, 아니면 생성 모델까지 포함해 전체적으로 파인튜닝할 수 있다. 

**임베딩 기반 검색의 단점**

- 그러나 데이터를 임베딩으로 변환하면 EADDRNOTAVAIL(99) 같은 특정 오류 코드나 제품 이름과 같은 키워드가 희석되어 나중에 검색하기 어려워질 수 있다.




### 용어 / 임베딩 비교

| 구분 | 용어 기반 검색 | 임베딩 기반 검색 |
|------|---------------|------------------|
| 질의 처리 속도 | 임베딩 기반 검색보다 훨씬 빠르다. | 질의 임베딩 생성과 벡터 검색으로 인해 느릴 수 있다. |
| 성능 | 일반적으로 초기 성능이 뛰어나지만 개선하기 어렵다. 용어의 모호성 때문에 잘못된 문서를 검색할 수 있다. | 파인튜닝을 통해 용어 기반 검색을 능가할 수 있다. |
| 비용 | 임베딩 기반 검색보다 훨씬 저렴하다. | 임베딩 생성, 벡터 저장, 그리고 벡터 검색 솔루션에 비용이 많이 들 수 있다. |

<br>

### 검색기의 품질 측정 지표

RAG 시스템 품질 평가를 위해서는 
**1. 검색 품질을 평가한다.**
**2. 최종 RAG 출력 결과를 평가한다.**
**3. 임베딩 기반 검색을 사용한다면 임베딩 품질도 평가한다.**


**데이터의 품질로 평가** 

- context precision : 검색된 모든 문서 중에서 실제로 질의와 관련된 문서의 비율은 얼마인가?

- context recall : 질의와 관련된 모든 문서 중 실제로 검색된 문서의 비율은 얼마인가?

**문서 순서가 중요한 경우 사용하는 지표**

- NDCG (Normalized Discounted Cumulative Gain)

- MAP (Mean Average Precision)

- MRR (Mean Reciprocal Rank)

**임베딩 기반 검색의 추가 평가 요소**

비슷한 문서가 벡터 공간에서 가까이 위치하면 좋은 임베딩이고, 독립적으로 품질 평가 가능하다.

- 대표 벤치마크: MTEB (Muennighoff et al., 2023) → 검색, 분류, 클러스터링 등 다양한 작업에서 임베딩 성능 평가

- 지연시간 : 질의 임베딩 생성과 백터 검새겡 따른 추가 지연 시간

- 비용 : 임베딩을 생성하는 드는 비용

**index와 질의 사이의 균형 유지**


- 색인이 상세할수록 **검색 과정은 더 정확해지지만**, **메모리 사용량이 많아지고 느려진다.**
	
    - ex) 고객 정보 색인에 이름, 회사, 이메일, 관심사까지 추가하면 검색은 쉬워지지만, 구축 시간, 저장 공간은 증가한다.

- HNSW 같은 상세한 색인은 높은 정확도와 빠른 질의 제공하지만, 구축하는데 상당한 시간과 메모리 필요

- LSH 같은 단순한 색인은 생성하는데 더 빠르고 메모리가 적게 들지만, 질의 속도가 느리고 정확도도 떨어진다.


**ANN 벤치마크 주요 지표**

- 재현율 (Recall) : 실제 최근접 이웃을 얼마나 잘 찾는가

- QPS (Queries Per Second) : 초당 처리 가능한 질의 수

- 구축 시간 : 색인 생성에 걸리는 시간 (데이터 변경 잦을수록 중요)

- 색인 크기 : 저장 공간 및 확장성 평가



### 하이브리드 검색(검색 알고리즘 결합 = 용어 기반 검색 + 임베딩 기반 검색)
    
**1) 하이브리드 검색의 2가지 대표 패턴**

- **A**. 병렬(Parallel) 하이브리드 + 퓨전(Fusion)
    
    A-1) 점수 기반 퓨전(가중합 / “비율” 방식)
    
    - 개념: 두 검색기가 만든 점수를 정규화한 뒤 가중합
    - 예시:
        - `final = α * norm(BM25_score) + (1-α) * norm(vector_score)`
    - 장점
        - 직관적(α로 “BM25 vs Dense 비중” 조절)
        - 튜닝하면 특정 도메인에서 잘 맞음
    - 주의점(매우 중요)
        - **스코어 스케일이 다름**(BM25 vs cosine/inner product) → **정규화 없으면 α가 의미 없어짐**
        - 실무에서 이 이유로 “점수 기반”보다 “순위 기반”을 더 선호하는 경우 많음
    
    A-2) 순위 기반 퓨전(Rank Fusion)
    
    - 개념: 점수 대신 **랭킹(등수)** 만 이용해 합친다
    - 대표: **RRF(Reciprocal Rank Fusion)**
        - 여러 랭킹에서 상위에 자주 등장하는 문서를 우대
    - 장점
        - 점수 스케일 문제를 피함
        - 구현/운영이 비교적 안정적
    - 단점
        - “비율(α)”처럼 정밀한 조절은 상대적으로 어려움(파라미터는 있지만 직관성이 덜함)
- B. 계단식(Cascade, 2-stage) 하이브리드
    
    비용이 싼 검색기로 **후보군을 넓게 생성**하고, 다음 단계에서 **더 비싸고 정확한 방식으로 재정렬/필터링**한다.
    
    - 대표 구조 예시
        - 1단계(cheap): BM25로 top-1000 후보 생성
        - 2단계(mid): 후보에 대해 dense similarity로 top-200 재정렬**[Reranking의 과정!!!]**
        - 3단계(expensive): **reranker**로 top-20 최종 정렬
    - 장점
        - 대규모 코퍼스에서 비용/지연시간을 통제하기 좋음
        - 상위 단계로 갈수록 “정확도”를 끌어올리는 설계가 쉬움
    - 단점
        - 파이프라인이 길어지고 복잡해짐(관측/튜닝 포인트 증가)

cf) 리랭커(Reranker) : 리랭커는 “쿼리-문서”를 더 정밀하게 비교해 **관련도 점수로 재정렬**하는 모듈

![image.png](Images/image.png)

(1) Cross-Encoder 리랭커 (가장 대표)

(2) Bi-Encoder / Late-Interaction 계열(리랭커로도 쓰임)

(3) LLM 기반 리랭커

(4) 규칙/학습 기반 랭킹(LTR) 리랭커




## 검색 최적화


### 1. 청킹 전략

**청크**란, 텍스트 데이터를 작은 조각으로 나누는 것을 말한다.

    
  ![스크린샷 2026-01-02 오후 5.34.54.png](Images/스크린샷_2026-01-02_오후_5.34.54.png)
  
  ![스크린샷 2026-01-02 오후 5.35.04.png](Images/스크린샷_2026-01-02_오후_5.35.04.png)

- 가장 단순한 전략은 특정 단위를 기준으로 문서를 **동일한 길이의 청크**로 나누는 것이다.

- 각 청크가 크기 안에 들어올 때까지 점점 작은 단위를 사용해 **문서를 재귀적으로 분할**할 수도 있다.

- 문서를 겹침 없이 나누면, 중요한 컨텍스트에서 끊겨 정보가 손실 될 수 있기 때문에, 청크 간의 겹침은 중요한 경계 정보가 최소한 하나의 청크에 포함되게 보장해준다.

- 임베딩 기반 접근법을 사용할 때 청크 크기는 임베딩 모델의 컨텍스트 제한을 넘으면 안된다. 

- 생성 모델의 토크나이저가 나누는 토큰을 기준으로 청킹할 수 있지만, 생성 모델을 바꾸면 데이터를 처음부터 다시 색인화 해야한다는 단점이 있다.


청크 크기가 작을수록 모델은 더 많은 청크를 포함할 수 있기 때문에 다양한 정보를 가질 수 있다.

하지만, 청크 단위가 너무 작으면 중요한 정보가 손실 될 수 있으며 계산 부담도 증가시킨다. 

따라서, 적당한 크기의 청크를 실험을 통해 찾아야한다.


### 2. 재순위화

- 검색기가 생성한 초기 문서 순위를 재순위한다.
- 모델의 컨텍스트에 맞추거나 입력 토큰 수를 줄이기 위해 검색된 문서 수를 줄여야 할 때 유용하다.
- 시간 기준으로 최신 데이터에 높은 가중치를 부여하는 전략
- 컨텍스트 재순위화는 항목의 정확한 위치가 덜 중요하다.

### 3. 질의 재작성

질의 재구성, 질의 정규화, 질의 확장 모두 질의 재작성에 포함된다.

- **전통적인 검색 엔진에서는 휴리스틱을 사용**해 질의 재작성을 수행하는 경우가 많다.

- **AI 애플리케이션에서는 프롬프트를 사용**해 다른 AI모델로 질의 재작성을 요청할 수 있다.

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/e69c72f7-4bd1-4fc8-bc1f-01c92510b082/image.png" />
<p align="center">다른 생성 모델에게 질의 재작성을 요청하는 예시</p>
	

### 4. 컨텍스트 검색

컨텍스트 검색의 핵심 아이디어는 **각 청크에 관련 컨텍스트를 추가해** 필요한 청크를 더 쉽게 검색할 수 있게 하는 것이다.

- 메타데이터(태그, 키워드)로 청크를 보강할 수 있다.
	
    - ex) 전자상거래의 경우 상품 설명과 리뷰를 함께 저장할 수 있다. 
    - ex) 이미지나 동영상을 제목이나 캡션을 통해 검색할 수 있다.
    
- 청크에서 자동으로 추출된 개체 또한 메타데이터에 포함될 수 있다.

- 각 청크에 실제로 물을 수 있는 질의들을 추가할 수 있다.

**문서를 여러 청크로 나누면, 일부 청크는 검색기가 내용을 이해하는 데 필요한 컨텍스트가 부족**해질 수 있다.

- 이를 방지하기 위해 원본 문서의 제목, 요약, 혹은 위치 정보를 각 청크에 추가한다.

- ex) Anthropic은 AI 모델을 활용해 각 청크의 위치를 알려주는 context로 보강해 컨텍스트 검색을 용이하게 했다.

![](https://velog.velcdn.com/images/algorithm_cell/post/3a845950-d742-4bd9-9446-7524f459f086/image.png)



### 검색 솔루션 평가하기
  **검색 솔루션을 평가할 때 고려해야 할 주요 사항**들은 다음과 같다.
  • 어떤 검색 방식을 지원하는가? 하이브리드 검색을 지원하는가?
  • 벡터 데이터베이스라면, 어떤 임베딩 모델과 벡터 검색 알고리즘을 지원하는가?
  • 데이터 저장량과 트래픽 측면에서 얼마나 확장 가능한가? 트래픽 패턴에 적합한가?
  • 데이터를 색인화하는 데 얼마나 시간이 걸리는가? 한 번에 얼마나 많은 데이터를 대량으로 처리(추가/삭제 등)할 수 있는가?
  • 다양한 검색 알고리즘에 대한 질의 지연 시간은 어느 정도인가?
  • 관리형 솔루션인 경우, 가격 체계는 어떻게 되는가? 저장된 문서/벡터 양에 따라 가격이 책정되는지, 아니면 검색 요청 횟수에 따라 책정되는가?
  
  이 목록에는 접근 권한 관리, 법규 준수, 데이터 레이어와 제어 레이어 분리 등 일반적인 기업용 솔루션이 갖추는 기능들은 포함되어 있지 않다.
  


## 멀티모달 RAG

### 1. 이미지 데이터

- 텍스트 데이터베이스 + 이미지 데이터베이스를 함께 사용해 질의에 답하는 RAG 구조

- 텍스트 정보만으로 부족한 질문을 이미지 정보로 보완할 수 있다.
![](https://velog.velcdn.com/images/algorithm_cell/post/1b629289-e1db-43d1-bb4e-498614909ddc/image.png)

**이미지 검색 방식**

1. **메타데이터 기반**

	- 이미지에 제목, 태그, 캡션 같은 텍스트 메타데이터가 있는 경우 **질의와 메타데이터의 텍스트 유사도**로 검색
    
2. **내용 기반 (멀티모달 임베딩)**

	- 이미지 내용 자체를 기준으로 검색하려면, 텍스트와 이미지를 같은 벡터 공간으로 변환해야 하기 때문에  멀티모달 임베딩 모델을 사용한다.
	
    - ex) **CLIP** : 텍스트와 이미지를 공통 임베딩 공간에 매핑해 **텍스트–이미지 간 유사도 계산이 가능**하다.

### 2. 표 형식 데이터
  
대부분의 애플리케이션은 텍스트, 이미지 같은 비정형 데이터뿐 만 아니라 표 형식(tabular) 데이터도 함께 처리한다.
  
표 형식 데이터로 context를 보강하는 과정은 일반적인 RAG workflow와 상당히 다르기 때문에 예시를 통해 살펴보자.
  
> EX)![](https://velog.velcdn.com/images/algorithm_cell/post/c98abf51-30d8-4c3e-aedb-04a5fb499046/image.png)
>
>
> 
> **질문 : “지난 7일 동안 과일 페도라가 몇 개 팔렸나요?”**
> 단순 검색으로는 답을 할 수 없기 때문에, **제품명이 과일 페도라인 주문 필터링, 최근 7일 조건 적용, 판매 수량 합산**을 거쳐야한다.
> **즉, 집계 연산이 필요하고, SQL로 조회한다.**
>
>**전체 파이프라인**
> - Text-to-SQL : 사용자 질의 + 테이블 스키마를 입력으로 받아 필요한 SQL 쿼리를 생성
> - SQL 실행 : 생성된 SQL을 실제 데이터베이스에서 실행
> - 응답 생성 : SQL 실행 결과를 바탕으로 응답 생성
>
> ![](https://velog.velcdn.com/images/algorithm_cell/post/bbf9e716-588e-4615-8a91-907462d93ced/image.png)
 
- 일반적인 표 형식 데이터 보강하기 위해서 (가져오기 위해서) DB의 경우 Text-to-SQL을 사용한다.
- 일반적인 문서에서 가져올 경우,
    - **VLM/멀티모달 기반:** PaddleOCR 등
    - **문서 파싱+구조화(레이아웃/OCR/테이블 추출) 기반**: Unstructured, Camelot 등

  [https://github.com/docling-project/docling](https://github.com/docling-project/docling)
        
---

# 2) 에이전트




---

# 3) 메모리

메모리는 모델이 정보를 저장하고 활용할 수 있게 하는 방식이다.

**RAG** 시스템은 검색된 정보로 컨텍스트를 보강하고, 이를 처리하기 위해 메모리를 사용한다.

**Agent** 기반 시스템은 지시, 예시, 컨텍스트, 도구 목록, 성찰 등을 저장하기 위해 메모리를 사용하다.

## AI 모델의 메모리

### 1. 내부 지식

- 모델이 학습한 데이터로부터 얻은 모든 지식을 의미한다.

- 모델이 새로 업데이트되지 않는 한 변하지 않는다.

- 모델은 모든 질의에 대해서 이 지식에 접근할 수 있다.

### 2. 단기 메모리

- 모델의 컨텍스트를 의미한다. 컨텍스틑 작업(질의)가 끝나면 사라지기 때문에, 단기 메모리라고 볼 수 있다.

- 이전 대화 내용을 컨텍스트에 추가하면 모델이 다음 응답을 생성할 때 이 정보를 활용한다.

- 접근 속도는 빠르지만, 용량이 한정되어 있기 때문에 **현재 작업에서 가장 중요한 정보들을 저장하는데 사용한다.**

### 3. 장기메모리

- 모델이 검색을 통해 접근할 수 있는 외부 데이터 소스를 의미한다. 이 정보는 작업이 끝나고 유지되므로, 장기 메모리로 볼 수 있다.

- 모델의 내부 지식과 달리, 장기 메모리의 정보는 모델 자체를 업데이트 하지 않고도 수정하거나 삭제할 수 있다.


## AI 메모리 핵심 기능

### 1. 메모리 관리

- 어떤 정보를 단기 및 장기 메모리에 저장할지 관리한다.

- 외부 메모리 저장 공간은 상대적으로 저렴하고 쉽게 확장할 수 있지만, 단기 메모리는 모델의 최대 컨텍스트 길이에 의해 제한되기 때문에 무엇을 추가하고 삭제할지에 대한 전략이 필요하다.


> ### 메모리 관리 전략
> **1. 선입선출 (FIFO)**
> : 가장 먼저 추가된 정보가 가장 먼저 외부 저장소로 이동하는 방식이다.
> : 긴 대화에서 이 전략은 **초기 메시지가 현재 대화와 덜 관련이 있다고 가정한다.**
> : 하지만, 첫 메시지가 가장 중요한 정보를 담고 있을 수 있기 때문에, FIFO는 구현하기 쉽지만, 모델이 중요한 정보를 놓치게 만들 위험이 있다.
>
>
> **2. 중복을 줄이기**
> : 중복을 자동으로 감지하는 방법이 있다면 메모리 사용량은 크게 줄어들 것이다.
> : 이를 위해 **대화의 요약**을 활용한다.
>
> **3. 성찰 분류기**
> : 각 행동을 한 후에, 에이전트는 두 가지 수행을 하도록 요청받는다.
>> 1. 방금 생성된 정보에 대해 성찰한다.
>> 2. 이 새로운 정봅가 추가되어야하는지, 병헙되어야 하는지, 대체해야 하는지 결정한다.


### 2. 메모리 검색

- 장기 메모리에서 작업과 관련된 정보를 검색한다.



## AI 메모리 장점

### 1. 세션 내 정보 과부화 관리 

- 에이전트는 작업 중 많은 정보를 얻게 되며, 컨텍스트 한계를 넘는 정보는 장기 메모리 시스템에 저장해 관리할 수 있다. 

### 2. 세션 간 정보 유지의 중요성

- 사용자의 이전 대화나 취향을 기억하면 반복 설명이 필요 없어진다. 이를 통해 개인화된 서비스 제공이 가능해진다.

### 3. 모델의 일관성 향상

- 이전 응답을 기억하면 동일하거나 유사한 질문에 대해 더 일관된 답변을 할 수 있다.

### 4. 데이터 구조 무결성 유지

- 텍스트 컨텍스트는 비정형이라 구조 보장이 어렵다. 따라서 엑셀, 큐 등 구조화된 메모리 시스템을 활용하면
데이터의 구조와 작업 순서를 안정적으로 관리할 수 있다.
