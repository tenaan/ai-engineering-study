
이번 장에서는 각 질의에 적절한 컨텍스트를 구성하는 방법에 초점을 맞춘다.

컨텍스트를 구성할 땐 주로 **검색 증강 생성(RAG)와 에이전트**, 이 두 가지 패턴을 사용한다. RAG 패턴은 모델이 외부 데이터 소스에서 관련 정보를 검색할 수 있고 주로 컨텍스트를 구성하는 데 사용된다. 반면 에이전트 패턴은 모델이 웹 검색이나 뉴스 API 같은 도구를 사용해 정보를 수집할 수 있게 한다.

---
# RAG
- 외부 메모리 소스에서 관련 정보를 검색해 모델의 생성을 향상시키는 기술
    - 외부 메모리 소스: 내부 DB, 사용자의 이전 채팅 세션, 또는 인터넷
- **검색 후 생성** 패턴 연구에서 시스템은 먼저 질의와 가장 관련성이 높은 5개의 위키피디어 페이지를 검색한 다음, 모델이 이 페이지의 정보를 사용하거나 읽어 응답을 생성한다.
![](https://velog.velcdn.com/images/dkan9634/post/34fd9131-e046-489c-82c4-76d539518811/image.png)
- **검색 증강 생성**이라는 용어를 처음 사용한 논문에서 RAG를 제안했다.이는 가용한 지식 전부를 모델에 직접 입력하기 어려운 집약적 작업을 해결하기 위한 방법이었다.
- RAG는 모든 질의에 동일한 컨텍스트를 사용하지 않고, 각 질의에 특화된 컨텍스트를 만들어내는 기술로 볼 수 있다. 
    - 이 기술은 사용자 데이터 관리에도 유용한데, **특정 사용자에 관한 질의가 들어올 때만 해당 사용자의 데이터를 컨텍스트에 포함하면 되기 때문**이다.
    

## RAG 아키텍처
- RAG 시스템은 외부 메모리 소스에서 정보를 검색하는 **검색기**와 검색된 정보를 기반으로 응답을 생성하는 **생성 모델**, 이 두 가지 구성 요소로 이루어져 있다.
![](https://velog.velcdn.com/images/dkan9634/post/0acd14b3-08e5-4d08-b0e1-aae73823aaed/image.png)

- 요즘엔 이 두 요소를 따로 학습 시키는 경우가 많다.
- RAG 시스템의 성공은 **검색기의 품질**에 달려 있다.
    - 검색기의 기능
        - 색인화: 나중에 빠르게 검색할 수 있도록 데이터를 처리하는 작업
        - 질의: 관련 데이터를 검색하기 위해 시스템에 요청을 전송하는 과정

- 문서 전체를 그냥 검색하면 컨텍스트가 지나치게 길어질 수 있다. 이를 방지하기 위해 각 문서를 더 관리하기 쉬운 **청크(chunk)**로 분할한다.
- 각 질의의 목표는 **가장 관련성 높은 데이터 청크를 검색하는 것**이다.
    - 이렇게 찾아낸 데이터 청크를 사용자 프롬프트와 합치기 위해서 약간의 후처리 작업이 필요하다. 이렇게 만들어진 최종 프롬프트가 생성 모델에 입력된다.
> 이 장에서는 문서와 청크를 모두 문서라고 부른다.

## 검색 알고리즘
- **검색**은 주어진 질의에 대한 문서들의 관련성을 기준으로 **순위를 매기는 방식**으로 작동한다.
- 따라서 검색 알고리즘은 관련성 점수를 계산하는 방법에 따라 달라진다.

> 희소 벡터: 대부분의 값이 0인 벡터
> - 용어 기반 검색이 희소 방식으로 분류(원핫 벡터로 표현 가능)

> 밀집 벡터: 대부분의 값이 0이 아닌 벡터
> - 임베딩 기반 검색은 밀집 방식으로 분류(임베딩이 대체로 값이 꽉찬 밀집 벡터이기 때문) 

### 용어 기반 검색
- 질의가 주어지면 관련 문서를 찾는 가장 간단한 방법은 **키워드를 사용**하는 것
    - 이런 접근 방식을 **어휘적 검색**이라고 부른다.
- TF-IDF는 용어 빈도인 TF와 역문서 빈도인 IDF를 결합한 알고리즘
- 자주 사용되는 두 가지 용어 기반 솔루션: **엘라스틱서치**와 **BM25**
    - 엘라스틱서치
        - **역색인**이라는 데이터 구조를 활용한다. 
        - 이는 용어를 문서로 매핑하는 사전이다. 이 사전 덕분에 어떤 용어가 주어졌을 때 관련 문서들을 빠르게 찾을 수 있다.
        - 용어 빈도나 해당 용어를 포함한 문서의 개수인 문서 같은 추가 정보도 저장할 수 있다.
        => 이런 정보는 TF-IDF 점수 계산할 때 유용하다.
        ![](https://velog.velcdn.com/images/dkan9634/post/e40efcfa-45b1-4f1b-bc34-9416b48b30b9/image.png)
    - BM25
        - 베스트 매칭 알고리즘의 25번째 세대
        - TF-IDF를 개선한 점수 계산 방식이다.
        - TF-IDF와 비교해, BM25는 **문서 길이를 고려해 용어 빈도 점수를 정규화**한다. 즉 긴 문서일수록 특정 용어를 포함할 가능성이 높고 용어 빈도 값도 자연히 높아지기 때문이다.
        - 임베딩 기반 검색 같은 더 현대적이고 정교한 검색 알고리즘과 비교할 때 중요한 기준점이 된다.
- 토큰화로 개별 단어가 분리되며 본래의 의미가 사라질 수 있으니 자주 등장하는 **n-gram**을 하나의 용어로 취급하는 방법도 있다.
- 또한 모든 문자를 소문자로 바꾸고 구두점을 없애고 불용어를 제거할 수 있다.

### 임베딩 기반 검색
- 용어 기반 검색은 의미가 아닌 단어의 형태만 가지고 관련성을 계산한다. 의미를 제대로 반영하지 못할 수 있기 때문에 사용자의 의도와 무관한 문서가 반환될 수 있다.
- **임베딩 기반 검색기**는 **문서의 의미가 질의와 얼마나 가까운지를 기준으로 순위를 매긴다.**
=> **의미 기반 검색(semantic retrieval)**
- 임베딩 기반 검색에서 색인화는 하는 일이 하나 더 있는데, 원본 데이터 청크를 임베딩으로 변환하는 것이다.
    - 이렇게 만들어진 임베딩을 저장하는 데이터베이스를 **벡터 데이터베이스**라고 부른다. 
- 질의 과정은 두 단계로 이루어진다.
1. 임베딩 모델: 색인화에 사용된 것과 동일한 임베딩 모델을 사용해 **질의를 임베딩으로 변환**
2. 검색기: 질의 임베딩과 가장 가까운 k개의 데이터 청크를 가져온다. k값은 활용 사례, 생성 모델, 질의에 따라 달라진다.
![](https://velog.velcdn.com/images/dkan9634/post/dc03d3ea-1c90-44c0-8744-39052561dfc8/image.png)

-> 이 임베딩 기반 검색 과정은 간소화된 것. 실제 의미 기반 검색 시스템에는 검색된 후보들의 순위를 다시 매기는 재순위 모듈이나 지연 시간을 줄이기 위한 캐시 같은 다른 구성 요소들도 포함될 수 있다.

> **임베딩**: 원본 데이터의 중요한 특성을 보존하는 것을 목표로 하는 벡터
>**벡터 데이터베이스**: 말 그대로 벡터를 저장하는 공간

- 단순히 벡터를 저장하는 것은 쉬운일이고 어려운 부분은 **벡터 검색**이다. 질의가 임베딩으로 변환되면, 벡터 데이터베이스는 이 질의 벡터와 매우 유사한 벡터들을 데이터베이스에서 찾아내야 한다. **그래서 벡터들은 빠르고 효율적인 검색이 가능한 방식으로 색인화되고 저장돼야 한다.**
- 벡터 검색은 보통 최근접 이웃 검색 문제로 접근한다.
    - **k-최근접 이웃(k-NN)** 알고리즘: 주어진 질의에 대해 k개의 가장 가까운 벡터를 찾는 것이다.
    1. 질의 임베딩과 데이터베이스의 모든 벡터 간의 유사도 점수를 코사인 유사도와 같은 지표를 사용해 계산
    2. 모든 벡터를 유사도 점수에 따라 순위를 매긴다.
    3. 높은 유사도 점수를 가진 상위 k개의 벡터를 반환한다.
    => 결과가 정확하지만 계산이 많이 필요하고 느림
    - 큰 데이터셋에서는 보통 **근사 최근접 이웃(ANN)** 알고리즘으로 벡터 검색을 수행한다.
    	- 벡터 검색 라이브러리 `FAISS`, `ScaNN`, `Annoy`, `Hnswlib` 
- 벡터 DB는 벡터를 버킷, 트리 또는 그래프로 구성한다.
- 벡터 검색 알고리즘은 각각 다른 휴리스틱을 사용해 비슷한 벡터들이 저장 공간에서 서로 가까이 위치하도록 만든다.
- **벡터 검색 알고리즘**
    - 지역 민감 해싱(LSH)
    - 계층적 탐색이 가능한 소규모 세계(HNSW)
    - 제품 양자화
    - 역파일 색인(IVF)
    - 근사 최근접 이웃 탐색(Annoy)
    
### 검색 알고리즘 비교
- **용어 기반 검색**
    - 색인화, 질의 모두에서 일반적으로 임베딩 기반 검색보다 **빠르다**
    - 별도의 설정 없이도 잘 작동한다. 엘라스틱서치와 BM25 같은 솔루션들은 많은 검색 및 정보 검색 애플리케이션을 성공적으로 지원해왔다. 그러나 이런 단순함은 성능 향상을 위해 조정할 수 있는 구성 요소가 적다는 것을 의미하기도 한다.

- **임베딩 기반 검색**
    - 시간이 지나면서 상당히 개선돼 용어 기반 검색보다 더 **좋은 성능**
    - 임베딩 모델과 검색기를 각각 따로 파인튜닝하거나, 둘을 함께 파인튜닝하거나, 아니면 생성 모델까지 포함해 전체적으로 파인튜닝할 수 있다.
    - 그러나 데이터를 임베딩으로 변환하면 `EADDRNOTAVAIL(99)` 같은 특정 오류 코드나 제품 이름과 같은 키워드가 희석되어 나중에 검색하기 어려워질 수 있다.

    => 이런 한계는 이 장의 뒷부분에서 설명할 **용어 기반 검색과 임베딩 기반 검색의 결합**으로 해결할 수 있다.
    
- 검색기의 품질은 검색되는 데이터의 품질로 평가할 수 있다.
- RAG 평가 프레임워크에서 주로 사용되는 두 가지 지표
1. **컨텍스트 정밀도**: 검색된 모든 문서 중에서 실제로 질의와 관련된 문서의 비율은 얼마인가?
2. **컨텍스트 재현율**: 질의와 관련된 모든 문서 중에서 실제로 검색된 문서의 비율은 얼마인가?
- 만약 검색된 문서의 순위가 중요할 땐, 더 관련성 높은 문서가 상위에 나와야 한다면 **정규화된 할인 누적 이득(NDCG),  평균 정밀도(MAP), 평균 역순위(MRR)** 같은 지표 사용
![](https://velog.velcdn.com/images/dkan9634/post/d3448791-6a10-4211-8365-258dfa98632f/image.png)
- ANN-벤치마크 웹사이트는 색인화와 질의의 균형을 고려해 네 가지 주요 지표로 여러 데이터셋에서 다양한 ANN 알고리즘을 비교한다. 
    - **재현율**: 알고리즘이 찾은 최근접 이웃의 비율
    - **초당 질의 수(QPS)**: 알고리즘이 초당 처리할 수 있는 질의 수. 이는 트래픽이 많은 애플리케이션에서 중요하다.
    - **구축 시간**: 색인을 구축하는 데 필요한 시간. 이 지표는 특히 색인을 자주 업데이트해야하는 경우(데이터 변경) 중요하다.
    - **색인 크기**: 알고리즘이 생성한 색인의 크기로, 확장성과 저장 공간 요구사항을 평가하는 데 중요하다.
> RAG 시스템의 품질은 구성 요소와 전체 시스템 모두에서 평가해야 한다. 이를 위해 다음과 같은 작업을 수행해야 한다.
> 1. 검색 품질 평가
> 2. 최종 RAG 출력 결과 평가
> 3. 임베딩 기반 검색을 사용한다면 임베딩 품질 평가

### 검색 알고리즘 결합하기
- 용어 기반 검색과 임베딩 기반 검색을 결합하는 것을 **하이브리드 검색**이라고 한다.
- 서로 다른 알고리즘은 **순차적으로 사용**할 수 있다. 먼저 용어 기반 시스템처럼 비용이 적게 들지만 정확도가 낮은 검색기로 후보군을 추려내고, 그 다음 k-최근접 이웃과 같이 더 정확하지만 비용이 더 많이 드는 방식으로 이 후보 중에서 최상의 결과를 찾는다.
=> 이 두 번째 단계를 **재순위화**라고도 한다.
- 또한, 서로 다른 알고리즘을 **앙상블 기법**으로 결합할 수도 있다.
> 검색기는 질의에 대한 관련성 점수에 따라 문서의 순위를 매기는 방식으로 작동한다는 점을 기억하자. 여러 검색기를 동시에 사용해 후보를 가져온 다음, 이 다양한 순위들을 하나로 결합해 최종 순위를 생성할 수 있다.

- 서로 다른 순위를 결합하는 알고리즘을 **역순위 퓨전(RRF)**라고 한다.
    - 이 방식은 검색기가 매긴 순위에 따라 각 문서에 점수를 부여한다.
    
## 검색 최적화
작업 특성에 따라, 관련 문서가 검색된 가능성을 높이는 전략들이 있다. 여기서 논의할 네 가지 전략은 **청킹 전략, 재순위화, 질의 재작성, 컨텍스트 검색**이다.

### 청킹 전략
- 가장 단순한 전략은 **특정 단위를 기준으로 문서를 동일한 길이의 청크로 나누는 것**이다.
    - 주로 문자, 단어, 문장, 단락 같은 단위를 사용한다.
- **각 청크가 최대 청크 크기 안에 들어올 때까지 점점 작은 단위를 사용해 재귀적으로 분할**할 수도 있다. 
     - 예를 들어, 문서를 여러 절로 나누고, 절이 너무 길면 단락으로, 단락이 여전히 길면 문장으로 나누는 식이다.
     - 이렇게 하면 관련 내용이 임의로 끊길 가능성이 줄어든다.
 - 문서를 겹침 없이 청크로 나누면, 중요한 컨텍스트에서 끊겨 중요한 정보가 손실될 수도 있다.
     - **청크 간 겹침**은 중요한 경계 정보가 최소한 하나의 청크에 포함되도록 보장할 수 있다.
 - **생성 모델의 토크나이저가 나누는 토큰을 기준으로 문서를 청킹**하는 방법도 있다.
 - 청크 크기가 너무 작으면 중요한 정보가 손실될 수 있고 계산 부담도 증가시킨다.
     - 청크 크기를 절반으로 줄이면 색인화할 청크가 두 배로 늘어나고, 생성하고 저장해야 할 임베딩 벡터도 두 배가 된다. 벡터 검색 공간이 두 배로 커지면 질의 속도가 느려질 수 있다.
 
 ### 재순위화
 - **재순위화**는 모델의 컨텍스트에 맞추거나 입력 토큰 수를 줄이기 위해 검색된 문서 수를 줄여야 할 때 특히 유용하다.
 - 먼저 비용이 적게 들지만 정확도가 낮은 검색기가 후보를 가져오고, 그 다음 더 정확하지만 비용이 많이 드는 메커니즘이 이런 후보들을 재순위화한다. (위 내용 `검색 알고리즘 결합하기` 절에서 언급함)
 - 문서는 최신 데이터에 더 높은 가중치를 부여한다.
    - 뉴스 수집, 이메일 채팅, 주식 시장 분석 같은 시간에 민감한 애플리케이션에 유용하다.
- **컨텍스트 재순위화는 항목의 정확한 위치가 덜 중요하다는 점에서 전통적인 검색 재순위화와 다르다.**
    - 일반 검색에서는 순위가 중요하다.

### 질의 재작성
- 질의 재구성, 질의 정규화, 질의 확장이라고도 한다.
- 사용자의 실제 의도를 반영해 질의를 재작성하는 것이다. 새롭게 작성한 질의는 별도의 컨텍스트 없이도 의미가 명확해야 한다.
- 전통적인 검색 엔진에서는 휴리스틱을 사용해 질의 재작성을 수행하는 경우가 많다.
- AI 애플리케이션에서는 `다음 대화를 고려할 때, 사용자가 실제로 묻고 있는 내용을 반영하도록 마지막 사용자 입력을 재작성하세요`와 같은 프롬프트를 사용해 다른 AI 모델로 질의 재작성을 수행할 수 있다.
![](https://velog.velcdn.com/images/dkan9634/post/06a36afd-0964-41fa-a770-653f4bba7f9a/image.png)

-> 위 프롬프트를 사용하여 챗GPT가 질의를 어떻게 재작성했는지 보여준다.
+) 질의 재작성은 신원 확인이나 다른 지식을 통합해야 하는 경우 복잡해질 수 있다. 
ex) 그의 아내가 어떻냐고 물어봤을 때, 데이터베이스에서 그의 아내가 누구인지 찾아야 한다. 이런 정보가 없다면 재작성 모델이 이름을 무작위로 지어낼 수 없도록 질의에 대한 답을 찾을 수 없다고 명확히 알려야 한다.

### 컨텍스트 검색
- 핵심 아이디어: ** 각 청크에 관련 컨텍스트를 추가**해 필요한 청크를 더 쉽게 검색할 수 있게 하는 것
- 간단한 방법은 태그나 키워드 같은 메타데이터로 청크를 보강하는 것이다. ex) 상품에 설명과 리뷰를 함께 저장, 이미지와 동영상은 제목이나 캡션을 통해 검색
- 각 청크에 응답할 수 있는 질의들을 추가할 수도 있다.
- 문서가 여러 청크로 나뉘면, 일부 청크는 검색기가 그 내용을 이해하는 데 필요한 컨텍스트가 부족할 수 있다. 이를 방지하기 위해 **원본 문서의 제목이나 요약 같은 정보**를 각 청크에 추가할 수 있다.
- 엔트로픽은 AI 모델을 사용해 청크와 원본 문서의 관계를 설명하는 **숏 컨텍스트**(보통 50~100 토큰)를 생성했다.
다음은 앤트로픽이 이 목적으로 사용한 프롬프트다.
```text
<document>
{{WHOLE_DOCUMENT}}
</document>
여기에 전체 문서 내에서 위치시키고 싶은 청크가 있습니다:
<chunk>
{{CHUNK_CONTENT}}
</chunk>
청크의 검색 검색을 개선하기 위해 이 청크를 전체 문서 내에서
위치시킬 수 있는 짧고 간결한 컨텍스트를 제공해 주세요. 
간결한 컨텍스트만 응답하고 그 외에는 아무것도 응답하지 마세요.

```
각 청크에 대해 생성된 컨텍스트는 각 청크 앞에 추가
보강된 청크는 검색 알고리즘에 의해 색인화된다.



![](https://velog.velcdn.com/images/dkan9634/post/f7e7c0d8-e9d8-4bba-8ac8-449c7cf620fd/image.png)

-> 앤트로픽이 따르는 프로세스를 시각화

#### 검색 솔루션 평가하기
**검색 솔루션을 평가할 때 고려해야 할 주요 사항**들은 다음과 같다.
• 어떤 검색 방식을 지원하는가? 하이브리드 검색을 지원하는가?
• 벡터 데이터베이스라면, 어떤 임베딩 모델과 벡터 검색 알고리즘을 지원하는가?
• 데이터 저장량과 트래픽 측면에서 얼마나 확장 가능한가? 트래픽 패턴에 적합한가?
• 데이터를 색인화하는 데 얼마나 시간이 걸리는가? 한 번에 얼마나 많은 데이터를 대량으로 처리(추가/삭제 등)할 수 있는가?
• 다양한 검색 알고리즘에 대한 질의 지연 시간은 어느 정도인가?
• 관리형 솔루션인 경우, 가격 체계는 어떻게 되는가? 저장된 문서/벡터 양에 따라 가격이 책정되는지, 아니면 검색 요청 횟수에 따라 책정되는가?

이 목록에는 접근 권한 관리, 법규 준수, 데이터 레이어와 제어 레이어 분리 등 일반적인 기업용 솔루션이 갖추는 기능들은 포함되어 있지 않다.


## 텍스트를 넘어선 RAG
앞에서는 외부 데이터 소스가 텍스트 문서인 텍스트 기반 RAG 시스템을 살펴봤다. 하지만 외부 데이터 소스가 텍스트만 있지 않다.

### 멀티모달 RAG
- 생성 모델이 멀티모달 데이터를 처리할 수 있다면, 그 컨텍스트는 텍스트 문서뿐만 아니라 외부 소스의 이미지, 비디오, 오디오 등도 활용할 수 있다. 
- 질의가 주어지면, 검색기는 관련된 텍스트와 이미지를 모두 가져온다. 
![](https://velog.velcdn.com/images/dkan9634/post/c7d69cc9-98ae-4f93-b47f-a2600b726352/image.png)

- 이미지에 제목, 태그, 캡션 같은 메타 데이터가 있다면, 이 메타데이터를 사용해 검색할 수 있다. 예를 들어, 이미지의 캡션이 질의와 관련 있다고 판단되면 그 이미지가 검색 결과로 나온다.
- 이미지 내용을 기반으로 검색하고 싶다면, 이미지와 질의를 비교할 방법이 필요하다. 질의가 텍스트 형태라면, 이미지와 텍스트 모두를 벡터로 변환할 수 있는 멀티모달 임베딩 모델이 필요하다. ex) CLIP
    1. 텍스트와 이미지를 포함한 모든 데이터의 CLIP 임베딩을 생성하고, 이를 벡터 데이터베이스에 저장한다.
    2. 질의가 주어지면, 그에 대한 CLIP 임베딩을 생성한다.
    3. 벡터 데이터베이스에서 질의 임베딩과 가장 유사한 이미지와 텍스트를 찾는다.
    
### 표 형식 데이터를 활용한 RAG
- 대부분의 애플리케이션은 텍스트, 이미지 같은 비정형 데이터뿐만 아니라 표 형식 데이터도 처리한다. 많은 질의에 응답하기 위해 표에 담긴 정보가 필요할 수 있다.
- 표 형식 데이터로 컨텍스트를 보강하는 과정은 일반적인 RAG 워크플로와 상당히 다르다.

![](https://velog.velcdn.com/images/dkan9634/post/866a9abc-01a9-4463-8629-aff92f38976a/image.png)

-> ** 자연어 질문을 SQL로 바꿔 실제 데이터베이스를 조회하고,
그 결과를 바탕으로 LLM이 정확한 답변을 생성하는 구조를 보여준다. **

---
# 에이전트

많은 사람이 AI의 궁극적인 목표로 여기는 것이 바로 **지능형 에이전트**다.

## 에이전트 개요
- **에이전트**는 자신의 환경을 인식하고 그 환경에서 행동할 수 있는 모든 것을 말한다. 즉, 작동하는 환경과 수행할 수 있는 행동들로 정의된다.
- AI 에이전트가 수행할 수 있는 행동 집합들은 접근할 수 있는 도구에 의해 확장된다.
    - 웹을 검색하고 파이썬 코드를 실행하고 이미지를 생성할 수 있는 ChatGPT, RAG 시스템, 텍스트 검색기, 이미지 검색기 SQL 실행기 같은 도구도 다 에이전트다.
- 에이전트의 환경과 도구들 사이에는 강한 의존성이 있다.
    - 환경은 에이전트가 잠재적으로 사용할 수 있는 도구를 결정한다.
    - 에이전트의 도구 목록은 활동할 수 있는 환경을 제한한다.

![](https://velog.velcdn.com/images/dkan9634/post/89af01c4-9a30-4b23-b13c-aabd40bd3582/image.png)

-> GPT-4 위에 구축된 에이전트인 SWE-agent의 시각화
-> 이 에이전트는 터미널과 파일 시스템이 있는 컴퓨터를 환경으로 사용한다. 에이전트가 할 수 있는 행동의 집합에는 저장소 탐색, 파일 검색, 파일 보기, 줄 편집 등이 포함된다.

- 에이전트는 두 가지 이유로 더 강력한 모델이 필요하다.
1. **누적되는 오류**: 여러 단계를 거쳐야 하는데, 단계 수가 증가할수록 전체 정확도는 감소한다.
2. **더 큰 위험성**: 도구에 사용할 수 있게 되면서 에이전트는 더 영향력 있는 작업을 수행할 수 있지만, 실패할 경우 더 심각한 결과를 초래할 수 있다.

### 도구
에이전트의 도구 목록은 에이전트가 할 수 있는 일을 결정하기 때문에, 에이전트에게 어떤 도구를 얼마나 제공할지 신중히 고려해야 한다.
도구가 많을수록 에이전트의 능력은 향상되지만, 그만큼 이를 제대로 이해하고 효과적으로 활용하는 것이 더 어려워진다.

고려해볼 만한 세 가지 도구 유형은 다음과 같다. **지식 증강(ex. 컨텍스트 구성), 능력 확장, 에이전트가 환경에 직접 변화를 줄 수 있는 도구들**이다.

### 계획 수립
![](https://velog.velcdn.com/images/dkan9634/post/c30d2e65-4675-41a6-bae3-5ab5fa1c3ab8/image.png)

 작업을 해결하는 과정은 일반적으로 다음과 같은 단계로 이루어진다.
 (성찰은 반드시 필요한 건 아니지만, 성능을 크게 향상시킨다.)
 1. **계획 생성**
 2. **성찰과 오류 수정**: 생성된 계획을 평가 후 계획이 좋지 않다면 새로운 계획 생성
 3. **실행**: 생성된 계획에 따라 행동을 수행, 주로 특정 함수를 호출하는 방식
 4. **성찰과 오류 수정**: 행동 결과를 평가하고 목표가 달성됐는지 확인한다. 오류가 있다면 찾아서 수정하고, 목표가 완료되지 않았다면 새로운 계획을 세운다.
 
 
 - 에이전트의 계획 수립 능력을 향상시키기 위한 방법들
     - 더 많은 예시가 포함된 더 나은 시스템 프롬프트 작성
     - 모델이 더 잘 이해할 수 있도록 도구와 파라미터에 대해 더 자세히 설명
     - 복잡한 함수를 두 개의 더 간단한 함수로 나누는 등, 함수 자체를 더 단순하게 만듦
     - 더 강력한 모델을 사용
     - 계획 생성을 위해 모델을 파인튜닝
     
     
![](https://velog.velcdn.com/images/dkan9634/post/3683e456-b132-417a-a145-12c906600075/image.png)

#### 성찰과 오류 수정
- 성찰은 자기 비평 프롬프트를 활용해 동일한 에이전트가 수행할 수 있고, 각 결과에 구체적인 점수를 매기는 전문 평가 모델과 같은 별도의 구성 요소를 통해 이루어질 수도 있다.
![](https://velog.velcdn.com/images/dkan9634/post/18f10b51-4619-4fd2-8639-3cec454db566/image.png)

- 성찰은 멀티 에이전트 설정에서도 구현할 수 있는데, 한 에이전트는 계획을 세우고 행동을 취하고, 다른 에이전트는 각 단계나 여러 단계 이후에 결과를 평가하는 방식

![](https://velog.velcdn.com/images/dkan9634/post/3dd4cc83-6346-4d50-b59c-fad2d6deb39a/image.png)

-> 실제로 작동하는 리플렉션 에이전트의 예시를 보여주는데 여기선 계획을 지칭하기 위해 **궤적**이라는 용어를 사용했다. **각 단계에서 평가와 성찰 후, 에이전트는 새로운 궤적을 제안**한다.

- 계획 생성과 비교했을 때, 성찰은 상대적으로 구현하기 쉽고 놀랍도록 높은 성능 향상을 가져올 수 있다. 이 접근 방식의 **단점은 지연 시간과 비용**이다.


#### 도구 선택
최상의 도구 세트를 선택하는 완벽한 방법은 없다.
`툴포머`는 다섯 가지 도구를 사용하도록 GPT-J를 파인튜닝 했다.
`고릴라`는 에이전트가 1645개의 API 중에서 적절한 것을 선택하게 했다.

도구가 많을수록 에이전트의 능력은 향상되지만, 그만큼 이를 효율적으로 사용하긴 어려워진다. 이는 사람이 너무 많은 도구를 익히기 어려운 것과 비슷하다.

AI 애플리케이션 개발 과정의 다른 결정과 마찬가지로, 도구 선택에는 실험과 분석이 필요하다. 결정에 도움이 될 수 있는 몇 가지 방법은 다음과 같다.
- 여러 도구 조합으로 에이전트의 성능을 비교한다.
- 특정 도구를 제거했을 때 에이전트의 성능이 얼마나 떨어지는지 확인하는 제거 연구를 수행한다. 성능 저하 없이 제거할 수 있는 도구가 있다면 제외한다.
- 에이전트가 자주 실수하는 도구를 찾아본다. 광범위한 프롬프팅이나 파인튜닝에도 에이전트가 제대로 사용하지 못하는 도구가 있다면, 그 도구를 교체한다.
- 도구 호출 분포를 시각화하여 가장 많이 사용되는 도구와 거의 사용되지 않는 도구를 파악한다. 
아래 그림은 GPT-4와 ChatGPT의 도구 사용 패턴 차이를 보여준다.
![](https://velog.velcdn.com/images/dkan9634/post/ce186c60-9ba9-4553-aa45-5d1343123327/image.png)

## 에이전트 실패 유형과 평가
평가란 실패를 발견하는 과정
에이전트를 평가하려면, 실패 유형을 파악하고 각 유형이 얼마나 자주 발생하는지 측정해야 한다.
다양한 실패 유형을 보여주는 `버클리 함수 호출 리더보드`, `AgentOps 평가 도구`,  `TravelPlanner 벤치마크` 같은 에이전트 벤치마크와 리더보드도 있다.

### 계획 수립 실패
- 가장 흔한 계획 수립 실패 유형은 **도구 사용 실패**다.
    - 유효하지 않은 도구
    - 유효한 도구, 유효하지 않은 파라미터
    - 유효한 도구, 잘못된 파라미터 값
- 계획 수립 실패의 또 다른 유형은 에이전트가 목표를 이루지 못하는 **목표 달성 실패**다.
- 이외에 주목할 만한 계획 수립 실패 유형은 **성찰 오류**
    - 에이전트가 실제로는 작업을 완료하지 못했는데도 완료했다고 확신하는 것이다. 
- 계획 수립 실패에 대해 에이전트를 평가하기 위한 방법은 **각 예시가 (작업, 도구 목록) 쌍으로 구성된 계획 수립 데이터셋을 만드는 것**이다.
    -  그리고 각 작업에 대해 에이전트를 사용해 K개의 계획을 생성한 다음, 유효한 계획의 비율, 유효한 호출의 비율 등의 지표를 계산해본다.
    
    
### 도구 실패
- 올바른 도구를 사용했지만 그 결과가 잘못된 경우에 발생한다.
- 한 가지 실패 유형은 도구 자체가 단순히 잘못된 출력을 내놓는 경우다.
- 도구 실패는 도구마다 다르게 나타나기 때문에 각 도구는 개별적으로 테스트되어야 한다.

---
# 메모리
- RAG와 에이전트는 메모리가 더 많이 필요하지만, 정보를 유지해야 하는 모든 AI 애플리케이션에서 메모리는 유용하게 쓰인다.

- AI 모델의 메모리는 크게 세 가지로 나뉜다.
1. **내부 지식** : 모델은 학습한 데이터에서 얻은 모든 지식을 저장하고 있기 때문에, 그 자체로 하나의 기억 시스템이라고 할 수 있다. 모델이 새로 업데이트되지 않는 한 변하지 않는다. 모델은 모든 질의에서 이 지식에 접근할 수 있다.
2. **단기 메모리** : 모델의 컨텍스트를 기억한다. 다만 작업(질의)이 끝나면 사라지므로 단기 메모리이다. 접근 속도는 빠르지만 용량이 한정되어 있어, 주로 현재 작업에 가장 중요한 정보를 저장하는 데 사용한다.
3. **장기 메모리**: RAG 시스템처럼 모델이 검색을 통해 접근할 수 있는 외부 데이터 소스도 하나의 메모리로 사용할 수 있다. 이 정보는 작업이 끝나도 유지되므로 모델의 장기 메모리로 볼 수 있다. 모델의 내부 지식과 달리, 모델 자체를 업데이트하지 않고도 수정하거나 삭제 가능하다.

![](https://velog.velcdn.com/images/dkan9634/post/0ca564ba-e868-4ec7-a6a6-9e19f08c1b41/image.png)

- AI 모델에 메모리 시스템을 추가하면 다음과 같은 이점이 있다.
    - 세션 내 정보 과부하 관리
    - 세션 간 정보 유지
    - 모델의 일관성 향상
    - 데이터 구조 무결성 유지
- AI 모델을 위한 메모리 시스템은 일반적으로 두 가지 핵심기능
    - 메모리 관리
    - 메모리 검색
 ---
 [my blog](https://velog.io/@dkan9634/AI-Engineering-Chap-6.-RAG%EC%99%80-%EC%97%90%EC%9D%B4%EC%A0%84%ED%8A%B8)
