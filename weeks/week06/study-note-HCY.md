[7장 velog 링크](https://velog.io/@algorithm_cell/AI-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-7%EC%9E%A5.-%ED%8C%8C%EC%9D%B8%ED%8A%9C%EB%8B%9D)

**파인튜닝**은 **모델 전체나 일부를 추가로 학습시켜 특정 작업에 맞게 모델을 조정**하는 과정이다.

파인튜닝을 통해 모델의 다양한 측면을 향상시킬 수 있지만, 가장 많이 활용하는 목적은 **모델의 지시 수행 능력을 향상**시키기 위함이다.


# 1) 파인튜닝

파인튜닝은 **필요한 기본적인 능력을 갖춘 기본 모델을 가지고 시작한다.** 

파인튜닝의 목표는 **특정 작업을 충분히 잘 수행하도록 만드는 것이다.**



## 전이학습

이미 학습된 모델을 가져와, **다른 작업에 적용**하여 모델을 더 빠르고 효율적으로 학습시키는 기법이다.

이를 통해, **표본 효율성**을 높여 모델이 더 적은 예시로도 같은 행동을 학습할 수 있게 한다.


### 1. 파인튜닝

사전 학습 이후에 모델을 추가로 학습시키는 과정을 통칭한다.
	
- **1) 지속적 사전 학습**
	
    - 이미 사전학습된 모델을 **추가 데이터로 더 학습시키는 단계**
    
    - 비싼 작업별 데이터로 파인튜닝하기 전에 관련 분야 데이터로 먼저 자기 지도 학습을 적용
    
    - 즉, 지속적 사전 학습은 **태스크를 가르치기 전, 도메인에 먼저 익숙하게 만드는 파인튜닝**
    
    
    
- **2) Infilling fine-tuning**
	
    - 문장의 문맥을 활용해, **비어 있는 토큰을 자연스럽고 정확하게 생성하도록 모델의 출력 방식을 확장**
    - 자기회귀 방식으로 사전 학습된 모델이라도 인필링 파인튜닝이 가능
	
    	- 모델 구조를 바꿀 필요 없음
        
       - 훈련 데이터 포맷과 loss만 바꾸면 됨


- **3) Supervised fine-tuning**
	
    - **(입력, 출력) 쌍으로 모델 학습**
    
    

- **4) Preference fine-tuning**
	
    
   - 사람이 더 좋아하는 응답을 생성하도록 만드는 단계
   
   - **강화학습 기반**으로, (지시, 선호 응답, 비선호 응답) 형태의 **비교** 데이터 사용

- **5) long context fine-tuning**
	
    - 모델이 더 긴 입력 시퀀스를 활용하도록 **컨텍스트 길이를 확장**하는 파인튜닝 기법
    
    - **임베딩 조정과 같은 모델 구조 변경이 필요함**
    
    
    - 활용 예시)
    ![](https://velog.velcdn.com/images/algorithm_cell/post/2fe59381-7e0e-4e53-be21-cf0bee0cdd6f/image.png)


### 2. 특성 기반 전이

- 모델이 데이터에서 특성을 추출하도록 학습되고, 주로 임베딩 벡터 형태로 추출된 특성을 다른 모델이 활용한다.

- CV (computer vision) 분야에서 많이 사용된다.

<br>

**일반 능력과 특정 작업 수행 능력을 모두 향상**시키는 것이 파인튜닝을 하는 주요 목적이다.

기본 모델은 학습 데이터의 편향을 그대로 반영하므로, 파인튜닝 단계에서 신중하게 선별된 데이터를 사용하면 **편향 완화가 가능**하다.

## 파인튜닝이 효과적인 경우

1. **사용하려는 모델이 수행하는 작업에 충분히 학습되지 않은 경우**

- 해당 작업과 관련된 데이터로 파인튜닝하는 것이 효과적이다.

2. **모델 크기가 작은 경우**

- 메모리 요구량이 적고, 학습이 쉽고, 추론 속도가 빠르기 때문이다. 

- ex) **지식 증류(Distillation) : 큰 모델이 생성한 데이터를 사용해 작은 모델을 학습**


### ex)

**프롬프트에 포함하는 예시들로 모델을 파인튜닝하는 경우**

매번 프롬프트에 예시를 포함하는 대신,**해당 예시들로 모델을 파인튜닝하면** 모델은 더 짧은 프롬프트로도 동일한 결과를 낼 수 있다.

![](https://velog.velcdn.com/images/algorithm_cell/post/944a1db3-aaed-4eab-b303-458053d48c58/image.png)

이제는 반복적인 프롬프트 부분을 저장했다가 재사용할 수 있는 **프롬프트 캐싱 기술**이 도입 되었기 때문에 장점은 크게 줄었다.

하지만, **파인튜닝에서는 활용할 수 있는 예시 수의 제한**이 없기 때문에 여전히 활용되고 있다.



하지만, 다양한 프롬프트를 사용해야하는 애플리케이션에서는 **특정 작업은 향상될지라도, 전반적 성능이 감소할 가능성이 있다.**



## 파인튜닝이 어려운 이유

1. **파인튜닝은 모델 학습 방식에 대한 지식이 필요**

- 조정할 수 있는 학습 파라미터를 이해하고, 학습 과정을 모니터링하고, 디버깅할 줄 알아야한다.
	
    - **optimizer 작동 방식, lr** .. 등을 이해해야한다.
    
    
2. **파인튜닝된 모델을 서빙하는 방법에 대한 이해가 필요**

- 직접 호스팅할지 아니면, API 서비스를 사용할지 정해야한다.


3. **모델을 모니터링하고 유지보수하기 위한 예산을 수립이 필요**


### ex)

**금융 도메인에서의 범용 모델과, 금융 특화 모델의 성능 차이 비교**

범용 모델이 더 성능이 뛰어날 수 있기 때문에, **범용 모델이 특정 도메인 작업에 잘 작동하지 않는 다는 이야기는 주의해야한다.**

![](https://velog.velcdn.com/images/algorithm_cell/post/85981f82-dfb2-4d57-ad74-eac2e82aaf18/image.png)


## RAG와 Fine-tuning

앞 장의 RAG와 비교하자면, **파인튜닝은 형식을 위한 것이고, RAG는 사실을 위한 것이다.**

RAG는 모델에게 **외부 지식을 제공해** 환각을 제어해 더 정확한 답을 만드는 것이고, 파인튜닝은 모델이 특정 문구와 스타일을 잉해하고 따르는데 도움을 준다.

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/11b58ebf-cfd3-4975-bea9-ac67432ba143/image.png" />
<p align="center">애플리케이션 개발 과정이 취할 수 있는 여러 경로</p>

### 모델 조정 과정

1. **프롬프트만으로 모델이 원하는 작업을 수행**


2. **프롬프트에 더 많은 예시를 추가** (사용 사례에 필요한 예시의 수는 1~50개 정도)


3. **관련 데이터 연결**

- RAG를 시작할 때는 키워드 기반 검색 같은 기본적인 검색 방법부터 시작하는 것이 좋다. 단순한 검색이라도 관련성 높고 정확한 정보를 추가하면 모델의 성능에 어느 정도 향상이 있어야 한다.

4. **모델의 오류 유형**에 따라 다음 단계 중 하나를 시도해 볼 수 있다.

    a. **모델이 계속해서 정보 관련 오류**를 보인다면, **임베딩 기반 검색** 같은 RAG 방법을 시도해 볼 수 있다.

   b. **모델이 관련 없는 내용을 생성하거나, 형식이 잘못되거나, 안전하지 않은 응답**을 생성하는 등의 행동 측면의 문제가 지속된다면 **파인튜닝**을 고려한다. 


---


# 2) 메모리 병목 현상


파운데이션 모델의 큰 규모로 인해, **추론과 파인튜닝 모두에서 메모리가 주요 병목 지점이 된다.**

파인튜닝 기법은 메모리 사용량을 최소화하는 데 중점을 둔다.

파인튜닝 메모리 사용량에 큰 영향을 미치는 요소는 **모델의 전체 파라미터 수, 학습 가능한 파라미터 수, 수치 표현 방식**이다.


추론 과정에서는 순방향 패스만 실행되지만, **학습 과정에서는 역전파, 순전파 양방향 패스가 모두 실행**된다. 

역방향 패스 동안에는 gradient, optimizer state 같은 값들을 동반함으로 더 많은 메모리가 필요하다.

따라서, **학습 가능한 파라미터 수**가 많을수록, 더 많은 메모리가 필요하다.




<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/31f806fd-6998-4bdd-a9e4-bc563a0a339f/image.png" />
<p align="center">신경망의 순방향 및 역방향 처리</p>

## 메모리 계산

모델에 적절한 하드웨어를 선택하기 위해 **모델에 필요한 메모리 양을 미리 계산**해야한다.

모델의 메모리 사용량은 모델 자체 뿐 아니라 **작업 부하, 메모리 최적화 기법**에 따라 달라지지만, 모든 최적화기법과 작업 부하를 고려하는 것은 불가능하기 때문에 이 절에서는 계산식만 다룬다.

이를 통한, 대략적인 메모리 양을 가늠할 수 있다.

### 1. 추론에 필요한 메모리

추론에서는 순방향 패스만 실행되므로, **모델 가중치를 위한 메모리와 활성화 메모리**가 필요하다.

- N : 모델 파라미터수 
- M : 각 파라미터에 필요한 메모리

- 활성화 값(Activation) : 각 층을 지나며 계산한 중간 출력 값
	
    - **모델 가중치 메모리의 약 20%로 가정**
	
    - 다음 레이어 계산을 위해 직전 레이어 출력이 필요하기 때문에 저장
    
    - ex) Transformer에서 Q/K/V vector, attention score 등

    
따라서 약, **N * M * 1.2 * 각 파라미터 당 byte**의 메모리가 필요하다.

모델이 커질수록, **메모리는 모델 운영의 병목이 된다.**
    

### 2. 학습에 필요한 메모리

학습에 필요한 메모리는 **모델의 가중치, 활성화 메모리** 뿐만 아니라, **그래디언트, 옵티마이져 스테이트를 위한 메모리**도 필요하다.

- **gradient** : 1 per parameter

- **optimizer**
	
    - SGD : 0 state
    - Momentum : 1 state per parameter
    - Adam : 2 state per parameter
  


### 3. 활성화 메모리 절감 방법

추론 시 활성화 메모리가 가중치 메모리보다 작다고 가정했지만, 실제로 **역전파를 위해 활성화를 저장하면 활성화 메모리가 가중치 메모리보다 훨씬 커질 수 있다.**


![](https://velog.velcdn.com/images/algorithm_cell/post/5bd22323-14ee-40e9-826a-e5641057ca48/image.png)

**그래디언트 체크포인팅** (= 활성화 재계산, activation recomputation)

- 활성화를 전부 저장하지 않고 일부만 저장한 뒤, 필요할 때 다시 계산하는 방식

- 장점 : 메모리 사용량 크게 감소

- 단점 : 재계산으로 인해 학습 시간 증가


## 수치 표현 방식

모델에서 **각 parameter를 몇 비트로 표현**하느냐는 전체 메모리 사용량에 직접적인 영향을 미친다.

신경망의 수치 값은 전통적으로 **부동소수점 수**로 표현되므로 이에 대해 먼저 알아보자.


### 부동소수점


| 형식   | 비트 수  | 바이트 | 특징 |
| ------ | ---- | ---- | --------------------------- |
| FP64 | 64bit | 8B | 배정밀도(double)
| FP32 | 32bit | 4B | 단정밀도(single), 가장 표준적인 표현 
| FP16 | 16bit | 2B | 반정밀도 (half)
| BF16 | 16bit | 2B | TPU 기반 학습 |
| BF16 | NVIDIA GPU에서 연산 방식 |-|FP32 유지하고 가수만 10bit로 줄여 연산 기준 약 19bit 정밀도의 NVIDIA 전용 연산 포맷
FP64는 메모리 사용량 때문에 신경망에서는 거의 사용되지 않는다.

**Bit 사용**

![](https://velog.velcdn.com/images/algorithm_cell/post/01ee7152-7fbc-49a1-af31-e9687bc40089/image.png)

- **1bit** : 부호

- 나머지 bit
	
    - **범위** : 범위 비트는 형식이 표현할 수 있는 값의 범위 결정
    - **정밀도** : 숫자를 얼마나 정확하게 표현할 수 있는지 결정
    
보통 비트 수가 많은 형식일수록, 정밀도가 높다고 간주된다. 

모델을  사용할 때는 반드시 지정된 형식으로 로드해야한다. 잘못된 수치 형식으로 로드하게 될 경우 성능이 크게 저하될 수 있다. 




## 양자화

정밀도를 낮추는 것을 **양자화**라고 한다.
* 엄밀히 말하면, 대상 형식이 **정수**일 때만, 양자화라고 해야하지만, 실제로는 저정밀도 형식으로 변환하는 모든 기법을 양자화라고 한다.

모델의 메모리 사용량을 줄이는 매우 효과적인 방법이다.

**구현이 간단하고, 다양한 작업과 아키텍처에 두루 적용할 수 있다.**


### 1. 고려해야할 것
1. **무엇을 양자화?**

	- 원칙적으로는 메모리를 가장 많이 차지하는 요소부터 양자화하는 것이 이상적이지만, 실제로는 **성능 저하 없이 적용 가능한지가 더 중요하다.**
    - **가중치 양자화**가 활성화 양자화보다 더 일반적이다.
    	
        - 안정적이고 정확도 손실이 적다.
        
2. **언제 양자화?**

	- 학습 중 또는 학습 후에 수행 가능하다.
    
    - **학습 후 양자화 (PTQ, Post-Training Quantization)**
    	
        - 모델 학습이 완전히 끝난 뒤 적용한다.
        - 모델을 직접 학습시키지 않는 AI 애플리케이션 개발자에게 특히 적합하다.


### 2. 추론 양자화

딥러닝 초기에는 FB32를 사용해 모델을 학습하고 서빙하는 것이 표준이였으나, 2010년대 후반부터는 **16bit 및 더 낮은 정밀도로 모델을 서빙**하는 것이 점점 보편화됐다.

모델은 상황에 맞춰 가능할 때는 정밀도를 낮추고, 필요할 때는 높은 정밀도를 유지하는 **혼합 정밀도로 서빙될 수 있다.**

양자화는 효과적인 기법이지만, 무한정 적용할 수는 없다. 이론상 1bit보다 더 작게는 양자화할 수 없다.


![](https://velog.velcdn.com/images/algorithm_cell/post/8e65b727-166b-4698-be69-da7f8194e4ed/image.png)

- BitNet b1.58은 1.58bit만을 필요로 하는 모델로 파라미터가 39억 개일 때까지는 LLama와 비슷한 성능을 보인다.


정밀도를 낮추는 것의 **장점**

- 더 큰 배치 크기를 사용할 수 있어서, 모델이 **더 많은 입력을 병렬처리할 수 있다.**

- 계산 속도를 높여 추론 지연 시간과 학습 시간을 단축 시킨다.

정밀도를 낮추는 것의 **단점**

- 형식 변환에 필요한 추가 계산이 필요하다.

- 각 변환은 **작은 값의 변화를 가져올 수 있다.** 이것들이 모여 큰 성능 차이를 만들 수 있다.

  

### 3. 학습 양자화

학습 양자화는 아직 보편적이지는 않다.

학습 양자화에는 두가지 뚜렷한 목표가 있다.

1. **추론 과정에서 낮은 정밀도에서도 우수한 성능을 보이는 모델을 만드는 것**
	- 학습 후 양자화(PTQ)에서 모델 품질이 저하될 수 있는 문제를 해결하기 위한 것
    
2. **학습 시간과 비용을 줄이는 것** 


**1. 양자화 인식학습 *quantization-aware training (QAT)***
	
- 모델은 학습 중에 낮은 정밀도 동작을 시뮬레이션하기 때문에, 낮은 정밀도에서도 품질 높은 출력을 생성하도록 학습할 수 있다.

- 계산이 여전히 높은 정밀도로 이루어지므로 **학습 시간 자체는 줄지 않지만**, **낮은 정밀도 동작 추가 작업으로 인해** **학습 시간이 늘어날 수 있다.**

**2. 혼합 정밀도 *Mixed Precision***

- 낮은 정밀도 학습은 주로 혼합 정밀도 방식으로 수행된다.

	
    - 가중치 사본 → 높은 정밀도
    
    - 그래디언, 활성화 → 낮은 정밀도


- ex) **LLM-QAT**: 가중치와 활성화를 4bit, 임베딩은 16bit로 양자화


# 3) 파인튜닝 기법


모델 크기가 작았을 때는 전체 파인튜닝을 했지만, 점점 학습 가능한 파라미터가 많아지면서 **부분 파인튜닝**을 시작하게 되었다.

**부분 파인튜닝**에서는 모델 파라미터의 일부만 업데이트가 된다. 

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/1e5b7a60-17ed-400f-bbee-35b80795abf8/image.png" />
<p align="center">약 25%의 파라미터를 업데이트하면 전체 파인튜닝과 비슷한 성능을 얻을 수 있다.</p>

## PEPT *parameter-efficient*

수십 배 더 적은 학습 가능한 파라미터를 통해 전체 파인튜닝에 근접한 성능을 달성할 수 있는 방법을 의미한다.

### 1. 소프트 프롬프트 기반 방법 *soft propmt-based method*


- 하드 프롬프트

  - 사람이 읽을 수 있는 프롬프트로, 수동으로 다양한 조합을 탐색한다.
  - 고정되어 있고 학습이 불가능하다.

- 소프트 프롬프트

  - 사람이 읽을 수 없는 프롬프트로, 입력 임베딩 공간에 존재하는 학습 가능한 vector이다.
  - 튜닝 과정에서 역전파를 통해 최적화할 수 있다.

![](https://velog.velcdn.com/images/algorithm_cell/post/0a1e4bfc-f16a-4681-a6e9-6ae12d7b217d/image.png)

1. **prefix-tuning** : 입력 앞단에 task-specific soft prompt를 붙여서 사용

2. **prompt tuning** : 임베딩된 입력 앞에만 소프트 프롬프트 토큰을 붙인다.

3. **p-tuning** : 소프트 프롬프트를 생성하하는 prompt encoder가 존재한다. 모든 위치에 자유롭게 위치할 수 있다.


### 2. 어댑터 기반 방법 *adapter-based method*

- 모델 가중치에 추가 모듈을 붙이는 모든 방식들을 말한다.

- 파라미터를 추가하기 때문에 부가적 방법이라고도 부른다.

### LoRA

![](https://velog.velcdn.com/images/algorithm_cell/post/23511aa9-d819-470d-9dc5-3dccd630e715/image.png)


- 주어진 가중치 행렬을 분해하기 위해 더 작은 행렬들의 차원을 선택한다. (선택된 값 : r) 
- 두 개의 행렬을 구성한다: A(n × r 차원)와 B(r × m 차원)
- 이 두 행렬의 곱 W_AB는 원래 행렬 W와 동일한 차원을 가진다. 여기서 r은 LoRA 랭크라고 부른다.

- $$W_{AB}$$를 원래 가중치 행렬 W0에 더해 새로운 가중치 행렬 W′를 생성고 모델에서는 W 대신 W′를 사용한다.
- 하이퍼파라미터 α를 통해 $$W_{AB}$$가 새 행렬에 얼마나 영향을 미칠지 조절할 수 있다 
	
    - $$W' = W + \frac{\alpha}{r} W_{AB}$$

    
      - **랭크 r**
      - **스케일 파라미터 α**

- 파인튜닝 과정에서는 A와 B의 파라미터만 업데이트하고, W는 변경하지 않고 그대로 유지한다



**LoRA가 효과적인 이유?**

- LLM은 수많은 파라미터를 가지고 있음에도 실제로는 매우 낮은 내재적 차원을 가진다.

- **내재적 차원** : 모델이 실제로 의미 있는 변화를 만들기 위해 필요한 최소한의 자유도(차원 수)다.

- 사전 학습을 거치면서 **모델은 이미 저차원 내재 공간에 압축된 표현을 학습**하므로, 파인튜닝에서는 전체 파라미터를 바꿀 필요 없이, **저랭크(low-rank) 방향의 변화만으로도 행동을 충분히 바꿀 수 있다**.


**LoRA adaptor 서빙 방식**

1. **모델 서빙 전**에 가중치 A,B를 원본 모델에 미리 병합해서 새로운 행렬을 만든다.

    - 추론할 때 추가 연산이 필요하지 않아 지연 시간도 늘어나지 않는다.

    - 서빙할 LoRA 모델이 하나뿐일 때 유용하다.

2. **서빙 시** W,A,B 가중치를 각각 따로 유지한다.

    - 추론 시에 A와 B를 W에 병합해야 하므로 지연 시간이 늘어난다.

    - 멀티 LoRA를 서빙할 때 유리하다.

  ![](https://velog.velcdn.com/images/algorithm_cell/post/fc0ddaae-3ce7-487c-9a4c-a3e918853a19/image.png)
	
    - W를 재사용할 수 있다는 점에서 유리하다.
    
    - 멀티 LoRA 서빙을 활용하면 전문화된 여러 모델을 쉽게 결합할 수 있다.
    
**LoRA 단점**

- 전체 파인튜닝만큼 강력한 성능을 제공하지 못한다.

- 모델의 구현을 수정해야하므로 코딩 기술과 아키텍쳐에 대한 이해가 필요하다.
	
    - 단, 인기 있는 기본 모델에 대해서는 LoRA를 별도 설정 없이 바로 사용할 수 있다.


**양자화된 LoRA**

![](https://velog.velcdn.com/images/algorithm_cell/post/c45e17e0-7fee-4b45-9c42-506447a42f58/image.png)

LoRA adaptor가 사용하는 메모리는 모델 가중치에 비해 매우 작기 때문에, LoRA 파라미터를 줄여도 전체 메모리 사용량은 거의 줄어들지 않는다.


파인튜닝 시 **모델의 가중치, 활성화 그래디언트를 양자화하는 편이 메모리를 훨씬 효과적으로 절약할 수 있다.**

- **QLoRA**

  - 사전 학습된 모델 가중치를 4비트(NF4) 로 저장하고, 순전파/역전파 계산 시에만 BF16으로 역양자화해 사용한다.

    - *NF4(NormalFloat-4)* : 사전 학습 가중치는 중앙값이 0인 정규분포를 따른다는 가정을 활용해 양자화한다.

    - *페이징 최적화(paged optimizer)*: GPU 메모리가 부족할 때 CPU–GPU 간 메모리를 자동 전송하여, 매우 긴 시퀀스나 초대형 모델도 학습 가능하게 한다.


  - 한계 : NF4 양자화 과정에 비용이 많이 든다. 따라서 양자화하고 다시 되돌리는 과정에서 시간이 늘어날 수 있다.

## 모델 병합

>모델 병합을 하지 않는다면 **동시 파인튜닝, 순차 파인튜닝** 같은 방식을 통해 모델을 학습해야한다.  
> - **동시 파인튜닝**
>	
>    - 모든 작업에 대한 예시를 하나의 데이터셋에 담아서 모델이 모든 작업을 동시에 학습한다.
>    - 더 많은 데이터와 학습이 필요하다.
>
>
> - **순차 파인튜닝**
>
>   - 각 작업을 하나씩 순차적으로 모델을 파인튜닝하는 방식이다.
>   - **신경망의 재앙적 망각 현상**에 취약하  


### 1. 모델 병합이란?


모델 병합은 **연합 학습**의 한 방식으로, 이 아이디어는 **모델 앙상블**에서 출발했다.
![](https://velog.velcdn.com/images/algorithm_cell/post/53c7d86f-c163-4ded-9612-de69aa2e38f2/image.png)


- **여러 모델을 결합하여 맞춤형 모델을 만드는 방법으로 더 큰 유연성을 제공한다.**

- **유용한 통합 모델을 만들 수 있고, 메모리 사용량을 줄여 비용을 절약할 수 있다.**

- **GPU 없이 할 수 있다.**

- 온디바이스 환경처럼 메모리가 제한된 기기에서 효과적이다.
	
    - 온디바이스 배포가 필요한 이유
    
    	- **개인정보 보호**: 데이터가 기기 밖으로 나갈 수 없는 경우
       - **네트워크 제약**: 인터넷 연결이 불안정하거나 신뢰할 수 없는 환경
        - **비용 절감**: 기기 내 연산 비중 증가 → 데이터센터 추론 비용 감소



### 2. 모델 병합 방법(접근법)

**1. 합산**

구성 모델들의 가중치를 더하는 방식이다.

![](https://velog.velcdn.com/images/algorithm_cell/post/de5cf355-86a7-4437-bc2d-357c2fe96500/image.png)

- **합산 방법**

  1. **선형 결합**
	
    - 가중 평균 : $${Merge}(A, B) = \frac{W_A^A + W_B^B}{W_A + W_B}$$
    	
  <p align="center">
    <img src="https://velog.velcdn.com/images/algorithm_cell/post/01916b3c-b0ad-40cb-8b4c-7ec55752a98b/image.png" />
  <p align="center">Wa=Wb=1일 때 예시</p>

    - 전체 모델을 합칠 수도 있고 LoRA 가중치 등 일부 파라미터만 결합하는 것이 더 일반적이다.
    - 특히 같은 기본 모델에서 **파인튜닝된 모델들끼리 병합**할 때 효과가 가장 좋다.
 
    - **작업 벡터와 작업 산술**
    
    	- **작업 벡터** : 파인튜닝된 모델 − 기본 모델 = 특정 작업의 변화량(Δ 파라미터)
    
    	- 작업 벡터가 있으면 **작업 산술 가능** : 더해서 작업 능력 결합, 빼서 특정 기능 제거
        
     2. **구면 선형 보간 *Spherical Linear Interpolation (SLERP)***
     
     - **파라미터를 벡터 공간의 구 표면 위 점으로 보고 결합**하는 방식이다.
 
     - 두 벡터에 대해서만 정의되므로, 여러 모델 병합 시에는 **순차적**으로 SLERP 적용한다.

     - 수학적으로 복잡하지만 대부분 **모델 병합 도구가 내부적으로 처리**한다.
    


- **합산 전 전처리** - **불필요한 가지치기**
	
    
    - 연구 결과 **작업 벡터의 상위 일부만 남겨도 성능 거의 유지**된다.
    ![](https://velog.velcdn.com/images/algorithm_cell/post/252f2e6c-8a5e-4508-8355-45d5a1267a70/image.png)
    
    - 단일 모델에서는 문제가 없어도 여러 모델을 병합하면 불필요한 파라미터 간 간섭 발생하기 때문에 **병합 전에 불필요한 작업 벡터 파라미터를 0으로 리셋(재설정)**한다. 



**2. 레이어 쌓기**

**패스스루 *pass throught*, 프랑켄머징 *franken merging***이라고도 한다.

![](https://velog.velcdn.com/images/algorithm_cell/post/e2e25572-f8fa-4455-b8a2-ec2d233111b7/image.png)

여러 모델에서 서로 다른 레이어를 가져다가 차곡차곡 쌓아올리는 방법이다.

이렇게 하면 기존에 없던 고유한 아키텍처와 파라미터 수를 가진 모델을 만들 수 있다.

하지만, 제대로 된 성능을 내려면 **보통 추가 파인튜닝이 필요하다.**


- **활용 방법**
	
   1. **전문가 혼합 *MOE***
   
     - 사전학습된 모델에서 특정 레이어나 모듈을 여러 개 복사해 라우터를 추가하고 입력을 가장 적합한 복사본으로 보내 병합된 모델과 라우터를 함께 추가 학습시켜 성능을 개선한다.

    ![](https://velog.velcdn.com/images/algorithm_cell/post/d45f8bbe-0fd0-4e62-b411-31b6aa10f08c/image.png)
  
    - 이 방식으로 오픈소스 공개 모델 6개를 조합해 **에이전트 혼합**을  만들어 GPT-4o 수준의 성능을 달성했다.
    
    
  
    <br>

  2. **모델 업스케일링**
  - 적은 자원으로 더 큰 모델을 만드는 방법을 연구하는 분야이다.
  - ex) **뎁스와이즈 스케일링**
       ![](https://velog.velcdn.com/images/algorithm_cell/post/1686d90e-81b0-4c4d-9eb9-7f7b7c2f5f45/image.png)
    
      ![](https://velog.velcdn.com/images/algorithm_cell/post/df4ff522-5286-4d4e-9951-443e93415f54/image.png)



**3. 연결**

![](https://velog.velcdn.com/images/algorithm_cell/post/f07985a0-310e-4675-90ad-85f771ed85cf/image.png)

구성 모델의 파라미터를 그냥 연결하는 방식이다.

따라서, 병합된 구성 요소의 파라미터의 개수는 모든 구성 요소의 파라미터의 합과 같아진다.

연결 방식은 모델을 각각 따로 서빙하는 것과 비교해서 **메모리 사용량이 전혀 줄어들지 않는다.**

<p align="center">
  <img src="https://velog.velcdn.com/images/algorithm_cell/post/2d49d8ed-77e2-4575-889c-5026df59391f/image.png" />
<p align="center">두 LoRA 어댑터를 병합한 예시</p>




## 전술

### 파인튜닝 과정에서 고려해야하는 것

1. **기본 모델**
- 프로젝트 초기에 예산 범위 내에서 가장 강력한 모델부터 시도하는 것이 효율적이다.
- 파인튜닝의 시작 모델은 프로젝트에 따라 다르며, 대표적인 접근은 두 가지 경로다.
	
    - **진행 경로**
    	
        - **저렴하고 빠른 모델 → 중간급 모델 → 최고 성능 모델** 순으로 점진적 실험
   
       - 성능을 어디까지 끌어올릴 수 있는 지 확인하고, **비용 대비 성능**을 파악해 모델을 선택

    - **증류 경로**
    	
        - **가장 강력한 모델을 소량의 데이터로 먼저 파인튜닝** → 그 모델로 추가 학습 데이터 생성 → 생성된 데이터로 더 **저렴한 모델을 학습**



2. **파인튜닝 방법**

- 처음 파인튜닝 시에는 **LoRA** 같은 방식을 사용한 후에, 전체 파인튜닝을 진행한다.

- 데이터가 매우 적다면, 전체 파인튜닝이 LoRA보다 오히려 성능이 더 나쁠 수 있다.

- 파인튜닝 방식은 **서빙 전략**과도 연결된다.

  - LoRA: 하나의 기본 모델 + 여러 어댑터 → 서빙 효율 높음

  - 저체 파인튜닝: 작업마다 전체 모델 필요 → 서빙 비용 증가


3. **파인튜닝 프레임워크**

- **파인튜닝 API 사용**

  - 장점: 빠르고 간편

  - 한계: 지원 모델 / 설정이 제한적, 맞춤 제어 어려움

- **프레임워크 사용**
	
    - ex) LLaMA-Factory, Unsloth, PEFT, Axolotl, LitGPT 등을 다양한 파인튜닝 방법 지원
 
    - 전체 파인튜닝은 오픈소스 코드 클론 후 직접 실행 가능

- **직접 파인튜닝**

  - 장점: 최대 유연성

  - 단점: 컴퓨팅 자원 직접 준비 필요
  
  - 대규모/분산 학습 시에는 DeepSpeed, PyTorch Distributed, ColossalAI 등 필요

4. **파인튜닝 하이퍼파라미터**

- **학습률 *learning rate (lr)***

  - 파라미터의 업데이트 속도를 결정

  - 너무 크면 불안정, 너무 작으면 수렴이 느리기 때문에 보통 1e-7 ~ 1e-3 범위에서 탐색

  - 사전 학습 학습률의 0.1~1배로 시작하는 경우가 많음

  - 스케줄링 방식을 활용해 초반 크게 → 후반 작게

- **배치 크기 *batch size***

  - 한 번의 업데이트에 사용하는 샘플 수

  - 작으면 학습 불안정, 크면 안정적이나 메모리 소모 증가

  - 하드웨어 제약이 크기 때문에, **그래디언트 누적(여러 작은 배치의 gradient를 모아 한 번에 업데이트)** 방식 활용

  - 다양한 배치 크기 실험 권장
 
- **에폭 수 *epochs***
 
  - 전체 데이터를 몇 번 반복 학습할지

  - 작은 데이터셋 → 더 많은 에폭 필요하고 , 큰 데이터셋 → 1~2 에폭으로도 충분할 수 있음

  - **학습 loss/검증 loss 둘 다 감소 → 에폭 늘려도 괜찮지만, 학습 감소/검증 증가 → 과적합을 의미하므로 에폭 줄이기**

- **프롬프트 손실 가중치**
 
  - 지시 파인튜닝에서 프롬프트와 응답 중 무엇을 더 학습할지 결정

  - 추론 시에는 응답이 더 중요하기 때문에 응답만 생성

  - 가중치 의미: 100%: 프롬프트와 응답 동일 비중 , 0%: 응답만 학습

      - 보통 기본값 10%
