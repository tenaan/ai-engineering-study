# Chap 9. 추론 최적화(2)
- 추론 최적화는 모델, 하드웨어, 서비스 수준에서 할 수 있다.
- 이상적으론 속도와 비용을 위해 모델을 최적화해도 모델 품질은 그대로여야 한다.
- 라마 모델을 여러 추론 서비스 제공업체가 제공할 때 **각 벤치마크의 성능 차이**를 보여준다.
![](https://velog.velcdn.com/images/dkan9634/post/45684868-a00d-4bd7-8175-40b973f06c79/image.png)

## 모델 최적화
- 목표: **모델 자체를 수정하는 방식으로 효율성을 높이는 것**
- 추론을 리소스 집약적으로 만드는 세 가지 특성: **모델 크기, 자기회귀 디코딩, 어텐션 메커니즘**

### 모델 압축
- 모델 크기를 줄이는 여러 기법
- 모델이 작아지면 속도도 빨라질 수 있다.
- 이미 두 가지 모델 압축 기법을 다뤘음: **양자화, 증류**

큰 모델 안에 전체 모델의 동작을 담을 수 있는 파라미터의 부분집합이 존재하지 않을까?=> **프루닝**의 핵심 아이디어
- 프루닝의 두 가지 뜻
1) 신경망 노드 전체를 제거 => 아키텍처를 바꾸고 파라미터 개수 줄임
2) 예측에 별로 도움이 안되는 파라미터를 찾아서 0으로 설정 => 전체 파라미터 개수를 줄이지 않고 0이 아닌 파라미터 개수만 줄임 => 모델이 더 희소해져 모델 저장 공간도 줄고 연산도 빨라짐
- 프로닝한 모델은 그대로 써도 되지만, **추가로 파인튜닝**해서 남은 파라미터를 조정하면 프루닝 때문에 떨어진 성능을 회복할 수 있다.
- **가중치 전용 양자화**가 이 분야에서 단연코 가장 인기 있는 방법
   - 쓰기 쉽고 모델에서 바로 작동하며 효과가 뛰어난다.
   - 모델 정밀도를 32비트에서 16비트로 줄이면 메모리 사용량이 절반으로 줄지만 거의 한계이다.(값 하나당 1비트보다 더 낮출 수 없기 때문)
   - 그래서 요즘은 **증류**도 많이 쓰인다.
   
   
 ### 자기 회귀 디코딩 병목 현상 극복하기
 - 이 분야가 빠르게 발전하면서 불가능해 보이는 병목을 극복하는 새로운 기법들이 개발되고 있다.
 #### 추측 디코딩
 - 추측 샘플링이라고도 함
 - 더 빠르지만 성능이 낮은 모델을 사용해 토큰 시퀀스를 생성한 다음, 이를 목표 모델이 검증하는 방식
 - 더 빠른 모델은 초안 출력을 제안하기 때문에 초안 모델이나 제안 모델이라고 불린다.
 1. 초안 모델이 K개의 토큰 시퀀스를 생성
 2. 목표 모델이 이 K개의 생성된 토큰을 병렬로 검증
 3. 목표 모델이 초안 시퀀스를 순서대로 검증해 예측이 엇갈리는 지점 바로 앞까지의 토큰들만 수락
 4. 수락 후 목표 모델은 추가로 하나의 토큰을 직접 생성
 
 ![](https://velog.velcdn.com/images/dkan9634/post/0e5b07e8-2343-4a68-9431-1752105c880d/image.png)

- 만약 **모든 초안 시퀀스가 거부**된다면, 목표 모델은 검증 작업 + 전체 응답을 생성해야 해서 **오히려 지연 시간이 늘어날 수도 있다.**
하지만 다음 세 가지 통찰 덕분에 이런 상황은 피할 수 있다.
1) 검증은 병렬화할 수 있지만 생성은 순차적이기 때문에, 목표 모델이 토큰 시퀀스를 검증하는 데 걸리는 시간은 직접 생성하는 데 걸리는 시간보다 짧다. 추측 디코딩은 사실상 디코딩의 연산 방식을 프리필 방식으로 바꾸는 효과가 있다.
2) 출력 토큰 시퀀스에서 어떤 토큰은 다른 토큰보다 예측하기 쉽다. 예측하기 쉬운 토큰들을 잘 맞추는 약한 초안 모델을 선택할 수 있어서 초안 토큰 수락률이 높아진다.
3) 디코딩은 메모리 대역폭에 제약을 받아서 디코딩 과정에선 보통 남는 FLOP을 검증에  활용할 수 있다.

#### 참조 기반 추론
- 응답할 때 입력 토큰들을 참조해야 하는 경우가 많다.
- 아이디어: 모델이 반복되는 토큰들을 새로 생성하게 하는 대신, **입력을 바로 복사해서 생성 속도를 높이면 어떨까?**
- 추측 디코딩과 비슷하지만, 모델을 사용해서 초안 토큰을 생성하는 대신 입력에서 초안 토큰을 가져오는 점에서 차이가 있다.
- 각 디코딩 단계 컨텍스트에서 가장 관련성 높은 텍스트 구간을 찾아내는 알고리즘을 개발하는 것이 핵심 과제이다.
   - 가장 간단한 방법은 현재 토큰들과 일치하는 텍스트 구간을 찾는 것
- 추가 모델이 필요 없지만 검색 시스템, 코딩, 멀티 턴 대화처럼 컨텍스트와 출력 사이에 상당한 중복이 있는 생성 시나리오에서만 유용하다.
![](https://velog.velcdn.com/images/dkan9634/post/a2210727-44f1-4c5e-bfb4-0f0f5e0c0d68/image.png)

#### 병렬 디코딩
- 병렬 디코딩은 기존의 순차적 토큰 생성 방식의 병목을 줄이기 위해, 여러 개의 미래 토큰을 동시에 생성하는 디코딩 기법이다. 
- 일반적인 자기회귀 모델은 이전 토큰이 확정되어야 다음 토큰을 생성할 수 있지만, 병렬 디코딩은 이러한 순차적 의존성을 일부 제거하여 추론 속도를 크게 향상시키는 것을 목표로 한다.
   - 이 방식이 가능한 이유는, 주어진 컨텍스트만으로도 다음 몇 개의 토큰을 비교적 정확하게 예측할 수 있는 경우가 많기 때문이다. 
       - ex) “the cat sits”라는 문장이 주어졌을 때, 다음 토큰이 무엇이든 간에 그 다음 단어가 “the”일 가능성은 높다. 이러한 통계적 구조를 활용해 여러 토큰을 미리 예측한다.

- 대표적인 병렬 디코딩 방식
1) Lookahead 디코딩
   - 여러 개의 미래 토큰을 한 번에 생성
   - 생성된 토큰들이 문맥적으로 일관적인지 검증한 뒤 최종 출력에 반영
   - 야코비(Jacobi) 방식과 유사한 반복적 검증·수정 구조를 가짐
   - 동작 과정
1. K개의 미래 토큰을 병렬로 생성
2. 생성된 토큰들이 컨텍스트와 의미적으로 일관되는지 검증
3. 하나라도 실패하면, 실패한 토큰만 다시 생성
4. 모든 토큰이 검증을 통과할 때까지 반복

이러한 계열의 알고리즘은 **야코비(Jacobi) 디코딩**이라고도 불린다.

2) Medusa 디코딩
- 원본 언어 모델에 여러 개의 디코딩 헤드(Medusa head)를 추가
- 각 헤드는 서로 다른 미래 위치의 토큰을 예측하도록 학습됨
- 원본 모델은 고정(frozen)된 상태에서, 작은 헤드들만 추가 학습
- 특징
   - 각 헤드는 하나의 토큰이 아니라 여러 후보 토큰(top-k)을 생성
   - 생성된 후보들은 트리 구조로 구성됨
   - 트리 탐색을 통해 가장 유망한 토큰 시퀀스를 선택
- NVIDIA는 Medusa를 통해 HGX H200 GPU에서 LLaMA 3.1 토큰 생성 속도를 최대 1.9배 향상시켰다고 보고
- 병렬 디코딩의 핵심 이슈: 검증과 통합
- 병렬 디코딩은 토큰이 순차적으로 생성되지 않기 때문에, 서로 잘 맞는지, 문맥적으로 자연스러운지를 확인하는 검증 단계가 매우 중요하다. 속도 이득의 대부분은 병렬 생성에서 나오지만, 품질은 검증과 통합 단계에서 결정된다.
![](https://velog.velcdn.com/images/dkan9634/post/fffae4e1-4c72-46ba-8737-243c76f8acee/image.png)

### 어텐션 메커니즘 최적화
- 다음 토큰을 생성하려면 이전 모든 토큰의 키와 값 벡터가 필요하다.
- 토큰을 생성할 때 이전 모든 토큰의 키와 벡터를 다시 연산하는 대신, 이전 단계에서 연산한 벡터들을 **재사용**한다.
   - 즉 가장 최근 토큰의 키, 벡터만 새로 연산하면 된다.
- 재사용을 위해 키와 값 벡터를 저장하는 캐시를 **KV 캐시**라고 한다.
![](https://velog.velcdn.com/images/dkan9634/post/26ff8b95-6d33-4044-9b9d-9cc5a29fa087/image.png)
> KV 캐시는 **추론할 때만** 사용된다. 학습할 땐 시퀀스의 모든 토큰을 미리 알 수 있으므로, 추론할 때처럼 하나씩 순차적이 아니라 다음 토큰 생성을 한꺼번에 연산할 수 있다. 따라서 KV 캐시가 필요 없다.
- 토큰 하나를 생성하려면 이전 모든 토큰과의 어텐션 점수를 연산해야 하므로, **어텐션 연산량은 시퀀스 길이에 따라 제곱에 비례해서 증가**한다.
- 반면 KV 캐시 크기는 시퀀스 길이에 따라 선형적으로 증가한다. 
   - KV 캐시 크기는 배치 크기가 클수록 더 커진다.
   - 롱 컨텍스트를 처리하는 애플리케이션에서 병목 현상
   - 캐시 메모리 크면 로드하는 데도 시간이 걸림
- 어텐션 메커니즘을 효율적으로 만들기 위해 많은 기법이 개발되었다.
=> **어텐션 메커니즘 재설계, KV 캐시 최적화, 어텐션 연산을 위한 커널 작성**

#### +) KV 캐시 크기 연산하기
최적화를 하지 않았을 때 KV 캐시에 들어가는 메모리는 다음과 같이 계산할 수 있다.
$$
2×B×S×L×H×M
$$

각 기호의 의미는 다음과 같다.
- B: 배치 크기 (Batch size)
- S: 시퀀스 길이 (Sequence length)
- L: 트랜스포머 레이어 개수
- H: 모델 차원 (Hidden dimension)
- M: 캐시 수치를 저장하는 데 필요한 메모리 (FP16 또는 FP32 등)

컨텍스트 길이가 길어질수록 KV 캐시 메모리 사용량은 급격히 증가할 수 있다.
ex)  **LLaMA-2-13B** 모델을 가정해보자.
레이어 수: 40, 모델 차원: 5,120, 배치 크기: 32, 시퀀스 길이: 2,048, 값 하나당 메모리: 2바이트 (FP16)
이때 최적화 없이 필요한 KV 캐시 메모리는 다음과 같다.

$$
2×32×2048×40×5120×2=54GB
$$

#### 어텐션 메커니즘 재설계
- 어텐션 메커니즘이 작동하는 방식 자체를 변경한다.
- 추론 최적화에 도움이 되긴 하지만, 모델의 아키텍처를 직접 변경하기 때문에 학습 또는 파인튜닝 단계에서만 적용할 수 있다.
- **로컬 윈도우 어텐션**
   - 새 토큰을 생성할 때 이전 모든 토큰에 어텐션을 적용하는 대신, **고정된 크기의 윈도우 안에 있는 토큰에만 어텐션을 적용**
   - 유효 시퀀스 길이가 고정된 윈도우 크기로 줄어들어서, KV 캐시와 어텐션 연산이 모두 줄어든다. 
   - **글로벌 어텐션과 섞어서** 쓸 수 있다.로컬 윈도우 어텐션은 가까운 맥락을 잡아내고 글로벌 어텐션은 문서 전체에서 작업에 필요한 정보를 찾아낸다.
- **크로스 레이어 어텐션, 멀티 쿼리 어텐션**
   - 키-값 쌍 개수를 줄여서 KV 캐시의 메모리 사용량을 줄인다.
   - 크로스 레이어 어텐션은 인접한 레이어들끼리 키와 값 벡터를 공유
      - 레이어 3개가 같은 키-값 벡터를 공유하면 KV 캐시가 1/3으로 줄어든다.
   - 멀티 쿼리 어텐션은 쿼리(질의) 헤드들끼리 키-값 벡터를 공유한다.
- **그룹 쿼리 어텐션**
   - 멀티 쿼리 어텐션을 발전시킨 방법
   - 모든 쿼리 헤드에 하나의 키-값 쌍 세트만 사용하는 대신, 그룹 쿼리 어텐션은 쿼리 헤드들을 더 작은 그룹으로 나누고 **같은 그룹 안에서만 키-값 쌍을 공유**한다.
   - 이렇게 하면 쿼리 헤드 수와 키-값 쌍 수 사이에서 더 유연하게 조절할 수 있다.
- AI 챗봇 애플리케이션인 `Character.AI`는 자신들의 평균 대화에는 180개 메시지 대화기록이 쌓인다고 밝혔다. 이처럼 시퀀스가 길기 때문에, 추론 처리량의 주요 병목은 **KV 캐시 크기**다.
   - 멀티 쿼리 어텐션, 로컬 어텐션, 글로벌 어텐션 번갈아 사용, 크로스 레이어 어텐션이라는 세 가지 어텐션 메커니즘 설계로 KV 캐시를 20배 이상 줄였다.
   - 중요한 건 KV 캐시가 이렇게 크게 줄어들면서 큰 배치로 서비스할 때 메모리가 더 이상 병목되지 않는다.
   
#### KV 캐시 크기 최적화
- 추론 시 메모리 병목 현상을 완화하고, 특히 롱 컨텍스트를 다루는 애플리케이션에서 더 큰 배치 크기를 가능하게 하는 데 매우 중요
- 따라서 KV 캐시를 줄이고 관리하는 많은 기법이 점점 더 활발히 개발되고 있다.
- 가장 빠르게 성장하고 있는 추론 프레임워크 중 하나인 **vLLM**은 **페이지드 어텐션**을 도입해서 인기를 얻었다. 
   - KV 캐시를 연속적이지 않는 블록으로 나누어 메모리 단편화를 줄이고, 메모리를 유연하게 공유해서 LLM 서빙 효율을 개선하고, 이를 통해 메모리 관리를 최적화한다.
- 다른 기법: KV 캐시 양자화, 적응형 KV 캐시 압축, 선택적 KV 캐시

#### 어텐션 연산을 위한 커널 작성
- 메커니즘 설계를 바꾸거나 저장 공간을 최적화하는 대신, 이 접근법은 어텐션 점수를 어떻게 연산하는지 살펴보고 이 연산을 더 효율적으로 만드는 방법을 찾는다.
- 이 접근법은 연산을 실행하는 하드웨어를 고려할 때 가장 효과가 좋다.
- **특정 칩에 최적화된 코드를 커널**이라고 한다.
- 어텐션 연산에 최적화된 가장 잘 알려진 커널 중 하나는 **플래시 어텐션**이다.
   - 트랜스포머 기반 모델에서 많이 사용되는 여러 연산을 하나로 융합해서 더 빠르게 실행되도록 했다.
   ![](https://velog.velcdn.com/images/dkan9634/post/6a79ca3e-47c1-4923-b49d-cf5b6db3ebd6/image.png)

#### 커널과 컴파일러
- 커널은 GPU나 TPU 같은 특정 하드웨어 가속기에 맞춰 최적화한 특수한 코드
   - 보통 연산량이 많고 반복적으로 실행해야 하는 작업을 처리하도록 만들어지며, 종종 병렬로 돌려서 가속기 성능을 최대한 끌어낸다.

**커널 작성이 어려운 이유**
- 커널을 직접 작성하려면 다음을 깊이 이해해야 한다.
   - 캐시, 글로벌 메모리, 공유 메모리, 레지스터로 이루어진 메모리 계층 구조
   - 레벨별 데이터 접근 및 이동 방식
   - 스레드 관리와 병렬 실행 구조

- 또한 커널은 보통 CUDA(NVIDIA), Triton(OpenAI), ROCm(AMD) 같은 저수준 언어로 작성되며, 이는 대부분의 AI 엔지니어에게 익숙한 파이썬보다 훨씬 어렵다.
- 그래서 과거에는 커널 최적화가 소수의 최적화 엔지니어만 다루는 영역이었다.

**추론 속도를 높이는 대표적인 커널 최적화 기법 4가지**
1) 벡터화(Vectorization)
- 루프에서 데이터를 하나씩 처리하는 대신 메모리에 연속된 여러 데이터를 한 번에 처리
- 메모리 I/O 감소 → 지연 시간 감소

2) 병렬화(Parallelization)
- 입력 배열을 여러 코어나 스레드로 나누어
- 동시에 연산 수행
- GPU의 대규모 병렬 구조를 최대한 활용

3) 루프 타일링(Loop Tiling)
- 하드웨어의 캐시 구조에 맞게
- 루프의 데이터 접근 순서를 재구성
- 단, CPU에 효과적인 타일링이 GPU에서는 잘 작동하지 않을 수도 있음

4) 연산자 융합(Operator Fusion)
- 여러 연산자를 하나의 패스로 결합
- 불필요한 메모리 읽기·쓰기 제거
- 가장 강력하지만, 모델 구조와 하드웨어에 대한 깊은 이해 필요

> 벡터화·병렬화·타일링은 비교적 범용적
 연산자 융합은 가장 효과적이지만 난이도도 가장 높음

**커널은 하드웨어에 종속된다**
- 커널은 하드웨어 아키텍처에 강하게 최적화된다.
- 즉, 새로운 GPU 아키텍처가 나오면 새로운 커널이 필요하다.
- ex) FlashAttention은 처음에 A100 GPU용으로 개발
이후 H100 GPU에 맞춰 FlashAttention-3가 새로 등장

**컴파일러와 로어링(Lowering)**
- 모델 스크립트는 “무엇을 계산할지”를 정의할 뿐이다.
이를 GPU에서 실행하려면 하드웨어 친화적인 코드로 변환해야 한다.
- 이 변환 과정을 로어링(lowering) 이라고 함
- 이 작업을 수행하는 도구가 컴파일러
- 컴파일러는 가능한 연산들을:
   - 하드웨어에서 더 빠르게 실행되는 전용 커널로 대체한다

**파이토치 추론 최적화 사례 (LLaMA-7B)**

- 파이토치 팀은 다음과 같은 단계적 최적화를 적용했다.
1. torch.compile로 더 효율적인 커널 사용
2. 가중치를 INT8 양자화
3. 가중치를 INT4로 추가 양자화
4. 추측 디코딩(speculative decoding) 적용

- 이 과정을 통해 처리량(token/s)이 단계적으로 크게 증가했다.
(실험은 A100 80GB GPU에서 수행)
- 다만 이러한 최적화가 출력 품질에 미치는 영향은 아직 완전히 규명되지 않았다.

**컴파일러 생태계**
- 컴파일러는 독립 도구이거나 프레임워크에 통합되어 사용된다.
- TVM, MLIR, torch.compile, XLA (TensorFlow / OpenXLA), TensorRT (NVIDIA GPU 최적화)
- 많은 AI 기업들은 자체 하드웨어 + 전용 커널 + 자체 컴파일러를 함께 보유해 추론 성능을 극대화한다.

---
## 추론 서비스 최적화
- 대부분 서비스 수준 최적화 기법은 리소스 관리에 집중한다.
- 한정된 리소스(연산과 메모리)와 계속 변하는 작업량이 있을 때, 목표는 지연 시간과 비용을 최적화하도록 작업량에 리소스를 효율적으로 배분하는 것이다.

### 배치 처리
- 비용을 줄이는 쉬운 방법 중 하나
- 운영 환경에서 추론 서비스는 여러 요청을 동시에 받을 수 있다.
- 각 요청을 처리하는 대신, 비슷한 시간에 들어온 요청들을 묶어서 배치 처리하면 서비스 처리량을 크게 높일 수 있다. 
- 배치 처리에는 크게 세 가지 방식: **정적 배치 처리, 동적 배치 처리, 연속 배치 처리**
- **정적 배치 처리**
   - 서비스가 정해진 수의 입력을 하나의 배치로 묶는다.(모든 자리가 찰 때까지 기다렸다가 출발하는 버스)
   - 단점: 배치가 꽉 찰때까지 모든 요청이 실행을 위해 기다려야 한다는 것
- **동적 배치 처리**
   - 각 배치에 최대 대기 시간을 설정
   - 만약 배치크기가 4이고 최대 대기 시간이 100ms라면, 서버는 4개의 요청을 받거나 100ms가 지나면, 둘 중 먼저 일어나는 조건에 따라 배치를 처리한다.(정해진 시간에 출발하거나 만석이 되면 출발하는 버스)
   - 지연 시간을 통제해서 먼저 온 요청이 나중 요청 때문에 지연되는 걸 막아준다.
   - 단점: 배치가 처리될 때 항상 꽉 차 있진 않을 수 있어 **연산 자원 낭비**로 이어질 수 있다.
   ![](https://velog.velcdn.com/images/dkan9634/post/eaaa5ae9-de8e-43f2-b1ec-d561f85abb5f/image.png)
- **연속 배치 처리**
   - 배치의 응답들이 끝나는 대로 바로 사용자에게 반환될 수 있게 한다.
   - 이는 한 응답을 만들 때 다른 응답을 막지 않는 연산들에 골라서 배치 처리하는 방식으로 작동한다.
   - 배치에서 요청이 끝나고 응답이 반환된 후, 서비스는 그 자리에 다른 요청을 배치에 추가해서 배치 처리가 계속 이어지게 할 수 있다.
   - ex) **인플라이트 배치 처리**
   ![](https://velog.velcdn.com/images/dkan9634/post/4a2ee793-b9f1-44cf-ab9f-7c1ad64f6eeb/image.png)

### 프리필과 디코딩 분리
- LLM 추론은 프리필과 디코딩 두 단계로 구성된다.
- **프리필**은 연산 집약적이고 **디코딩**은 메모리 대역폭 집약적이기 때문에, 동일한 장비에서 두 작업을 모두 수행하면 비효율적인 리소스 경쟁이 발생해 TTFT와 TPOT 모두 크게 느려질 수 있다.
- 이를 해결하기 위해 **프리필과 디코딩을 서로 다른 GPU(또는 인스턴스)** 로 분리한다.이 방식은 지연 시간 요구사항을 만족하면서 전체 처리량을 크게 향상시킨다.
- NVLink 같은 고대역폭 연결을 사용하면 중간 상태(KV 캐시) 전송 오버헤드는 크지 않다.
   - 입력이 길고 TTFT가 중요 → 프리필:디코딩 = 2:1 ~ 4:1
   - 입력이 짧고 TPOT가 중요 → 프리필:디코딩 = 1:2 ~ 1:1
   
### 프롬프트 캐싱
- 텍스트의 겹치는 부분을 재사용하기 위해 저장해두므로, 한 번만 처리하면 된다.
- 서로 다른 프롬프트에서 공통적으로 겹치는 대표적인 부분이 **시스템 프롬프트**다.
- 프롬프트 캐시가 없다면, 모델은 모든 질의마다 시스템 프롬프트를 처리해야 한다. 
- 프롬프트 캐시가 있다면, 시스템 프롬프트는 첫 번째 질의에서 단 한 번만 처리하면 된다.
- 프롬프트 캐싱은 긴 문서를 다루는 질의에도 유용
   - ex) 많은 사용자 질의가 동일한 긴 문서와 관련된 경우, 이 긴 문서를 캐시에 저장해 여러 질의에 걸쳐 재사용할 수 있다.
   - ex) 긴 대화를 나눌 때 이전 메시지들의 처리 결과를 캐시에 저장해두고 다음 메시지를 예측할 때 재사용하는 경우에도 유용
- 프롬프트 캐시는 **컨텍스트 캐시 또는 프리픽스 캐시**라고도 불린다.

![](https://velog.velcdn.com/images/dkan9634/post/8e8e8883-4107-4362-8c22-6050044826d1/image.png)

- 시스템 프롬프트가 긴 애플리케이션의 경우, 프롬프트 캐싱은 지연 시간과 비용을 모두 크게 줄일 수 있다.
- 프롬프트 캐싱이 여러 시나리오의 비용과 지연 시간에 미치는 영향
![](https://velog.velcdn.com/images/dkan9634/post/5b0ab986-f9f0-4b96-a49c-e4bdf4f0b782/image.png)

### 병렬 처리
- 가속기는 병렬 처리를 위해 만들어졌고, 병렬 처리 전략은 고성능 컴퓨팅의 핵심이다.
- 모든 모델에 적용할 수 있는 두 가지 병렬 처리 전략 **데이터 병렬 처리, 모델 병렬 처리**
- 특히 LLM에 적용되는 전략 계열은 **컨텍스트, 시퀀스 병렬 처리**가 있다.
- 하나의 최적화 기법에는 여러 병렬 처리 전략이 포함될 수 있다.
- **복제 병렬 처리**
   - 서비스하려는 모델의 복제본을 여러 개 만들어 작업을 병렬화하는 방식
   - 복제본이 많을수록 더 많은 요청을 동시에 처리할 수 있지만, 더 많은 칩이 필요하다.
   - 다양한 크기의 모델을 각기 다른 성능의 칩에 맞추는 건 빈 패킹 문제가 되며, 모델, 복제본, 칩의 종류가 많아질수록 이 문제는 더 복잡해질 수 있다.
- **모델 병렬 처리**
   - 하나의 모델을 여러 머신에 분산시키는 방식
   - 모델 병렬 처리를 사용하면 모델을 여러 칩에 배치하는 문제가 훨씬 더 복잡해질 수 있다.
- 모델을 분할하는 방법은 여러 가지가 있다.
   - 추론에서 가장 일반적인 접근법은 **텐서 병렬 처리**
      - 이는 연산자 내 병렬 처리라고도 알려져 있다.
      - 추론은 행렬 곱셈 같은 다차원 텐서에 대한 일련의 연산자를 포함
      - 이 접근법에선 한 연산자에 포함된 텐서들을 여러 장치에 걸쳐 분할하여, 해당 연산자를 병렬로 실행될 수 있는 더 작은 조각으로 효과적으로 나눈다. => 연산 속도 높일 수 있다.
      
      ![](https://velog.velcdn.com/images/dkan9634/post/41bcc7b2-f0e1-45a5-b73e-29d9150c7bef/image.png)

  - 텐서 병렬 처리의 장점
     - 단일 머신에 맞지 않는 큰 모델을 서비스할 수 있게 한다.
     - 지연 시간을 줄인다. 하지만 지연 시간 감소 효과는 추가적은 통신 오버 헤드 때문에 줄어들 수 있다.
     
  - **파이프라인 병렬 처리**
     - 하나의 배치를 더 작은 마이크로 배치로 나눠 여러 장비에 걸쳐 처리하는 방식이다. 
      - 대규모 모델을 여러 머신에서 실행할 수 있지만, 단계 간 통신 비용 때문에 추론 지연 시간이 증가한다. 
     - 그래서 엄격한 지연 시간 요구가 있는 추론에서는 주로 복제 병렬이 선호되며, 파이프라인 병렬은 처리량이 중요한 학습에서 주로 사용된다.
     
     ![](https://velog.velcdn.com/images/dkan9634/post/fe31ceae-4195-4bd9-b051-14192ac066da/image.png)
-> 4대 머신에서 파이프라인 병렬 처리가 어떻게 보이는지 보여준다.

이 외에도 긴 입력 시퀀스를 효율적으로 처리하기 위한 병렬화 기법들이 있다.

- 컨텍스트 병렬 처리: 입력 시퀀스를 여러 장비로 나누어 각각 처리
(예: 앞부분은 머신 1, 뒷부분은 머신 2)

- 시퀀스 병렬 처리: 입력 처리에 필요한 연산 자체를 여러 장비로 분할
(예: 어텐션은 머신 1, 피드포워드는 머신 2)

---
---
---
# Chap 10. AI 엔지니어링 아키텍처와 사용자 피드백

# AI 엔지니어링 아키텍처
AI 애플리케이션의 가장 단순한 형태: **애플리케이션이 질의를 받아 모델로 보내는 것**

![](https://velog.velcdn.com/images/dkan9634/post/377f9142-d394-44a6-9b50-996a3d25c2fa/image.png)

-> 모델이 응답을 생성해 사용자에게 반환
**모델 API 상자**: 오픈AI, 구글, 앤트로픽 같은 서드파티 API와 자체 호스팅 모델을 모두 가리킴

이렇게 단순한 아키텍처에서 시작해서 필요할 때마다 구성요소를 추가할 수 있다. 그 과정은 대략 다음과 같다.
1. 모델이 정보 수집을 위해 외부 데이터 소스와 도구에 접근할 수 있게 해서, 모델에 입력되는 **컨텍스트를 보강**한다.
2. 시스템과 사용자를 보호하기 위해 **가드레일**을 도입한다.
3. 복잡한 파이프라인을 지원하고 보안을 강화하기 위해 모델 라우터와 게이트웨이를 추가한다.
4. 캐싱을 통해 지연 시간과 비용을 최적화한다.
5. 시스템 성능을 극대화하기 위해 복잡한 로직과 실행 기능을 추가한다.

## 1단계: 컨텍스트 보강
- 컨텍스트는 텍스트 검색, 이미지 검색, 표 형태 데이터 검색 등 다양한 검색 메커니즘을 통해 구성할 수 있다.
   - 또한, 웹검색, 뉴스, 날씨, 이벤트 등의 API를 통해 모델이 자동으로 정보를 수집할 수 있게 해주는 도구를 사용해서 컨텍스트를 보강할 수도 있다.
- **컨텍스트 구성**은 파운데이션 모델을 위한 **특성 공학**과 같다.   
   - 이는 모델이 출력을 생성하는 데 필요한 정보를 제공하는 것

![](https://velog.velcdn.com/images/dkan9634/post/a33d971c-f94a-4f67-86e1-394a83556e04/image.png)

-> 컨텍스트 구성 추가

## 2단계: 가드레일 도입하기
- **가드레일**은 위험을 줄이고 개발자와 사용자를 보호하는 역할
- 위험에 노출될 수 있는 **모든 지점에 가드레일을 배치**해야 한다.

### 입력 가드레일
- 두 가지 유형의 위험을 막아준다.
1. 외부 API로 개인정보가 유출되는 것
2. 시스템을 망가뜨릴 수 있는 악성 프롬프트가 실행되는 것
- 서드파티 API를 사용할 때 잠재적인 유출을 완벽하게 막을 방법은 없지만 가드레일을 통해 줄일 수는 있다.
- 민감 데이터 탐지 도구는 AI를 사용해 잠재적으로 민감할 수 있는 정보를 식별한다. 
   - 특정 문자열이 실제 집 주소와 유사한지 판단하는 방식
   - 만약 질의에 민감한 정보가 포함된 것으로 확인되면, **질의 전체를 차단**하거나 **민감한 정보만 제거**하는 두 가지 선택지가 있다.
   - ex) 사용자 전화번호를 `[전화번호]` 같은 **플레이스홀더**로 마스킹할 수 있다. 만약 생성된 응답에 플레이스홀더가 들어 있으면 다음 그림처럼 **PII 역방향 사전**을 사용해서 플레이스홀더를 원래 정보로 되돌려 마스킹을 해제할 수 있다.
   ![](https://velog.velcdn.com/images/dkan9634/post/67f928fd-913f-4f47-a266-8b9e37e71e2f/image.png)

### 출력 가드레일
- 모델은 여러 방식으로 출력 생성에 실패할 수 있다.
- 출력 가드레일은 다음 두 가지 기능을 수행한다.
1. 출력 실패 탐지
2. 다양한 실패 유형을 처리하는 정책 명시
- 기준에 미치지 못하는 출력을 잡아내기 위해서는 **실패가 어떤 모습인지 알아야 한다.**
   - 모델이 응답해야 하는 상황에서 빈 응답을 내놓는 경우
   - 보통 **품질, 보안**에서 많이 실패한다.
- 보안을 측정할 땐 보안 실패뿐만 아니라 **오거부율 지표(false refusal rate)**
- 많은 실패는 간단한 재시도 로직으로 완화할 수 있다.

![](https://velog.velcdn.com/images/dkan9634/post/d594b02f-2210-4f8b-8160-abd97664d9ea/image.png)


## 3단계: 모델 라우터와 게이트웨이 추가
- 애플리케이션이 더 많은 모델을 다루게 되면, 여러 모델을 서빙하는 데 따르는 복잡성과 비용을 관리하기 위해 라우터와 게이트웨이가 필요해진다.
### 라우터
- 모든 질의에 하나의 모델만 사용하는 대신, 질의 유형별로 각기 다른 솔루션을 사용할 수 있다.
- **장점**
   - 특정 질의에 대해서 범용 모델보다 성능이 더 좋을 수 있는 특화 모델을 사용할 수 있다.
   - 비용 절약(모든 질의에 비싼 모델 하나만 쓰지 않고 단순한 질의는 저렴한 모델을 쓰는 식으로)
- 라우터는 보통 사용자가 무엇을 하려 하는지 예측하는 **의도 분류기**로 구성된다.
- ex) 고객 지원 챗봇과 관련된 의도들
   - 사용자가 비밀번호 재설정을 원하면, 비밀번호 복구에 대한 FAQ 페이지로 안내
   - 청구서 오류 수정 요청하면, 상담원에게 연결
- 의도 분류기는 **시스템이 범위를 벗어난 대화에 빠지는 것을 막을 수 있다.**
   - 질의가 부적절하다고 판단되면 API 호출을 낭비하지 않고 미리 준비된 응답 중 하나를 사용해 정중하게 응답을 거절할 수 있다.
   - ex) "저는 챗봇이라 투표할 수 없습니다. 저희 제품에 대한 질문이 있으시면 ~~"
- 의도 분류기는 시스템이 **애매한 질의를 감지**하고 더 자세히 물어보는 데도 도움이 된다.
   - ex) "Freezing" => 계정을 정지하고 싶은지 날씨를 얘기하는 건지 묻거나 단순히 좀 더 설명해달라고 요청할 수도 있다.
- 다른 종류의 라우터들은 모델이 다음에 무엇을 할지 결정하는 데 도움을 줄 수 있다.
   - 여러 작업이 가능한 에이전트라면 라우터가 다음에 모델이 코드 인터프리터를 사용해야 할지 검색 API를 사용해야 할지 결정하는 **다음 행동 예측기** 역할을 할 수 있다.
- **의도 분류기와 다음 행동 예측기는 파운데이션 모델을 기반으로 구현** 할 수 있다. 
   - **라우터는 빠르고 저렴해야 한다.** 그래야 여러 개를 사용해도 지연 시간과 비용이 크게 늘어나지 않는다.
- **라우팅은 보통 모델이 수행**하기 때문에 다음 그림에서 라우팅을 모델 API 박스 안에 넣었다.

![](https://velog.velcdn.com/images/dkan9634/post/453926f6-7dd5-44b7-bb80-2e7bc5b2c9ff/image.png)

- 라우팅이 **검색보다 먼저** 일어나는 경우가 많다는 점을 염두에 둬야 한다.
- 보통은 `라우팅 - 검색 - 생성 - 스코어링(평가)`의 흐름이 일반적

### 게이트웨이
- 조직이 다양한 모델과 통합되고 **안전한 방식으로 상호작용할 수 있게 해주는 중간 계층**이다.
- 가장 기본적인 기능은 자체 호스팅 모델과 상용 API 뒤에 있는 모델을 포함한 다양한 모델에 통합 인터페이스를 제공하는 것
- 모델 게이트웨이가 있으면 코드 유지보수가 더 쉬워진다.
- 만약 모델 API가 변경되더라도, 이 API에 의존하는 모든 애플리케이션을 업데이트할 필요 없이 게이트웨이만 업데이트하면 된다.

![](https://velog.velcdn.com/images/dkan9634/post/951bdca9-97bf-487e-a97e-2a358e14c4f1/image.png)

- 모델 게이트웨이는 **접근 제어**와 **비용 관리** 기능을 제공한다.
   - API 호출 사용량을 모니터링하고 제한해서, 남용을 방지하고 비용을 효과적으로 관리할 수 있다.
- 속도 제한이나 API 실패(자주 일어남)를 극복하기 위한 **폴백 정책(fallback policy)** 을 만드는 데도 활용할 수 있다.
   - 주요 API를 사용할 수 없을 때, 게이트웨이는 요청을 대체 모델로 보내거나, 잠시 기다린 후 재시도하거나, 다른 방식으로 실패를 원활하게 처리할 수 있다.
- 어차피 모든 요청과 응답이 게이트웨이를 거치게 되므로, 게이트웨이는 **로드 밸런싱, 로깅, 분석** 같은 다른 기능을 구현하기에 가장 적합한 장소다.
- 비교적 구현이 간단해서 바로 쓸 수 있는 게이트웨이가 많이 있다.
   - ex) 포트키의 AI 게이트웨이, MLflow AI 게이트웨이, 웰스심플의 LLM 게이트웨이, 트루파운드리, 콩, 클라우드플레어 등
   
   ![](https://velog.velcdn.com/images/dkan9634/post/92e464f8-3f30-4e92-b405-0a9c8ba87238/image.png)

-> 게이트웨이가 모델 API 상자를 대체한다.


## 4단계: 캐시로 지연 시간 줄이기
- **캐싱**은 오랫동안 SW 애플리케이션에서 **지연 시간과 비용을 절감**하는 데 핵심적인 역할을 해왔다.
   - KV 캐싱, 프롬프트 캐싱을 포함한 추론 캐싱 기법은 Chap9에서 다뤘다.
   
**이 절에서는 시스템 캐싱에 초점을 맞춘다.**
시스템 캐싱 메커니즘은 **완전 일치 캐싱**과 **시맨틱 캐싱**이라는 두 가지 방식이 있다.

### 완전 일치 캐싱
- **정확히 같은 항목이 요청될 때만** 캐시된 항목을 사용한다.
- 사용자가 모델에게 제품 요약을 요청하면, 시스템은 정확히 이 제품의 요약이 캐시에 있는지 확인한다. 만약에 있다면 이 요약을 가져오고, 없으면 제품을 요약한 후 캐시에 저장한다.
- 벡터 검색이 중복되는 것을 피하기 위해 **임베딩 기반 검색**에서도 사용된다.
- 빠른 검색을 위해 **인메모리 저장소**를 사용해 구현할 수 있다.
   - 하지만 인메모리 저장소는 용량이 제한적이므로, 속도와 저장 용량의 균형을 맞추기 위해 `PostgreSQL, Redis` 같은 데이터베이스나 계층형 저장소를 사용해 캐시를 구현할 수도 있다.
   - 캐시 크기를 관리하고 성능을 유지하려면 제거 정책이 중요한데, 가장 최근에 사용된 것부터 제거하는 **LRU**, 가장 적게 사용된 것부터 제거하는 **LFU**, 가장 먼저 들어온 것부터 제거하는 **FIFO** 등이 있다.
- 질의를 캐시에 **얼마나 오래 보관**할지는 해당 질의가 다시 호출될 가능성이 얼마나 높은지에 따라 달라진다.
   - 이 판단을 위해 질의를 캐시에 저장해야 할지 예측하는 **별도의 분류기**를 학습시키기도 한다.
   
### 시맨틱 캐싱
- 들어온 질의와 캐시된 항목이 **의미적으로만 비슷하면 캐시된 항목**을 사용한다.
- 시스템이 새로운 질의를 처음부터 다시 계산하는 대신 첫 번째 쿼리의 응답을 재사용할 수 있다. 하지만 모델의 성능을 저하시킬 수도 있다.
- 두 질의가 유사한지 판단할 수 있는 방법이 확실히 있을 때만 제대로 작동한다.
   - 일반적인 접근법은 **의미적 유사도**를 활용
   => 임베딩해서 임계값보다 높으면 유사한 걸로 판단해서 임베딩과 함께 캐시에 저장
   이 접근법은 캐시된 질의의 임베딩을 저장하기 위해 **벡터 데이터베이스**가 필요
- 다른 캐싱 기법과 비교해볼 때, 시맨택 캐싱의 가치는 좀 애매하다. 구성 요소들이 망가지기 쉽기 때문이다. 
   - 제대로 작동하려면 **좋은 품질의 임베딩, 안정적인 벡터 검색, 신뢰할 수 있는 유사도 측정**이 모두 필요하다. 심지어 적절한 유사도 임곗값을 설정하는 것도 까다로워서 시행착오를 많이 겪어야 한다.
   - 게다가 벡터 검색이 포함되어 있어 시간도 오래 걸리고 연산량도 많다.
   
   ![](https://velog.velcdn.com/images/dkan9634/post/30a4aa39-bd4b-4d57-911d-35304956246e/image.png)

-> 캐시 시스템을 추가한 이미지

## 5단계: 에이전트 패턴 추가
- 지금까지 살펴본 건 각 질의가 순차적인 흐름을 따르기 때문에 아직 꽤 단순한 편이다. 
- 애플리케이션 흐름은 **루프, 병렬 실행, 조건부 분기**를 통해 더 복잡해질 수 있다.
- Chap6에서 다룬 에이전트 패턴은 복잡한 애플리케이션을 개발하는데 도움이 된다.
   - ex) 시스템이 출력을 생성한 후 작업을 완료하지 못했다고 판단하고, 더 많은 정보를 수집하기 위해 또 다른 검색을 수행해야 한다고 결정할 수 있다. 그러면 원래 응답과 새로 검색한 컨텍스트를 **합쳐서**, 같은 모델이나 다른 모델에 다시 넣는다.

![](https://velog.velcdn.com/images/dkan9634/post/01544eda-b24d-49ae-9557-f9cfd8e76d53/image.png)

- 만약 모델에 쓰기 작업 접근 권한을 부여하는 것은 최대한 신중하게 이루어져야 한다. 쓰기 작업이 추가 되면 아키텍처는 다음과 같아진다.

![](https://velog.velcdn.com/images/dkan9634/post/0bc6aeb0-1082-4d4f-ab64-1fec7fcf4f85/image.png)

- 복잡한 시스템은 더 많은 작업을 처리할 수 있지만, 그만큼 실패의 유형과 지점도 다양해져 디버깅이 어려워진다.



## 모니터링과 관찰 가능성
- 제품이 복잡해질수록 관찰 가능성은 더욱 중요해진다.
- 검증된 모범사례와 바로 쓸 수 있는 상용 및 오픈 소스 솔루션을 갖춘 거대한 산업이기도 하다.
- 모니터링의 목표 = 평가 목표
   - **위험을 줄이고 기회를 발견하는 것**
   - 모니터링이 줄여야 할 위험: 애플리케이션 실패, 보안 공격, 드리프트
   
- 시스템의 관찰 가능성 수준을 평가하는 데 도움이 되는 세 가지 지표(feat. 데브옵스 커뮤니티)
   - **평균 탐지 시간(MTTD)**: 문제가 생겼을 때, 이를 감지하는 데 얼마나 걸리는가?
   - **평균 응답 시간(MTTR)**: 감지한 후, 해결되는 데 얼마나 걸리는가?
   - **변경 실패율(CFR)**: 수정이나 롤백이 필요한 실패를 일으키는 변경이나 배포의 비율. 만약 현재 CFR을 모른다면, 플랫폼을 더 관찰 가능하도록 재설계해야 할 때다.
- CFR이 높다고 해서 반드시 모니터링 시스템이 나쁘다는 건 아니다.
- **평가 단계에서 좋은 성능을 보인 모델이면 모니터링에서도 좋은 성능을 보여야 한다.**

> 이 책에서는
> **모니터링**: 시스템의 정보를 추적하는 행위
> **관찰가능성**: 시스템을 계측하고 추적하고 디버깅하는 전체 과정

### 지표
- 모니터링 얘기가 나오면 대부분의 사람들은 지표를 떠올린다. 하지만 지표 자체가 목적이 될 수는 없다.
- 어떤 지표를 추적할지 살펴보기 전에, 어떤 실패 유형을 잡아내고 싶은지 먼저 생각하고 이런 실패들을 중심으로 지표를 설계하는 게 중요하다.
   - ex) 애플리케이션이 환각을 일으키지 않게 하려면 환각을 탐지하는 데 도움이 되는 지표를 설계해야 한다.
   - ex) API 크레딧을 펑펑 써버리지 않게 하려면, 요청당 입출력 토큰 수나 캐시 비용과 캐시 적중률 같은 API 비용 관련 지표를 추적해야 한다.
- **형식(구조) 실패 지표** => 추적 easy
   - ex) JSON 응답을 기대했는데 유효하지 않은 JSON 비율, 잘못된 JSON 중 자동 수정 가능한 비율
   - 모든 오류가 같은 난이도는 아니다.
   - **얼마나 자주 틀리는가 + 얼마나 쉽게 고칠 수 있는가** 둘 다 봐야 한다.
- **생성 품질 지표**(일관성, 간결성, 창의성)
   - 개방형 생성은 정답이 없어서 전통적인 정확도 지표가 안 먹힘
   - 접근 방식
      - **AI 평가자(LLM-as-a-Judge)** 사용
      - 일관성, 간결성, 창의성, 응답 적절성 등으로 점수화
   - 주의
      - 완전 자동화보다는 **추세(trend)** 를 보는 용도에 적합
 - **안전·유해성 지표**
   - 입력/출력에서 **민감 정보가 얼마나 자주 등장하는지**
   - 가드레일이 얼마나 자주 작동하는지, 얼마나 자주 과도하게 거부하는지
   - 이상한 입력도 중요(프롬프트 공격, 엣지 케이스)
   - “모델이 틀렸는가”보다 **“위험해질 수 있는가”를 보는 지표**
   
 - **사용자 행동 기반 품질 지표**
   - 모델 품질은 **사용자 행동**에서도 드러난다.
   - 예시
      - 사용자가 응답 생성 중 **중단하는 비율**
      - 응답을 받은 뒤 **추가 질문을 하는 비율**
      - 프롬프트를 계속 수정하는지 vs 바로 쓰는지
   - 해석
      - 질문을 계속 바꾼다면 → 응답이 만족스럽지 않다는 신호
      - 바로 복사해서 쓰면 → 품질 OK 가능성 높음
- **지연 시간(Latency) 지표는 필수**
   - 사용자가 늘수록 지연 시간 = 비용
   - 대표 지표
       - TTFT: 첫 토큰까지 걸린 시간
       - TPOT: 토큰 하나 생성하는 데 걸리는 시간
       - 총 응답 시간
   - 사용자 수가 늘어날수록 이 지표들을 사용자별 / 세션별로 봐야 확장성 판단 가능
   
- **비용 관련 지표**
    - 단순 토큰 수만 보면 안 됨
    - 같이 봐야 하는 것들: 요청 수, 초당 토큰(TPS), 캐시 적중률
   - 속도 제한 있는 API라면 초당 요청 수를 추적해서 장애 방지
- **표본 검사 vs 전수 검사**
   - 표본 검사
      - 빠르게 문제 탐지
      - 실시간 모니터링에 적합
   - 전수 검사
      - 전체 성능 파악
      - 비용은 비쌈
   - 상황에 따라 둘 다 필요
- **지표는 반드시 쪼갤 수 있어야 한다**
   - 사용자, 릴리즈 버전, 프롬프트 / 체인 버전, 시간대별로 나눠서 볼 수 있어야
 언제, 어디서, 왜 망가졌는지 알 수 있다.
 
### 로그와 트레이스
- 지표는 보통 집계된 값이다. 덕분에 시스템이 어떻게 돌아가고 있는지 한눈에 파악할 수 있지만 지표만으로는 답할 수 없는 질문들이 여전히 많다.
- 특정 활동과 관련된 지표가 급증한 것을 볼 때, 정확한 이유는 알 수 없고 '이런 일이 전에도 있었나?'라고 궁금해할 수 있다. 로그가 이런 질문에 답하는 데 도움을 준다.
- 지표가 속성과 이벤트를 나타내는 수치적 측정값인 반면, 로그는 **추가만 가능한(append-only) 이벤트 기록**이다.
- 로깅의 일반적인 원칙은 **모든 것을 로깅**하는 것
   - 보통 모델 API 엔드포인트, 모델 이름, 샘플링 설정(온도, top-p, top-k, 중단 조건 등), 프롬프트 템플릿 등모든 곳에서 로깅 하기 때문에 로그를 기록할 땐 시스템의 어느 부분에서 온 로그인지 알 수 있도록 **태그와 ID를 부여**해야 한다.
- 로그의 양이 매우 빠르게 증가하기 때문에 자동화된 로그 분석과 로그 이상 탐지를 위한 많은 도구가 AI로 구동된다.
-  **트레이스**는 요청이 다양한 시스템 구성 요소와 서비스를 거쳐 실행되는 경로를 자세히 기록한 것이다.
- AI 애플리케이션에서 트레이스를 하면 사용자가 질의를 보내는 시점부터 최종 응답이 반환되는 시점까지 전체 과정을 볼 수 있다.
   - 시스템이 뭘 했는지, 어떤 문서를 검색했느닞, 모델에 보낸 최종 프롬프트가 무엇인지도 여기에 포함
   - 각 단계에 걸린 시간과 측정할 수 있다면 관련 비용까지 함께 보여줘야 한다.

![](https://velog.velcdn.com/images/dkan9634/post/64fbf67f-2666-4ee7-83f3-d0c159561fba/image.png)

- 트레이스를 활용할 때, 이상적으론 각 질의가 시스템을 거쳐가며 변화하는 과정을 단계별로 추적할 수 있어야 한다.
   - 이렇게 해야 질의가 실패했을 때 정확히 어느 단계에서 문제가 발생했는지 알 수 있다.
   
   
### 드리프트 감지
시스템의 구성 요소가 많을수록 바뀔 수 있는 것들도 많아진다.
- 시스템 프롬프트 변경
- 사용자 행동 변화
- 기반 모델 변경

## AI 파이프라인 오케스트레이션
- **오케스트레이터**는 복잡함을 해소하기 위해 서로 다른 구성 요소들의 상호작용 방식을 정의하여, 엔드투엔드 파이프라인을 구성하고 구성 요소 간에 데이터가 원활하게 흐르도록 보장한다.
- 큰 틀에서 보면, 오케스트레이터는 **구성 요소 정의**, **체이닝** 두 단계로 작동한다.

### 구성 요소 정의
- 시스템이 어떤 구성 요소들을 쓰는지 오케스트레이터에 알려줘야 한다.
- 다양한 모델, 검색을 위한 외부 데이터 소스, 시스템이 사용할 수 있는 도구들이 포함
- 모델 게이트웨이를 사용하면 모델을 더 쉽게 추가할 수 있다.
- 평가나 모니터링용 도구를 쓴다면 그것도 오케스트레이터에 알려줄 수 있다.

### 체이닝
- 기본적으로 함수를 조합하는 것이다.
- 즉, 여러 다른 함수(구성 요소)들을 하나로 엮는다.
- 사용자 질의를 받는 순간부터 작업을 완료할 때까지 시스템이 수행하는 단계들을 오케스트레이터에게 알려준다. 단계들의 예시는 다음과 같다.
1. 원본 질의 처리
2. 처리된 질의를 바탕으로 관련 데이터를 검색
3. 원본 질의와 검색된 데이터를 결합해서 모델이 예상하는 형식의 프롬프트 만들기
4. 모델이 프롬프트를 바탕으로 응답을 생성
5. 응답을 평가
6. 응답이 좋다고 판단되면 사용자에게 반환, 그렇지 않으면 질의를 상담원에게 보냄

> AI 파이프라인 오케스트레이터는 에어플로나 메타플로 같은 일반적인 워크플로 오케스트레이터와는 다르다.

- AI 오케스트레이션 도구는 `랭체인, LlamaIndex, Flowise, Langflow, Haystack` 등 많이 있다.
   - RAG 및 에이전트 프레임워크 또한, 오케스트레이션 도구 역할을 한다.
- 오케스트레이터를 평가할 때 명심해야 할 세 가지 측면은 다음과 같다.
- 애플리케이션 개발 과정의 후반 단계에 접어들면 오케스트레이터를 도입하면 도움이 될 거라고 판단해 도임을 검토하게 되는 경우가 많다. 
**오케스트레이터를 평가할 때 명심해야 할 세 가지 측면**
   - 통합과 확장성
   - 복잡한 파이프라인 지원
   - 사용 편의성, 성능, 확장성

---
# 사용자 피드백
- 사용자 피드백은 애플리케이션에서 두 가지 핵심적인 역할
   - 성능을 평가하고 개발 방향에 정보를 제공하는 것
- 사용자 피드백은 독점 데이터고, 데이터는 경쟁 우위의 원천이기 때문에 훨씬 더 중요한 역할을 한다.

## 대화형 피드백 추출
- 전통적으로 피드백은 **명시적 피드백(explicit feedback)과 암시적 피드백(implicit feedback)** 으로 나눌 수 있다.
   - 명시적 피드백은 좋아요/싫어요, 추천/비추천, 별점 평가, 예/아니오 => **사용자 피드백**
   - 암시적 피드백은 사용자 응답이 아닌 행동에서 추론한 정보
- 사용자가 AI와 대화하는 것 자체가 애플리케이션 성능과 사용자 선호도에 대한 피드백 역할을 한다.
- **암시적 대화형 피드백(implicit conversational feedback)** 은 사용자 메시지의 내용과 의사소통 패턴 둘 다에서 추론할 수 있다.
   - 일상 대화 속에 자연스럽게 섞여 있어서 추출하기가 어렵다.

### **자연어 피드백**
- 사용자와 나눈 대화 내용에서 추출한 피드백
- 대화가 어떻게 진행되고 있는지를 파악하는 데 활용할 수 있다.
- 자연어 피드백 신호 예시

#### 조기종료
- 사용자가 응답 생성을 중간에 멈추거나, 애플리케이션을 종료하거나, 음성 어시스턴트를 사용할 때 모델에게 그만하라고 말하거나, 단순히 에이전트를 방치하는 경우
#### 오류 교정
- 만약 사용자가 후속 질의를 "아니요", "내 말은..."으로 시작한다면 모델의 응답이 빗나갔을 가능성이 높다.
- 오류를 고치기 위해 사용자들은 요청을 다른 방식으로 표현할 수도 있다.

![](https://velog.velcdn.com/images/dkan9634/post/60db2706-3417-449c-8b4c-5c6b1030c7e5/image.png)

-> 모델의 오해를 바로잡으려는 사용자의 시도
이렇게 다른 방식으로 표현하려는 시도는 휴리스틱이나 ML 모델을 사용해 탐지할 수 있다.
- 행동 교정 피드백은 사용자가 에이전트를 더 좋은 행동으로 유도하는 에이전트 활용 사례에서 흔하다. 
- **사용자 편집**은 선호 데이터의 귀중한 데이터가 되는데, 말 그대로 사용자가 무엇을 선호하는지 알 수 있게 해주는 데이터다. 
   - 선호 데이터는 보통 (질의, 선호 응답, 비선호 응답) 형식으로 되어 있고, 모델을 사람의 선호도에 맞게 조정하는 데 사용할 수 있다.
   - 사용자가 편집할 때마다 선호도 예시가 하나씩 만들어진다.
   - 원래 생성된 응답이 비선호 응답이 되고 편집된 응답이 선호 응답이 되는 식
   
#### 불평
- 사용자들은 종종 애플리케이션의 출력을 교정하려고 하지 ㅇ낳고 그냥 불평만 하는 경우도 있다.
- 응답이 틀렸다거나, 관련이 없다거나, 유해하거나, 너무 길다거나, 세부 정보가 부족하다거나, 그냥 별로라고 불평할 수 있다.
- **대화형 대화 및 검색 피드백(FITS)** 데이터셋을 자동 클러스터링해서 얻은 8개의 _자연어 피드백 그룹_을 보여주는 예시

![](https://velog.velcdn.com/images/dkan9634/post/40245ca1-bae0-47f1-a154-339a89a4ec2a/image.png)

#### 감정
- 불평을 아무 이유를 말하지 않고 단순히 부정적인 감정(좌절, 실망, 조롱 등)을 표현할 때도 있다.

### 기타 대화형 피드백
- 메시지 대신 사용자 행동에서 얻을 수 있는 다른 유형의 대화형 피드백들

#### 재생성
- 재생성 신호는 구독 기반 애플리케이션보다 사용량 기반 과금 애플리케이션에서 의미가 더 클 수 있다. 
   - 사용량 기반 과금 방식에선 사용자들이 단순한 호기심으로 추가 비용을 내가며 재생성을 할 가능성이 낮기 때문이다.

![](https://velog.velcdn.com/images/dkan9634/post/3232f55a-938b-45fb-b7db-233106417f66/image.png)

#### 대화 관리
- 삭제, 이름 변경, 공유, 북마크 등 사용자가 대화를 관리하기 위해 취하는 행동 또한, 신호가 될 수 있다.
- 대화 삭제 => 대화가 좋지 않았다는 강력한 신호
- 대화의 이름 바꾸기 => 대화 내용은 좋았지만 자동으로 생성된 제목이 별로였음을 시사

#### 대화 길이
- 대화당 턴 수
- 좋은 신호인지 나쁜 신호인지는 애플리케이션에 따라 다르다.
   - ex) AI 친구 => 긴 대화 좋음
   - ex) 고객 지원처럼 생산성을 목표로 하는 챗봇 => 긴 대화는 봇이 사용자의 문제를 해결하는데 비효율적이라는 뜻일수도

#### 대화 다양성
- 고유 토큰이나 주제 개수로 측정할 수 있는 대화 다양성과 함께 해석할 수 있다.
- ex) 대화는 길지만 봇이 몇 마디 말만 계속 반복한다면, 사용자는 루프에 갇혔을 수 있다.

> **명시적 피드백**은 해석하기 더 쉽지만, 사용자에게 추가적인 노력을 요구한다.
> **암시적 피드백**은 명시적 피드백보다 훨씬 풍부하지만 그만큼 더 노이즈가 많다. => 해석이 어려울 수도 있다.


## 피드백 설계
### 피드백을 수집하는 시점
- 처음 시작할 때
- 문제가 생겼을 때

![](https://velog.velcdn.com/images/dkan9634/post/6e0847ed-e317-46d4-aa77-62ea3703be6a/image.png)

- 모델의 신뢰도가 낮을 때

![](https://velog.velcdn.com/images/dkan9634/post/2769f0f2-3484-405e-9b45-eb5e9cd61dc2/image.png)

![](https://velog.velcdn.com/images/dkan9634/post/07ff4ec8-b756-4841-9c41-33b50233a425/image.png)

![](https://velog.velcdn.com/images/dkan9634/post/1dd57445-196b-41f0-a47c-ccfaec6548cc/image.png)

### 피드백 수집 방법
- 피드백은 사용자의 워크플로에 자연스럽게 녹아들어 별다른 수고 없이 쉽게 피드백을 줄 수 있어야 한다.
- 사용자 경험을 방해하지 않아야 하고, 쉽게 무시할 수도 있어야 한다.
- 추가로 사용자가 좋은 피드백을 제공하도록 유도하는 인센티브도 있어야 한다.
- 좋은 피드백 설계의 예시
   - **이미지 생성 앱 미드저니**
     - 프롬프트마다 한 세트(4개)의 이미지를 생성하고, 아래 그림처럼 사용자에게 다음과 같은 선택지를 제공한다.
      - 확대해서 생성 or 변형해서 생성 or 재생성
   
   ![](https://velog.velcdn.com/images/dkan9634/post/1831da0c-b295-46c0-b80a-39f740d4ee66/image.png)
  
   - **깃허브 코파일럿**
   
   ![](https://velog.velcdn.com/images/dkan9634/post/716bb2e7-de6d-4d61-9f3c-99d7a10b2829/image.png)

- 최대한 사용자의 이해를 돕기 위해 선택지에 **아이콘이나 툴팁**을 추가하자.
- 사용자를 헷갈리게 할 수 있는 디자인은 피해야 한다.

![](https://velog.velcdn.com/images/dkan9634/post/1c8d3593-59ad-453f-84e9-a15eb3a76881/image.png)

- 사용자 피드백을 비공개로 할지 공개로 할지도 신중하게 결정해야 한다.
   - 2024년에 X(구 트위터)는 '좋아요'를 비공개로 전환했는데, 이 변경 후 '좋아요' 수가 크게 늘었다고 일론 머스크가 밝혔다.
- 하지만 비공개 신호는 **발견 가능성**과 **설명 가능성**을 감소시킬 수 있다.
   - 만약 X가 팔로우하는 사람들의 좋아요를 기반으로 트윗을 추천한다면, 좋아요를 숨기는 것은 사용자들이 특정 트윗이 왜 피드에 나타나는지 이해하기 어렵다.




## 피드백의 한계

### 편향
- 사용자 피드백에도 다른 데이터와 마찬가지로 **편향이 있다.**
   - 애플리케이션마다 고유한 편향이 있는데 몇 가지 편향 예시를 살펴보자
   - **관대함 편향**
      - 사람들이 실제보다 더 긍정적으로 평가하는 경향
      - 갈등을 피하고 싶거나, 친절해야 한다고 느끼거나, 혹은 단순히 그게 가장 쉬운 선택지이기 때문에 발생한다.
      - 더 세밀한 피드백을 원한다면, 낮은 평점이 주는 부정적인 느낌을 ㅈ루이는 것도 방법임
      - “훌륭한 여정, 훌륭한 기사입니다.”, “꽤 좋았습니다.”, “불만은 없지만 딱히 뛰어나지도 않았습니다.”, “더 나을 수 있었음.”, “이 기사와 다시는 매칭하지 말아 주세요.” 
      
   - **무작위성**
      - 악의가 아니라 신중하게 생각할 동기가 부족해서 아무렇게나 피드백을 준다.
      - 비교 평가를 위해 긴 응답이 두 개가 나란히 표시될 때 사용자들은 둘 다 읽기 싫어서 그냥 아무거나 클릭할 수 있다.
      

   - **위치 편향**
      - 선택지가 사용자에게 어떤 위치로 제시되느냐가 그 선택지가 어떻게 인식되는지 영향을 준다.
      - 일반적으로 사용자들은 두 번째 제안보다 첫 번째 제안을 클릭할 가능성이 더 높다.
      - 이러한 편향은 제안의 위치를 무작위로 바꾸거나 위치에 따른 제안의 실제 성공률을 계산하는 모델을 만들어서 줄일 수 있다.
      
   - **선호도 편향**
      - 사람들이 나란히 비교할 때 더 긴 응답이 덜 정확해도 선호할 수 있다.
      - 길이는 부정확한 부분보다 눈에 잘 띄기 때문이다.
      - 또 다른 편향은 **최근성 편향**(마지막에 본 응답을 선호하는 경향)
      

### 퇴화 피드백 루프
- **사용자 피드백은 기본적으로 불완전하다는 점을 명심**해야 한다.
- 사용자에게 보여준 것에 대한 피드백만 얻을 수 있기 때문이다.
- 사용자 피드백이 모델의 동작을 수정하는 시스템에서는 **퇴화 피드백 루프**가 생길 수 있다. 
- 퇴화 피드백 루프: **예측 자체가 피드백에 영향을 주고, 이 피드백이 다시 모델의 다음 버전에 영향을 주면서 초기 편향이 점점 심해지는 현상**
- ex) 동영상을 추천하는 시스템에서 순위가 높은 동영상이 먼저 표시되므로 더 많은 클릭을 얻고, 이로 인해 시스템이 그 동영상을 최고의 선택이라고 믿는 것 => **노출 편향, 인기 편향, 필터 버블**
- 퇴화 피드백 루프는 제품의 초점과 사용자층을 바꿀 수 있다.
   - 초기에 제품을 이용하는 소수의 사용자가 고양이 사진을 좋아한다는 피드백을 주면 시스템은 고양이가 포함된 사진을 더 많이 생성하기 시작하는데 이렇게 되면 애플리케이션이 고양이 천국이 되어버린다.
   - 같은 원리로 인종차별, 성차별, 선정적 콘텐츠 선호 같은 다른 편향들도 키워질 수 있다.
- 대화형 에이전트를 사용자 피드백으로 학습시키면 모델이 가장 정확하거나 유익한 응답보다는 **사용자가 듣고 싶어하는 응답**을 하도록 학습될 수 있다고 한다.
- 사람 피드백으로 AI 모델이 아첨하는 경향을 보인다는 연구도 있다.

---
[Chap9(2)](https://velog.io/@dkan9634/AI-Engineering-Chap-9.-%EC%B6%94%EB%A1%A0-%EC%B5%9C%EC%A0%81%ED%99%942)

[Chap10](https://velog.io/@dkan9634/AI-Engineering-Chap-10.-AI-%EC%97%94%EC%A7%80%EB%8B%88%EC%96%B4%EB%A7%81-%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98%EC%99%80-%EC%82%AC%EC%9A%A9%EC%9E%90-%ED%94%BC%EB%93%9C%EB%B0%B1)
