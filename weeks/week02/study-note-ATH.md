[티스토리 링크](https://armugona.tistory.com/entry/AIE-Ch2-%ED%8C%8C%EC%9A%B4%EB%8D%B0%EC%9D%B4%EC%85%98-%EB%AA%A8%EB%8D%B8-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0)
> AI 엔지니어링 책 스터디의 일환으로 책 내용을 기반으로 작성

이번 장에서는 파운데이션 모델을 활용해 실제 서비스를 개발할 때 중요한 영향을 미치는 설계 요소들에 대해 설명한다.

일반적으로 파운데이션 모델의 차이는 **학습 데이터, 모델 아키텍처와 크기, 사람의 의도에 맞추는 사후 학습하는 방식** 등이 있다.

이번 장에서 아래와 같은 주제를 다룬다.

-   모델 개발자가 **학습 데이터**를 수집하는 방법, 학습 데이터의 분포에 초점을 맞춤
-   트랜스포머가 대세로 남아 있는 이유, 새로운 구조는 어떤 모습일지에 대한 질문
-   모델 개발 시 **적절한 모델 크기**를 어떻게 정하는지
-   **사람의 의도 Human preference**란 정확히 무엇인지
-   모델이 선택 가능한 옵션들 중에 어떤 것을 출력으로 선택할 것인지에 대한 **샘플링**이 학습에 미치는 영향

---

## 학습 데이터

-   학습 데이터가 AI 모델의 능력을 결정함
    -   학습 데이터에 한국어가 포함되어 있지 않으면, 모델은 영어로 한국어를 번역할 수 없다.
-   현실적인 데이터 수집의 문제
    -   많은 개발자들은 구할 수 있는 데이터를 최대한 활용하는데 대표적인 학습 데이터 커먼 크롤 Common Crawl 이 있다.
    -   낚시성 제목, 허위정보, 선전/선동, 음모론, 인종차별, 여성 혐오, 수상한 웹사이트 등이 포함되어 있어 품질이 의심스러우나 데이터를 쉽게 구할 수 있다는 이유로 데이터 출처를 공개하는 대부분의 파운데이션 모델이 커먼 크롤을 가공한 데이터를 사용하고 있다.
-   데이터 품질
    -   적은 양의 고품질 데이터로 학습한 모델이 대량의 저품질 데이터로 학습한 모델보다 더 나은 성능을 보일 수 있다.

### 다국어 모델

<img width="906" height="558" alt="image" src="https://github.com/user-attachments/assets/a655c222-c143-4b64-b8cb-5ab8b47e79b7" />


-   이 목록에 포함되지 않는 언어들은 **저자원 언어 _Low-resource language_** 라고 부른다.
    -   특정 언어의 실제 사용 인구 비율이 커먼 크롤에 포함된 데이터 비율과 일치하는 것이 이상적이지만, 실제 사용 인구가 많음에도 커먼 크롤에서 비중이 과소 대표되는 언어들이 있다.

<img width="898" height="559" alt="image" src="https://github.com/user-attachments/assets/56eb50a2-c4dc-4656-8058-44ba1732cd28" />


-   여러 연구에서 밝혀졌 듯 영어가 다른 언어보다 더 좋은 성능을 보이는 것은 당연하다.

-   GPT-4의 경우, MMLU 벤치마크나 수학 문제 해결 능력에서 영어 질문에 대한 정답률이 텔루구어, 마라티어 같은 저자원 언어보다 훨씬 높게 나타난다.
-   특히 버마어나 암하라어 같은 언어로는 수학 문제를 전혀 풀지 못하는 경우도 발생했다.

-   **하지만, 데이터가 적은 것이 성능 저하의 유일한 이유는 아니다**
    -   언어마다 고유한 구조와 문화적 특성이 존재해 모델이 이 부분을 학습하기 어려울 수 있다.
    -   다른 언어의 질문을 영어로 번역해 처리하고 다시 번역하는 방식은 정보가 손실될 수 있다.
    -   영어가 아닌 언어(e.g. 중국어)로 거짓 정보를 더 많이 생성한다는 사실이 발견되었다.
        -   원인은 명확하지 않음, 사전 학습 데이터, 사람 피드백 데이터의 일부 편향 등
    -   언어별 토큰화 효율성에 큰 차이가 있어 영어가 아닌 모델의 응답이 더 느리고 비용이 많이 들 수 있다.
-   이러한 문제를 해결하기 위해 ChatGLM, YAYI, Llama-Chinese(중국어), KoAlpaca(한국어), CroissantLLM(프랑스어), PhoGPT(베트남어), Jais(아랍어) 등 다양한 언어의 모델들이 있다.

### 도메인 특화 모델

제미나이, GPT, 라마 같은 모델은 코딩, 법률, 비즈니스 등 다양한 영역에서 뛰어난 성능을 보입니다. 이는 학습 데이터에 해당 도메인의 데이터가 포함되어 있기 때문이다.

<img width="900" height="644" alt="image" src="https://github.com/user-attachments/assets/be187575-7a3d-40ea-a433-55dcb2183df4" />





이미지 데이터는 텍스트 데이터보다 범주화하기 어렵기 때문에 이미지 데이터의 도메인 분포를 분석한 연구는 많지 않다.

모델의 벤치마크 성능을 통해 해당 모델이 다루는 분야를 추론할 수는 있다.

<img width="902" height="492" alt="image" src="https://github.com/user-attachments/assets/2892fe7b-9c9f-4a5b-9c65-cf83d87b9c85" />


-   범용 모델은 일반적인 질의에는 잘 답하지만, 학습 과정에서 접하지 못한 전문적인 작업에서는 좋은 성능을 내기 어렵다.  
    -   **신약 발견:** 단백질, DNA, RNA 데이터가 필요한데, 이는 획득 비용이 매우 비싸고 공개된 인터넷 데이터에서도 찾기 어렵다.
    -   **암 선별:** X선이나 fMRI 스캔 데이터가 필요하지만, 이는 개인정보 보호 문제로 구하기 어렵다.

-   도메인 특화 작업에서 좋은 성능을 내려면 전문적인 데이터셋이 필요하고 도메인 특화 모델의 예시는 다음과 같다.
    -   **AlphaFold (딥마인드):** 약 10만 개의 단백질 서열과 3D 구조로 학습하여 단백질 구조 예측에 특화
    -   **BioNeMo (엔비디아):** 신약 발견을 위한 생체분자 데이터에 초점을 맞춘 모델
    -   **Med-PaLM 2 (구글):** 의료 데이터와 LLM을 결합하여 의료 질의에 대한 응답 정확도를 높임

다음으로 모델 설계 방식이 성능에 미치는 영향을 살펴보자.

---

## 모델링

어떤 아키텍처를 따라야 할지, 파라미터는 몇 개여야 할지, 이러한 결정은 모델의 능력 뿐만 아니라 추후 서비스 사용성(배포 난이도, 지연 시간 최적화 등)에도 큰 영향을 미친다.

### 모델 아키텍처

언어 기반 파운데이션 모델에서 가장 널리 쓰이는 아키텍처는 Vaswani, Ashish, et al. "Attention is all you need." _Advances in neural information processing systems_ 30 (2017) 에 기반한 아키텍처이다.

이 장에서는 트랜스포머 아키텍처와 한계점, 대안을 살펴본다.

#### 트랜스포머 아키텍처

-   seq2seq는 입력을 처리하는 인코더와 출력을 생성하는 디코더로 구성되며, 입력과 출력 모두 토큰의 시퀀스이다. 
    -   2016년 구글이 이를 구글 번역에 도입하며 기계 번역 품질에 큰 개선을 이뤘다 주장했고 많은 관심을 얻게 되었다.
-   인코더와 디코더로 RNN을 사용한다.  가장 기본적인 형태는 인코더는 입력 토큰을 순차적으로 처리해 입력을 표현하는 hidden state를 출력하고, 디코더는 hidden state와 이전에 생성된 토큰을 기반으로 출력 토큰을 순차적으로 생성한다. 

**seq2seq의 문제점 두 가지**

1.  입력의 모든 정보를 **하나의 최종 은닉 상태**만 사용해 출력 토큰을 생성해 품질이 떨어졌던 기존 방식과 달리, **어텐션 메커니즘**을 통해 출력 생성 시 서로 다른 입력 토큰의 중요도에 가중치를 둘 수 있다.
2.  순차적으로 처리해야 해서 롱 시퀀스를 다룰 때 느렸던 RNN과 달리, RNN을 사용하지 않고 설계되어 입력을 병렬로 처리할 수 있어 입력 처리 속도가 크게 향상되었다.
    -   자기 회귀 언어 모델의 순차적 출력 병목 현상은 남아있다.

**트랜스포머 기반 언어 모델의 추론 두 단계**

1.  **프리필**: 모델이 입력 토큰을 병렬로 처리, 첫 번째 출력 토큰을 생성하는데 필요한 중간 상태를 만든다. 이때 각 입력 토큰의 Key 벡터와, Value 벡터 저장된다. (어텐션 weight를 거친 값이 저장되는 것)
2.  **디코드**: 모델이 출력 토큰을 한 번에 하나씩 생성한다.

<img width="907" height="669" alt="image" src="https://github.com/user-attachments/assets/0482427e-242d-4feb-a000-003815352fcb" />


#### 어텐션 메커니즘

트랜스포머 아키텍처의 핵심인 어텐션 메커니즘은 내부적으로 **Key, Value, Query 벡터**를 활용한다.

더보기

트랜스포머 이전에 어텐션이 등장했고 어텐션 메커니즘은 다른 알고리즘에도 적용할 수 있다. seq2seq에 어텐션 메커니즘을 함께 사용했지만 트랜스포머 이전에 RNN 없이 어텐션을 쓸 수 있다는 것을 보여주기 전까지는 큰 주목을 받지는 못했다.

-   Q는 디코딩 단계에서 디코더의 현재 상태, 요약을 만들기 위해 정보를 찾는 사람
-   K는 이전 토큰, 색인 키워드 (특정 디코딩 단계에서는 이전 토큰에 입력 토큰과 이미 생성된 토큰이 모두 포함된다)
-   V는 모델이 학습한 이전 토큰의 실제 값, 페이지 내용

어텐션 메커니즘은 Q와 K 간의 **내적 dot product**를 통해 입력 토큰에 얼마나 주목할지 계산한다. 점수가 높을 수록 해당 페이지 내용을 높은 비중으로 반영한다는 의미이다.

이전 토큰마다 K와 V가 있기 때문에 시퀀스가 길어질수록 더 많은 K, V값을 계산하고 저장해야 한다. 

트랜스포머 모델의 컨텍스트 길이를 늘리기 어려운 이유 중 하나다. 각 벡터를 효율적으로 계산, 저장하는 방법은 뒤에서 다룬다.

<img width="903" height="764" alt="image" src="https://github.com/user-attachments/assets/bcbf732b-e32e-475b-93a9-145caa802592" />


-   Q, K, V 행렬(W)의 차원은 모델의 은닉 차원에 해당한다.
    -   라마 2-7B에서는 모델의 은닉 차원의 크기가 4096이므로, 각 행렬의 크기는 4096$\\times$4096이다.
    -   Q, K, V 벡터는 각각 4096 차원을 가진다.
-   어텐션 메커니즘은 멀티헤드로 구현된다. 
    -   Q, K, V 벡터가 더 작은 벡터 나뉘어져서 하나의 어텐션 헤드에 할당된다. 라마 2-7B의 경우 32개의 헤드가 있으므로 4096 / 32 = 128개의 벡터로 나뉜다.
    -   각 헤드가 서로 다른 부분에 주목하게 된다. 같은 단어여도 문맥에 따라 여러 가지로 해석될 수 있기 때문이다.

$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d\_k}}\\right)V$$

모든 헤드의 출력은 이어붙여지고 출력 투영 행렬(Output projection, O, 은닉 차원과 같은 크기)을 통과한 뒤 다음 연산 단계로 넘어간다.

-   여러 어텐션 헤드의 정보를 조합하고 적절한 변환을 수행하는 행렬임 (얘도 learnable 파라미터)

#### 트랜스포머 블록

트랜스포머 아키텍처는 여러개의 트랜스포머 블록으로 구성된다. 모델마다 블록 내용이 다를 수 있지만 일반적으로 다음과 같은 모듈을 포함한다.

<img width="1280" height="839" alt="image" src="https://github.com/user-attachments/assets/ee879499-32c6-405a-95f3-27206cddb8fc" />


-   **어텐션 모듈**
    -   Q, K, V, O 4개의 가중치로 구성된다.
-   **MLP 모듈**  
    -   비선형 활성화 함수로 구분된 선형 레이어들로 구성됨
    -   Feed-forward 레이어라고도 함
        -   비선형 함수는 ReLU, GELU(GPT-2/3에서 사용)가 있다

트랜스포머 블록 수를 흔히 해당 모델의 레이어 수라고 한다. 

-   **트랜스포머 블록 이전 임베딩 모듈**
    -   토큰을 임베딩 벡터로 변환하는 임베딩 행렬, 토큰의 위치를 임베딩 벡터로 변환하는 위치 임베딩 행렬로 구성되며, 최종적으로 이 두 벡터를 합산한다. (Element-wise)
    -   각 토큰이 몇 번째에 있는지를 나타내는 위치 색인의 수가 모델의 최대 컨텍스트 길이를 결정한다. (위치 색인 수를 늘리지 않도 컨텍스트 길이를 늘릴 수 있는 기법들도 있다)
-   **블록 이후의 출력 레이어**
    -   모델 출력을 샘플링하는 데 사용되는 토큰 확률로 매핑
    -   언임베딩 Unembedding 레이어로 하나의 행렬로 구성됨 (일부는 출력 생성 전 모델의 마지막 레이어이므로 모델 헤드라고 부름)

<img width="1280" height="492" alt="image" src="https://github.com/user-attachments/assets/80292566-e593-4072-adfd-ad438a57ae9c" />


-   차원 값이 커지면 모델 크기도 커진다. 
-   늘어난 컨텍스트 길이는 모델 메로리 사용량에 영향을 미치지만 파라미터 수에는 영향을 미치지 않는다.
    -   라마는 RoPE(Rotary Positional Embedding)를 사용함 (Learnable한 방식이 아님)

#### 다른 모델의 아키텍처

2017년 이후 계속해서 최적화되었기에 트랜스포머보다 뛰어난 새로운 아키텍처를 개발하는 것은 쉽지 않다. 이를 대체할 아키텍처는 사람들이 사용하는 하드웨어에서 현실적으로 활용 가능한 수준의 성능을 보여줘야 한다.

이 책이 쓰여질 시점에 몇 가지 새로운 아키텍처들이 관심을 받기 시작했다.

롱 시퀀스를 모델링 하는 것은 LLM 개발의 핵심 과제로 남아있다.

-   Peng, Bo, et al. "Rwkv: Reinventing rnns for the transformer era." arXiv preprint arXiv:2305.13048 (2023) (RWKV)로 병렬로 학습을 할 수 있는 RNN 기반 모델이다.
    -   [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM) 
    -   RNN 특성상 이론적으로 트랜스포머 기반 모델이 가진 **컨텍스트 길이 제한**이 없다. (제한이 없다고 해서 롱 컨텍스트에서 좋은 성능이 보장되는 것은 아님)
-   Gu, Albert, et al. "Combining recurrent, convolutional, and continuous-time models with linear state space layers." _Advances in neural information processing systems_ 34 (2021): 572-585 (SSM)은 장거리 의존성 모델링에 큰 잠재력을 보여줬다.
    -   이 연구 이후로 더 효율적이고 롱 시퀀스 처리를 잘하며 더 큰 모델로 확장할 수 있게 해주는 여러 후속 연구가 등장했다.
    -   **[S4](https://arxiv.org/abs/2111.00396):** SSM을 효율적으로 만들기 위해 개발됨
    -   **[H3](https://arxiv.org/abs/2212.14052):** 트랜스포머의 어텐션과 비슷한 역할을 하면서도 연산 효율성이 더 높은 메커니즘을 도입 (초기 토큰을 참조하고 시퀀스 간 토큰을 비교할 수 있는 메커니즘을 포함)
    -   **[맘바 (Mamba)](https://arxiv.org/abs/2312.00752):** SSM을 30억(3B) 파라미터까지 확장한 모델. 동급 트랜스포머보다 성능이 뛰어나고, 추론 연산량이 시퀀스 길이에 비례해 선형적으로 증가(트랜스포머는 제곱으로 증가)하여 훨씬 효율적
    -   **[잠바 (Jamba)](https://arxiv.org/abs/2403.19887):**
        -   트랜스포머 레이어와 맘바 레이어를  교차 배치해 SSM을 더욱 확장
        -   문가 혼합(MoE) 방식을 사용하여 총 520억 파라미터 중 120억 개만 활성화하며, 80GB GPU 한 장에서도 구동 가능하도록 설계
        -   대 256K(약 25만) 토큰의 긴 문맥에서도 강력한 성능
        -   일반 트랜스포머와 비교해 메모리 사용량 적음

<img width="1280" height="1279" alt="image" src="https://github.com/user-attachments/assets/8c1f8f8e-4d27-4027-b56f-b83873e449b9" />


만약 다른 아키텍처가 트랜스포머를 앞지른다면, 이 책에서 다루는 모델 조정 기법 중 일부가 바뀔 수 있다. 그래도 근본적인 접근 방식은 변하지 않을 것.

### 모델 크기

모델의 이름 뒤에 붙는 숫자(예: Llama-13B)는 파라미터 수를 의미하며, 이는 모델의 학습 용량(Capacity)을 나타내는 지표이다. 일반적으로 모델의 파라미터 수가 늘어나면 학습 용량이 커져서 성능이 향상된다

같은 모델 시리즈 내에서도 130억 개의 파라미터를 가진 모델이 70억 개의 파라미터를 가진 모델보다 더 좋은 성능을 보이는 경우가 많다.

-   최근 연구자들이 대규모 모델 학습 방법을 잘 이해하면서 최신 세대들은 같은 크기의 이전 세대 모델들보다 더 좋은 성능을 보인다.

파라미터 수는 해당 모델을 학습하고 실행하는 데 필요한 컴퓨팅 자원을 추정하는 데 도움이 된다.

-   700억 개의 파라미터를 가지고 있고, 각 파라미터가 2바이트(16비트)를 사용한다면, 이 모델이 필요로 하는 GPU 메모리는 최소 140억 바이트(14GB)가 된다. (실제로는 더 크고 다른 장에서 사용량 계산 방법을 설명한다)

만약 모델이 희소하다면 (파라미터중 0인 값이 높은 비율을 차지한다면) 파라미터 수가 모델 크기를 불려줄 수 있다

-   700억 파라미터 모델이 90% 희소하다면 0이 아닌 파라미터는 7억 개뿐
-   데이터 저장과 연산이 더 효율적이라서, 모델이 크더라도 밀집 모델보다 더 적은 컴퓨팅 자원으로 실행할 수 있다.
-   전문가 혼합(MoE, Mixture of Experts) : MoE 모델은 여러 전문가 그룹으로 나뉘고, 그 그룹이 특정 작업에 특화되어 역할을 담당, 토큰을 처리할 때는 이전의 전문가들 중 일부만 활성화
    -   Mistral 8x7B는 8개의 전문가로 구성된 혼합 구조이며, 각 전문가는 70억 개의 파라미터를 가진다
    -   파라미터를 공유하지 않으면 560억 개의 파라미터를 가져야 하지만, 실제로는 467억 개의 파라미터만 가짐.
        -   입력 처리 시 2명의 전문가만 활성화됨, 129억 파라미터 모델 사용하는 것과 같이 효율적

모델의 크기를 논할 땐, **학습에 사용된 데이터의 크기도 고려해야한다.** (정확히는 양, 품질, 다양성 다 중요하지만 여기서는 규모에 초점을 맞춤)

-   대부분의 모델은 데이터셋 크기를 학습 데이터 개수로 측정한다
-   언어 모델을 학습할 때는 문장, 위키백과 페이지, 채팅 대화, 책 등 다양한 데이터를 사용하고 하나의 책이 담고 있는 정보량은 한 문장보다 훨씬 많이 때문에 단순히 개수로 크기를 측정하는 것은 적절하지 않다.
    -   대신 토큰의 개수를 세는 것이 좋은 방법이다.
    -   같은 데이터셋이라도 토큰 수는 다를 수 있고, 모델은 토큰 단위로 작동하기 때문에 토큰 수를 사용한다.
    -   현재 LLM은 조 단위 토큰을 가진 데이터셋으로 학습된다.
        -   토큰 수와 학습 토큰 수는 다르다. 1조개 토큰 데이터셋으로 2 에포크 학습하면 학습 토큰 수는 2조가 된다. (집필 시점에서 큰 모델들은 보통 1 에포크로 사전학습된다.)

<img width="1141" height="442" alt="image" src="https://github.com/user-attachments/assets/376ae414-23a3-4eed-a6df-0b3f7f1b3b34" />


-   **부동소수점 연산 FLOP, Floating point operation** : 모델의 컴퓨팅 요구사항을 나타내는 더 표준화된 단위로, 특정 작업을 수행하는데 필요한 **부동소수점 연산의 개수**를 의미한다.
    -   FLOPs는 작업에 필요한 전체 연산량
    -   FLOP/s는 기계의 최대 처리 성능 측정
        -   혼동을 피하기 위해 오픈AI를 포함한 일부 기업들은 FLOP/s-day를 사용한다
        -   1 FLOP/s-day = 60 \* 60 \* 24 = 86,400 FLOPs

-   **활용률** : 최대 컴퓨팅 성능 대비 실제로 사용할 수 있는 비율
    -   일반적으로 최대 성능의 50%를 달성하면 괜찮은 수준으로 본다.

#### 스케일링의 법칙 : 컴퓨팅 자원 최적 모델 만들기

1.  모델의 성능은 모델의 크기와 데이터셋의 크기에 좌우된다
2.  모델과 데이터셋이 더 커질수록 더 많은 컴퓨팅 자원이 필요하다.
3.  이런 컴퓨팅 자원에는 상당한 비용이 수반된다.

**컴퓨팅 예산이 주어졌을 때** 최적의 모델 크기와 데이터 크기를 계산하는 규칙을 친칠라 스케일링 법칙(Chinchilla scaling law)이라고 하는데, Hoffmann, Jordan, et al. "Training compute-optimal large language models (2022)." _arXiv preprint arXiv:2203.15556_ (2022) 논문에서 제안했다. 

-   컴퓨팅 최적 학습을 위해서는 **학습 토큰 수가 모델 크기의 약 20배**여야 한다는 것을 발견했다.
-   **모델 크기가 두 배가 될 때마다 학습 토큰 수도 두 배가 되어야 한다는 의미**
    -   FLOP 예산에 맞는 최적의 파라미터 개수와 토큰 수를 예측할 수 있고, 예상되는 학습 손실도 예측할 수 있음

<img width="1280" height="504" alt="image" src="https://github.com/user-attachments/assets/31978779-4b3f-40e0-966a-75587e9e7376" />


-   이러한 컴퓨팅-최적 계산은 데이터 획득 비용이 컴퓨팅 비용보다 훨씬 저렴하다는 가정에 기반한다.
    -   같은 논문에서는 학습 데이터 비용이 매우 클 때 적용할 수 있는 또 다른 계산법을 제안한다. (Data-optimal scaling law (Chinchilla))

-   이러한 스케일링 법칙은 컴퓨팅 예산이 주어졌을 때 모델 품질을 최적화한다.
    -   품질이 전부는 아니다, 최적 성능에는 못 미쳐도 다루기 쉽고 추론 비용이 적게 드는, 사용성이 좋은 모델이 널리 사용될 수 있었다. 
    -   [사르다나 등의 연구](https://arxiv.org/abs/2401.00448)는 추론 수요를 고려해 최적의 LLM 파라미터 수와 사전 학습 데이터 크기를 계산하도록 친칠라 스케일링 법칙을 수정했다. 
-   컴퓨팅 예산에 따른 모델 성능과 관련해서 주목할만한 점은 특정 모델 성능을 달성하는 비용이 감소하고 있다는 것
    -   같은 모델 성능에 드는 비용은 감소하고 있지만, 모델 성능 향상에 드는 비용은 여전히 높음
        -   85%에서 90% 보다 90%에서 95%가 더 비쌈
        -   오차율이 2%인 모델이 3%인 모델보다 컴퓨팅 자원, 에너지가 10배 더 필요할 수 있음 [https://arxiv.org/abs/2206.14486](https://arxiv.org/abs/2206.14486)

#### 스케일링 외삽

큰 모델은 한 번 학습에도 자원을 많이 소모하기 때문에 여러 가지 하이퍼파라미터 집합으로 여러 번 학습하고 성능 비교를 하기 어렵다.

-   **스케일링 외삽(scaling extrapolation)(또는 하이퍼파라미터 전이)은** 대규모 모델에서 어떤 하이퍼파라미터가 최상의 성능을 낼지 예측하려는 분야로 등장했다.
    -   현재 접근 방식은 **다양한 크기의 모델에서 하이퍼파라미터가 미치는 영향**을 연구하고, 이 **하이퍼파라미터가 목표 모델 크기에서 어떻게 작동할지 외삽**하는 것

 [X의 Jascha Sohl-Dickstein님(@jaschasd)

Have you ever done a dense grid search over neural network hyperparameters? Like a \*really dense\* grid search? It looks like this (!!). Blueish colors correspond to hyperparameters for which training converges, redish colors to hyperparameters for which tr

x.com](https://x.com/jaschasd/status/1756930242965606582)

-   **창발력 Emergent ability**은 외삽의 정확도를 떨어트린다. 
    -   창발력은 규모가 커져야만 나타나는 능력을 의미하며, 작은 데이터셋으로 학습된 모델에서는 관찰되지 않을 수 있다.

#### 스케일링 병목 현상

지금까지 모델 크기가 10배 증가할 때마다 모델 성능이 향상했다

규모 확장에서 두 가지 병목 현상이 나타나고 있다. 

-   **학습 데이터**
    -   파운데이션 모델은 너무 많은 데이터를 사용하기 때문에 앞으로 몇 년 안에 인터넷 데이터가 부족해질 수 있다는 현실적인 우려가 있다.
    -   학습 데이터셋의 크기 증가율이 새로 생성되는 데이터 증가율보다 훨씬 빠르다
        -   인터넷에 무언가를 올리면 학습 데이터에 포함됐거나 될 것이라고 가정할 수 있는데, 이를 악용해서 일부 사람들은 그들이 원하는 응답을 생성하도록 데이터를 주입하고 있다. 
        -   AI 모델이 생성한 데이터가 인터넷에 빠르게 늘어나고 있다. 새로운 모델들은 부분적으로 AI 생성 데이터를 사용할 것, 연구자들은 이에 대한 성능 저하를 우려한다.
        -   공개 데이터가 소진되면 독점 데이터를 활용하는 것이 추가 학습 데이터를 얻기 위한 가장 현실적인 방법이다. 

<img width="1280" height="837" alt="image" src="https://github.com/user-attachments/assets/4119dd81-d504-492d-9d7c-59e318562d68" />


-   **전기**
    -   데이터 센터는 전 세계 전기의 1%~2%를 소비하는것으로 추정된다.
        -   2030년까지 4%~20% 사이를 도달할 것으로 예상됨
    -   더 많은 에너지를 생산할 방법을 찾기 못하면, 데이터 센터는 최대 50배까지만 성장할 수 있다.
        -   전력 부족에 대한 우려가 생기고, 전기 비용을 상승시킬 것 

---

## 사후 학습

**사후 학습을 통해 해결하고자 하는 사전 학습 모델의 문제점 두 가지** 

-   자기 지도 학습은 모델을 대화가 아닌 텍스트 완성을 잘하도록 학습
    -   사전 학습은 토큰 단위 품질을 높이는 과정, 사용자는 토큰 수준 품질보다는 전체 응답의 품질에 신경 씀
-   인터넷에서 무차별적으로 수집한 데이터로 사전학습하면, 출력물이 차별적이거나 무례하거나 틀린 답일 수 있음

일반적으로

1.  지도 파인튜닝(SFT): 완성이 아닌 대화를 위해, 고품질 데이터로 학습된 모델을 파인튜닝
2.  선호도 파인튜닝: 사람의 선호도에 맞는 응답 출력하도록 파인튜닝

-   사전 학습에 비해 사후 학습은 적은 양의 자원을 소비함 (InstructGPT는 사후학습에 2%의 연산만 할애함)
    -   사후 학습은 모델이 이미 가지고 있지만 **단순 프롬프트로 활용할 수 없는 능력을 끌어내는 과정**

<img width="1260" height="736" alt="image" src="https://github.com/user-attachments/assets/16cb110e-2189-45ef-8a3a-6e55d3740c94" />


### 지도 파인튜닝

-   모델이 적절한 응답을 생성하도록 적절한 응답의 예시를 보여준다.
    -   **시연 데이터(demonstration data):** (프롬프트-응답) 형식
        -   질의응답, 요약, 번역 등 모델이 처리하기 원하는 모든 요청 범위를 포함
        -   AI가 지능적인 대화를 하는 방법을 배울 때도 좋은 레이블러가 중요하다. 시연 데이터는 비판적 사고, 정보 수집, 사용자 요청의 적절성에 대한 판단이 필요한 복잡한 프롬프트가 포함될 수 있다. (고학력 레이블러가 필요)

<img width="1280" height="674" alt="image" src="https://github.com/user-attachments/assets/fbaeaadc-3227-44d1-a10e-5b1364680aac" />


-   모든 기업이 고품질의 사람 데이터 레이블링을 감당할 수 있는 것은 아니다.
    -   비영리 단체 LAION에서 만든 데이터는 자원 봉사자에 의해 생성되었기에 편향성을 제어하기 어려웠음
    -   딥마인드는 인터넷 데이터에서 대화를 필터링하는 데 간단한 휴리스틱을 사용
-   따라서 많은 팀이 고품질 사람 데이터 의존도를 줄이고자 AI 생성 데이터로 눈을 돌리고 있다. (다른 장에서 다룸)

### 선호도 파인튜닝

시연 데이터는 대화하는 법은 가르치지만 어떤 종류의 대화를 해야 하는지 가르치지 않는다. 

인종 차별적인 글이나 비행기를 납치하는 등 이러한 예시는 어떻게 행동하는 것이 옳은지 명확히 알 수 있지만 현실은 그렇지 않다.

서로 다른 배경을 가진 사람들은 항상 의견이 다르고, 민감한 주제, 잠재적으로 논란이 될 수 있는 문제를 AI가 어떻게 정의하고 감지할 수 있을까. 검열의 정도는 어느 정도로 해야 할까?  
  

-   선호도 파인튜닝은 보편적인 사람의 선호도가 존재하다고 가정하고 그것을 AI에 내장할 수 있다고 가정하기 때문에 난이도가 높은 목표다.
-   그만큼 해결책도 복잡하다. 
    -   이 분야에서 널리 쓰이는 알고리즘은 RLHF ( Reinforcement Learning from Human Feedback)이며 아래와 같은 두 부분으로 구성된다.  
        1.  파운데이션 모델의 출력에 점수를 매기는 보상 모델을 학습
        2.  보상 모델이 최대 점수를 줄 응답을 생성하도록 파운데이션 모델을 최적화
    -   여전히 RLHF는 사용되고 있지만 [DPO](https://arxiv.org/abs/2305.18290) 같은 새로운 접근 방식도 인기를 얻고 있다.
        -   Direct Preference Optimization : RL 없이 바로 선호 데이터를 이용해 모델을 정책으로 최적화하는 방법
            -   선택된 답변이 더 가능성이 높아지도록 정책 파라미터 업데이트 (정책 $\\pi\_{\\theta}(y \\mid x)$)
-   RLHF가 DPO 보다는 복잡하지만 모델을 조정할 수 있는 더 많은 유연성을 제공하기 때문에 이 책에서는 RLHF를 다룬다.

#### 보상 모델

RLHF는 보상 모델 기반으로 작동한다.

(프롬프트, 응답) 쌍이 주어지면 응답에 대한 점수를 긴다.

-   신뢰할 수 있는 데이터를 얻는 것이 관건
-   레이블러마다 응답에 대한 점수가 들쭉날쭉할 것
-   같은 샘플에 대해서 같은 레이블러도 다른 점수를 줄 수 있음
    -   각 샘플을 독립적으로 평가하는 것을 **포인트와이즈 평가 _Pointwise evaluation_** 라고 한다

차라리 레이블러에게 두 응답을 비교해서 어떤 것이 더 나은지 고르라 하는 것이 더 쉬운 작업이다.

-   프롬프트에 대해 사람이나 AI가 여러 응답을 생성하고 그 결과로 얻은 레이블 데이터 \`(프롬프트, 선호 응답, 비선호 응답)\` 형식의 비교 데이터가 된다. 
-   좋은 응답의 기준은 사람마다 다르고, 이런 다양한 선호도를 하나의 수학적 공식으로 표현하기는 어렵다.
-   두 응답을 비교하는 작업도 시간이 꽤 소요된다. 
    -   응답의 사실 확인이 필요하기 때문에 수동 비교에 평균 3~5분이 걸린다.

비교데이터만 사용해서 모델이 구체적인 점수를 매기도록 학습하는 과정 (보상 모델 학습 과정)에서는 알맞은 목적 함수를 설계해주면 된다. 자주 사용되는 함수는 선호 응답과 비선호 응답의 출력 점수 차이를 최대화하는 것이다.

$$\\text{loss}(\\theta) = -\\frac{1}{K^2} \\, \\mathbb{E}\_{(x, y\_w, y\_l) \\sim \\mathcal{D}} \\left\[ \\log \\big( \\sigma \\big( r\_{\\theta}(x, y\_w) - r\_{\\theta}(x, y\_l) \\big) \\big) \\right\]$$

_InstructGPT에서 Reward Model에 사용한 공식_

-   $r\_{\\theta}$ : $\\theta$로 파라미터화된 학습 중인 보상 모델
    -   학습 데이터 형식은 $(x, y\_w, y\_l)$
        -   $x$ 프롬프트, $y\_w$ 선호 응답, $y\_l$ 비선호 응답
    -   $s\_w = r\_{\\theta}(x, y\_w)$ : 선호 응답에 대한 보상 모델의 스칼라 점수
    -   $s\_l = r\_{\\theta}(x, y\_l)$ : 비선호 응답에 대한 보상 모델의 스칼라 점수
-   $\\sigma$ : 시그모이드 함수
-   각 학습 샘플 $(x, y\_w, y\_l)$에 대해 손실값은 다음과 같이 계산된다.
    -   $\\log \\left( \\sigma \\left( r\_{\\theta}(x, y\_w) - r\_{\\theta}(x, y\_l) \\right) \\right)$  
        
    -   목표는 모든 학습 샘플에 대한 예상 손실을 최소화하는 $\\theta$를 찾는다.
    -   $- \\mathbb{E}\_{x} \\left\[ \\log \\left( \\sigma \\left( r\_{\\theta}(x, y\_w) - r\_{\\theta}(x, y\_l) \\right) \\right) \\right\]$ _기본적인 RM 학습 수식_  
        -   _위 loss는 한 프롬프트에 K개의 candidate 응답이 있을 때 가능한 모든 쌍을 고려하기 때문에 정규화 상수 $\\frac{1}{K^2}$등장_

어떤 사람들은 보상 모델이 파운데이션 모델의 응답을 평가하려면 최소한 파운데이션 모델만큼 성능이 좋아야 한다고 믿지만, 판단하는 것이 생성하는 것보다 쉽기 때문에 약한 모델도 더 강한 모델을 판단할 수 있다.

#### 보상 모델을 사용한 파인튜닝

-   학습된 보상 모델을 가지고 SFT 모델을 추가로 학습시켜 보상 모델이 높은 점수를 줄 수 있는 응답을 생성하도록 만든다.
-   이 과정에서 사용자가 입력한 프롬프트 등 다양한 프롬프트 집합에서 무작위로 프롬프트를 선택한다.
-   프롬프트를 모델에 입력하면 보상 모델이 그 응답의 점수를 매긴다
    -   이 학습 과정은 **PPO Proximal policy optimization**를 수행한다.

-   실제 경험에 따르면 RLHF나 DPO를 사용할 때 SFT만 사용할 때보다 성능이 좋아진다.
    -   왜 효과가 있는지에 대한 여러 논쟁이 있으며, 이 분야가 발전하면서 선호도 파인튜닝 방식은 앞으로 크게 변할 것으로 예상한다.
    -   RLHF와 선호도 파인튜닝에 대한 추가적인 정보를 원한다면 이 책의 깃허브 참고

 [GitHub - chiphuyen/aie-book: \[WIP\] Resources for AI engineers. Also contains supporting materials for the book AI Engineering (C

\[WIP\] Resources for AI engineers. Also contains supporting materials for the book AI Engineering (Chip Huyen, 2025) - chiphuyen/aie-book

github.com](https://github.com/chiphuyen/aie-book)

-   언젠가 더 좋은 사전 학습 데이터나 파운데이션 모델 학습 기법이 개발되면 SFT, 선호도 기반 학습이 필요하지 않을 수도 있다.
    -   보상 모델만으로도 충분하다는 것을 확인한 경우도 있다.
    -   Best of N 전략을 다음 절에서 살펴보자.

---

## 샘플링

모델은 샘플링이라는 과정을 통해 출력을 생성한다. 

-   이 절에서는 온도 Temperature, top-k, top-p를 포함한 다양한 샘플링 전략과 변수를 살펴본다.
-   여러 출력을 샘플링해서 모델의 성능을 향상시키는 방법을 알아본다.
-   특정 형식과 제약 조건을 따르는 응답을 생성하도록 샘플링 과정을 수정하는 방법을 알아본다.

### 샘플링의 기초

언어 모델은 다음 토큰을 생성하기 위해 먼저 어휘들의 모든 토큰에 대한 **확률 분포**를 계산한다.

<img width="1176" height="484" alt="image" src="https://github.com/user-attachments/assets/9c4f38dc-37b8-4d71-b320-a454c80a6881" />


-   **그리디 샘플링**
    -   항상 가장 가능성이 높은 결과를 선택하는 것
    -   이는 분류 작업에서 잘 작동하는 편
    -   언어 모델의 경우 **지루한 출력**을 만든다.
-   가능성이 높은 것만 고르는 대신 모델은 가능한 모든 값에 대한 확률 분포에 따라 다음 토큰을 샘플링할 수 있다.

<img width="1280" height="438" alt="image" src="https://github.com/user-attachments/assets/63728a6a-d07e-496d-9694-cb4bf9303909" />


-   입력이 주어지면 신경망은 로짓 벡터를 출력한다.
    -   언어 모델에서 로짓은 모델 어휘집에 있는 하나의 토큰을 나타낸다.
    -   로짓은 확률이 아니므로 소프트맥스 Softmax 레이어가 자주 사용된다.
    -   모델의 어휘가 $N$이고 로짓 벡터가 $x\_1, x\_2,...,x\_N$이라고 하면 $i$번째 토큰의 확률 $p\_i$는 다음과 같이 계산된다.

$$\\text{Softmax}(x\_i) = \\frac{e^{x\_i}}{\\sum\_{j} e^{x\_j}}$$

### 샘플링 전략

적절한 샘플링 전략을 사용하면 더 적합한 응답을 생성하도록 만들 수 있다.

-   창의적인 응답을 생성하거나, 예측 가능한 응답을 생성하거나
-   모델이 특정 속성을 가진 응답을 하도록 유도하기 위해 다양한 샘플링 전략이 등장했다.
-   로짓에 접근할 수 있으면 자신만의 샘플링 전략을 설계할 수도 있다.

#### 온도

가능한 값들의 확률을 재분배하기 위해, 온도를 사용해 샘플링할 수 있다.

-   더 **높은 온도**는 **흔한 토큰의 확률을 줄임**으로써, 결과적으로 **희귀한 토큰의 확률을 증가**시킨다.
    -   더 창의적인 응답 생성
-   온도는 소프트맥스 변환 전에 로짓에 나누는 상수, 조정된 로짓이 소프트맥스에 적용됨
    -   온도가 높을수록 명백한 값을 선택할 가능성이 낮아져서 창의적이지만 일관성이 떨어질 수 있다.
    -   온도가 낮을수록 명백한 값을 선택한 가능성이 높아지므로 일관적이지만 지루해질 수 있다.

<img width="1280" height="329" alt="image" src="https://github.com/user-attachments/assets/337b3c95-b983-49d5-b462-502b567176c4" />


-   제공업체는 0~2 사이로 제한한다.
-   0.7은 종종 창의적인 활용 사례에 추천되는데, 창의성과 예측 가능성의 균형을 맞추기 때문이다.
    -   실험을 통해 맞는 온도를 찾을 것
-   모델의 출력을 일관되게 만들기 위해 0으로 설정하는 것이 일반적이다
    -   기술적으로 **0으로 나눌 수 없기 때문에** 온도 값은 절대 0이 될 수 없다.
    -   0으로 설정할 때, 모델은 로짓 조정과 소프트맥스 계산을 하지 않고 **가장 큰 로짓을 가진 토큰을 선택**한다.
-   제공업체는 모델이 생성한 확률을 **로그프롭 logprob** 으로 제공한다.
    -   log probability 줄임말, 로그 스케일로 표현된 확률
    -   신경망의 확률을 다룰 때는 underflow 문제를 줄이기 위해 로그 스케일이 선호된다.
        -   많은 토큰의 확률이 컴퓨터로 표현하기에 너무 작을 수 있어 0으로 내림될 수 있다, 로그 스케일로 이 문제를 줄일 수 있다.
    -   로그프롭은 모델이 내부적으로 어떻게 작동하는지 이해하는데 유용하다.
        -   (모델 작업 시 확률이 무작위로 보이면 학습이 잘 안 되었다는 식으로 디버깅할 수 있음)
        -   로그프롭이 노출되면 다른 사람이 쉽게 모델을 복제할 수 있기 때문에 보안상의 이유로 로그프롭 API를 제한한다.

<img width="1280" height="557" alt="image" src="https://github.com/user-attachments/assets/ddffc613-2faf-4726-9d1a-89315cf91a9e" />


#### top-k

모델의 응답 다양성을 너무 희생하지 않으면서 계산 작업량을 줄이기 위한 샘플링 전략

-   소프트맥스 연산 과정은 계산 비용이 매우 높다
-   이 문제를 피하기 위해 로짓 계산 후, 상위 $k$개의 로짓을 선택한 뒤 이 로짓들에 대해서만 소프트맥스를 수행한다.
-   응답의 다양성에 따라 $k$는 50에서 500 사이일 수 있다. 이후 상위 값들에서 샘플링한다.

#### top-p

top-k에서의 $k$는 상황에 따라 변해야 한다. 

**뉴클리어스 샘플링 nucleus sampling** 이라고 알려진 top-p는 샘플링할 값을 더 동적으로 선택할 수 있게 해준다.

-   가장 가능성이 높은 다음 값의 확률을 내림차순으로 합산하고 합이 $p$에 도달하면 중단한다.
-   누적 확률에 포함된 단어들만 다음 단어 후보로 고려
    -   일반적인 값은 0.9~0.95
-   계산 부하를 반드시 줄여주지는 않는다.
-   컨텍스트에 적절한 문장을 생성할 수 있다.
-   실제로 사용했을 때 잘 작동해서 인기가 있다.

<img width="563" height="438" alt="image" src="https://github.com/user-attachments/assets/8ebe9aca-c986-4cf3-ada9-97921fb8fa59" />


관련된 샘플링 전략으로는 min-p가 있으며, 샘플링 중 고려 대상이 될 토큰의 최소 확률을 정한다.

#### 중단 조건

긴 출력 시퀀스는 더 오래 걸리고 더 많은 돈이 들고 사용자를 짜증나게 할 수 있다.

-   시퀀스 생성을 중단하는 조건 설정에서 가장 간단한 방법은 **일정 개수의 토큰**을 생성한 후에 자동으로 멈추도록 설정하는 것이다.
-   또는 **중단 토큰이나 중단 단어**를 사용하는 것이다.

단점으로는 특정 형식으로 출력이 생성되기를 원하는 경우, 조기 중단으로 인해 형식이 잘못될 수 있다.

### 테스트 시점 연산

이전에 다음 토큰을 샘플링하는 것을 봤다면, 이번에는 **전체 출력을 어떻게 샘플링**할 수 있는지 논의한다.

-   **테스트 시점 연산 Test time compute**
    -   모델의 응답 품질을 향상시키는 간단한 방법, Best of N 기법
    -   하나의 응답만 생성하는 대신, 좋은 응답이 나을 확률을 높이기 위해 무작위로 여러 출력을 생성, **출력의 다양성을 높인다** $\\rightarrow$ 가장 적합한 출력을 선택
        -   다양한 옵션 집합이 더 나은 후보를 산출할 가능성이 높음
        -   무작위로 생성하는 대신 빔 검색을 사용해서 시퀀스 생성 단계마다 가장 가능성이 높은 후보(빔)들을 정해진 개수만큼 생성할 수 있다.
        -   여러 출력을 샘플링하면 그만큼 비용이 많이 든다
-   최고의 출력을 선택하기 위해, **사용자**에게 여러 출력을 보여주고 마음에 드는 것을 고르도록 할 수 있다.
-   자동 선택의 한 가지 방법은 **확률이 가장 높은 출력을 고르는 것**
    -   출력에 있는 모든 토큰의 확률을 곱한 값
    -   시퀀스의 모든 토큰의 로그프롭의 합으로 계산 가능
        -   로그프롭은 보통 음수인데, 숏 시퀀스에 대한 편향을 피하기 위해 위 값을 길이로 나눠 평균 로그프롭을 사용할 수 있다. (평균 로그프롭이 가장 높은 것 선택)
-   **보상 모델**을 사용해 출력 점수 매기기

-   오픈 AI에서 검증기를 학습해 사용하는 것이 모델 크기를 30배 증가시키는 것과 비슷한 성능 향상을 가져왔다.
-   딥마인드는 테스트 시점 연산을 확장하는 것이 파라미터를 확장하는 것보다 더 효율적일 수 있다고 주장한다.

-   LLM의 크기와 구조를 고정하고, 추론 시 상당한 컴퓨팅 자원을 사용할 수 있다면 어려운 프롬프트에 대한 성능을 얼마나 향상시킬 수 있을까?
    -   오픈AI의 실험에서 많은 출력을 샘플링하면 성능이 향상되었지만, 400개를 넘어서면 성능이 감소되었다.  
        -   가설 : 샘플링된 출력 수가 증가하면 검증기를 속일 수 있는 적대적 출력 adversarial outputs을 찾을 가능성도 증가
        -   스탠퍼드의 실험에서 나온 다른 결론 : '몽키 비즈니스'는 샘플 수가 1에서 10,000으로 증가하면 해결된 문제의 수도 로그 선형적으로 증가한다는 것을 발견
            -   흥미롭지만 높은 비용의 실험

<img width="1280" height="855" alt="image" src="https://github.com/user-attachments/assets/063cf6a2-a3b9-4a7b-a4a4-b7c1b8644880" />


-   **휴리스틱**으로 가장 좋은 응답을 고를 수도 있다.
    -   짧은 답변에 유리한 앱이면 짧은 응답 고르기
    -   자연어를 SQL 쿼리로 변환하는 거면 유효한 SQL 쿼리 생성할 때까지 계속 출력하게 하기

-   또 다른 흥미로운 사례로는 **지연 시간 문제를 극복하는 것** 
    -   일부 질의, **생각의 사슬 CoT** 질의는 모델이 응답을 완료하는데 오랜 시간이 걸릴 수 있음
    -   모델에게 여러 응답을 병렬로 생성하도록 한 뒤, 가장 먼저 완성되는 유효한 응답을 사용자에게 보여줌

-   정확한 답을 요구하는 작업에서는 여러 출력 중 가장 많이 나온 결과를 고르기
    -   MMLU 벤치마크에서 제미나이 평가할 때 이런 방식 사용

입력이 조금 달라져도 출력이 크게 바뀌지 않으면 그 모델은 견고하다고 본다.

### 구조화된 출력

**구조화된 출력이 중요한 시나리오 두 가지**

-   **구조화된 출력이 필요한 작업**
    -   가장 일반적인 작업은 시맨틱 파싱
        -   자연어를 기계가 읽을 수 있는 구조화된 형식으로 변환하는 것 (e.g. Text to SQL)
    -   시맨틱 파싱 덕분에 사용자는 직접 쿼리를 작성하는 대신 자연어로 API와 상호작용할 수 있다. 또는 텍스트를 정규식으로 변환하도록 요청할 수 있다.
    -   출력이 유효한 클래스여야 하는 분류도 하나의 예시이다.
-   **다운스트림 애플리케이션에서 출력이 사용되는 작업**
    -   출력이 다음 애플리케이션에서 처리할 수 있는 형태여야 한다.
        -   이메일을 사용하는 다운스트림 애플리케이션이라면 이메일 자체는 구조화된 필요 없지만 JSON 문서 형식이 필요할 수 있다.
        -   이는 에이전트 워크플로우에서 특히 중요하다.

**구조화된 출력을 지원하는 프레임워크**

-   가이던스 guidance, 아웃라인 outlines, 인스트럭터 instructor, llama.cpp

<img width="1280" height="841" alt="image" src="https://github.com/user-attachments/assets/21cc7a65-acfd-4be8-aebf-417735bd67c3" />


#### 프롬프팅

구조화된 출력을 위한 첫 번째 조치 Prompting

-   모델에게 어떤 형식으로든 출력을 생성하도로 지시할 수 있지만, 이 지시를 따를 수 있는지는 모델의 수행 능력과 지시 명확성에 달려 있다.
-   프롬프트 검증 및 수정을 위해 AI를 사용할 수 있다. (AI 평가자 접근 방식의 예시

#### 후처리

후처리 post-processing 은 간단하고 비용이 적게 들지만, 큰 효과를 볼 수 있다.

-   모델이 자주 하는 실수를 파악하면, 수정하는 스크립트를 작성해 오류를 고칠 수 있다. 
    -   e.g. JSON 닫는 괄호 추가해주기
-   수정하기 쉬운 실수에만 효과가 있다, 모델의 출력은 올바른 형식이고 작은 오류가 포함되는 경우 발생

#### 제약 샘플링

제약 샘플링 constraint sampling은 특정 제약 조건에 맞게 텍스트 생성을 유도하는 기법, 구조화된 출력 도구와 함께 사용됨

-   제약 조건을 만족하는 값들 중에서 토큰을 샘플링
-   로짓 벡터에서 제약 조건을 만족하는 토큰만 걸러내고 걸러진 토큰들 중 하나를 뽑는다.

<img width="1280" height="831" alt="image" src="https://github.com/user-attachments/assets/d4189387-54a0-465e-b331-105ae08cf4a8" />


-   허용되는 것, 그렇지 않은 것을 지정하는 문법이 필요함
    -   일반화하기 어려움
-   제약 샘플링에 필요한 자원을 모델 학습하는 것에 투자하는 것이 더 좋다고 생각하는 사람들도 있음

#### 파인튜닝

파인튜닝 fine-tuning은 원하는 형식에 맞는 예시로 모델을 학습시키는 것

-   형식대로 출력을 생성하게 만드는 가장 효과적이고 일반적인 방법
    -   단순 파인튜닝으로 반드시 기대한 형식을 출력하는 건 아니지만 프롬프트만 쓰는 것보단 낫다
-   특정 작업의 경우 모델 아키텍처를 수정해서 출력 형식을 보장할 수 있는데 (e.g. 분류기 헤드를 붙여서 정해준 클래스 중 하나 출력), 이 방식은 **특성 기반 전이 feature-based transfer** 라고 한다.

<img width="1280" height="424" alt="image" src="https://github.com/user-attachments/assets/3a5609ea-b87c-423a-ad1e-cff88d83d6a9" />


-   파인튜닝 시 일부만 학습하거나 전체를 재학습할 수도 있다.
-   모델이 점점 강력해지면 이런 기법들의 중요성도 점차 줄어들 것

### AI의 확률적 특성

AI 모델이 응답을 샘플링하는 방식은 **확률적**, 같은 질문에 대한 응답이 달라질 수 있음

#### 비일관성 inconsistency

1.  **같은 입력, 다른 출력**
2.  **살짝 다른 입력, 완전히 다른 출력** (실수로 대문자로 쓰는 것처럼 아주 조금만 바꿔도 매우 다른 출력이 나올 수도)

<img width="1280" height="544" alt="image" src="https://github.com/user-attachments/assets/f8721bc7-4747-422a-8ca2-22b8316ab354" />


-   **같은 입력에 다른 출력**을 해결할 방법들 
    -   같은 질의가 들어왔을 때 같은 응답을 하도록 응답을 캐시에 저장해서 해결할 수 있다.
    -   온도, top-k, top-p 값과 같은 모델의 샘플링 변수를 고정할 수 있다.
    -   토큰 샘플리에 사용되는 난수 생성기의 시드 변수를 고정할 수 있다.
        -   변수들을 모두 고정해도 100% 일관된 결과를 내는 건 아님 (실행 방식, 숫자 범위, 출력 생성 하드웨어 등 영향을 미치는 것들이 많음)
    -   출력 생성 설정을 고정하는 것은 좋은 방법이지만 시스템에 대한 신뢰를 주지는 못함
-   **살짝 다른 입력, 완전히 다른 출력**인 경우는 더 까다롭다
    -   출력 생성 변수를 고정해도 다른 입력에 대해 다른 출력을 생성하도록 강제할 수는 없음
    -   신중하게 만든 프롬프트와 메모리 시스템을 통해 원하는 응답에 가까운 결과를 생성하게 한다.

#### 환각

사실이 중요한 작업에서 환각 hallucination은 치명적

-   NLG 자연어 생성 분야에서 환각을 탐지하고 측정하는 건 2016년부터 필수

환각의 원인은 미묘하다. 샘플링 과정만으로는 충분히 설명되지 않는다. 

**환각을 일으키는 원인에 대한 두 가지 가설**

-   언어 모델이 주어진 데이터와 자신이 생성한 데이터를 구분하지 못해서
    -   특이한 시퀀스로 시작해서 터무니없는 사실들을 계속 만들어냄 : **자기 기만 self-delusion, 눈덩이처럼 불어나는 환각**
-   모델의 내부 지식과 레이블러의 내부 지식이 일치하지 않아서
    -   SFT 과정에서 모델은 레이블러가 작성한 응답을 모방하도록 학습됨
    -   레이블러가 응답 작성 시 사용한 지식을 같이 전달하면 모델은 해당 응답이 근거 없이 지어낸 것이 아님을 알게되고 자신이 알고 있는 정보만 사용하도록 할 수 있겟지만, 실제로 이렇게 하기는 사실상 불가능

**딥마인드 논문에서 제시한 환각을 줄일 수 있는 두 가지 방법**

-   강화학습을 사용해서 프롬프트와 토큰을 구분하도록 만들기
-   지도학습을 활용해서 학습 데이터에 사실 factual과 반사실 counterfactual 데이터를 포함시키기

LLM이 무엇을 알고 무엇을 모르는지 스스로 파악할 수 있다는 주장

-   이 믿음이 사실이라면 모델이 자신이 아는 정보만을 바탕으로 응답하도록 강제해서 환각을 해결할 수 있음
-   두 가지 해결책
    -   모델에게 응답의 근거가 되는 출처를 검색하도록 요청
    -   다른 하나는 강화학습
        -   보상 모델은 A가 더 나은 이유에 대한 설명 없이 오직 비교만을 사용해 학습
        -   모델이 환각 현상을 보일 때 더 큰 불이익을 주는 방식으로 보상 함수를 환각을 줄이는데 도움이 된다고 주장
        -   But, InstructGPT 논문에서 RLHF가 환각을 증가시킨 것으로 나타났음

<img width="1280" height="593" alt="image" src="https://github.com/user-attachments/assets/8c62a902-5058-4493-8d1d-b2745de65bb3" />


모델이 생성해야 하는 토큰이 적을수록 만들어낼 기회가 적어져서 간결한 응답을 요청하는 것도 환각에 도움이 되는 것으로 보인다. 앞서 논의된 두 가설은 서로 보완적이다. 자기 기만 가설은 자기 지도 학습이 어떻게 환각을 일으키는지에 초점을 맞추고, 내부 지식 불일치 가설은 지도 학습이 어떻게 환각을 일으키는지에 초점을 맞춘다.

환각을 탐지하고 해결하기 위해 사람들은 계속 노력했고, 이후에 4장에서 환각을 어떻게 감지하고 측정하는지 설명하게 된다.
