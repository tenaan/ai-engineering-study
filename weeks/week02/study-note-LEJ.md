
본 포스팅은 칩후옌의 『AI 엔지니어링』을 참고하여 정리한 내용입니다.

---
이번 장은 파운데이션 모델을 활용해 실제 서비스 개발할 때 중요한 영향을 미치는 설계 요소들에 대해 설명한다. 또한 모델 개발자가 학습 데이터를 수집하는 방법을 다루면서 학습 데이터의 분포에 초점을 맞춘다.

사전학습을 통해 모델이 추론 능력을 갖추게 되지만, 안전성이나 사용편의성이 보장되는 것은 아니다. 이런 경우에 사후 학습이 필요하다. 사후 학습의 목표는 모델이 사람의 의도에 맞게 작동하도록 하는 것이다.

사람의 의도(human preference)란 정확히 무엇일까?
모델이 학습할 수 있는 형태로 어떻게 표현할 수 있을까?
이런 모호함으로 인해 모델 개발자가 자신의 모델을 조정하는 방식은 모델의 사용성에 큰 영향을 미친다.

대부분의 사람은 모델 성능에 학습이 미치는 영향은 **샘플링**이 미치는 영향은 쉽게 간과한다. 샘플링은 모델이 선택 가능한 옵션들 중에서 어떤 것을 출력으로 선택할 것인지에 대한 것이다. 아마도 AI에서 매우 저평가된 개념이다. 샘플링은 환각과 비일관성을 포함해 이해하기 힘든 AI 행동들을 설명할 뿐만 아니라, 적절한 샘플링 전략을 선택하면 비교적 적은 노력으로도 모델의 성능을 크게 향상시킬 수 있다.

---
# 학습 데이터
대규모 모델 학습에 필요한 데이터를 충분히 확보하는 것은 어렵고 비용도 많이 든다. 모델 개발자들은 원하는 데이터를 구하기 어려워, 한정적이긴 하지만, 구할 수 있는 데이터를 최대한 계속 활용하는 경우가 많다.
ex) 커먼 크롤(Common Crawl) 
- 비영리 단체가 인터넷 크롤링으로 주기적으로 데이터셋을 만든다.
- 구글은 커먼 크롤의 정제된 부분집합인 C4 데이터셋을 제공한다.
    - 이는 T5 모델의 데이터셋으로 쓰였다.
- 가짜 데이터셋에 많이 포함되어 있지만 **데이터를 쉽게 구할 수 있다는 이유**로 GPT-3와 제미나이처럼 학습 데이터 출처를 공개하는 대부분의 파운데이션 모델이 커먼 크롤을 가공한 데이터를 사용하고 있다. 또한, 학습 데이터를 공개하지 않는 모델도 커먼 크롤이 사용되고 있을 것으로 추측된다.
    - 일부 팀들은 저품질 데이터를 걸러내기 위해 휴리스틱 사용
ex) 레딧에서 최소 3개 이상의 추천을 받은 게시글만 사용

데이터가 많다고 해서 반드시 모델의 성능이 좋아지는 것은 아니다. 예를 들어, 적은 양의 고품질 데이터로 학습한 모델이 대량의 저품질 데이터로 학습한 모델보다 더 나은 성능을 보일 수 있다. ex) 구나세카르 등의 연구(2023)

## 다국어 모델
- 저자원 언어(low-resource language) : AI를 학습시키는 데 사용할 수 있는 데이터가 부족한 언어 <-> 고자원 언어
- 언어 모델에서 성능 저하가 나타난 원인 중 하나는 해당 언어가 적게 포함되어 있기 때문이다.
- 그럼 다른 언어로 된 모든 질의를 압도적인 고자원 언어인 영어로 번역하고, 응답을 받은 뒤 다시 원래 언어로 번역하면 되지 않을까?	
    - 1) 데이터가 적은 언어를 충분히 이해해 번역할 수 있는 모델 필요
    - 2) 번역 과정에서 정보가 손실될 수 있다.
- 영어가 아닌 언어에서도 예상치 못한 성능 문제가 발생할 수 있다. 예를 들어, ChatGPT가 영어보다 중국어로 잘못된 정보를 더 많이 생성한다.(원인은 명확X)
- MASSIVE에서 GPT-4를 벤치마킹한 결과, 동일한 의미를 전달하려면 버마어와 힌디어와 같은 언어가 영어나 스페인어보다 훨씬 많은 토큰이 필요하다는 사실을 발견했다.
- 같은 내용을 처리할 때 시간도 오래 걸리고 토큰 사용량도 훨씬 크므로 이를 해결하기 위해서 **많은 모델이 비영어권에 초점을 맞춰 학습됐다. **
    - 중국어: ChatGLM, YAYI, Llama-Chinese
    - 한국어: KoAlpaca
    - 프랑스어: CroissantLLM
    - 베트남: PhoGPT
    - 아랍어: Jais

## 도메인 특화 모델
- 제미나이, GPT, 라마 같은 범용 모델은 다양한 영역에서 놀라운 성능
- 그 이유는 학습 데이터의 여러 도메인의 데이터가 포함되었기 때문이다.
![](https://velog.velcdn.com/images/dkan9634/post/2ea3834a-eb21-4ac1-9d4f-6dd717e6e3c8/image.png)
- 모델의 벤치마크 성능을 통해 해당 모델이 다루는 분야를 추론할 수 있다.
- 범용 파운데이션 모델이 다양한 분야의 일반적인 질의에 답할 수는 있지만, 학습 과정에서 접하지 못한 도메인 특화 작업에서는 좋은 성능을 내기 어렵다. 
- 도메인 특화 작업에서 모델이 좋은 성능을 내려면, 해당 분야의 전문적인 데이터셋이 필요하다.
    - DeepMind의 알파폴드(AlphaFold): 약 10만 개의 단백질 서열과 3D 구조로 학습
    - 엔비디아의 바이오네모(BioNeMo): 신약 발견을 위한 생체분자 데이터에 초점
    - 구글의 메드-팜2(Med-PaLM2): 의료 데이터와 LLM을 결합해 의료 질의에 대한 정확도를 높인다.

---
# 모델링
모델을 학습하기 전에 개발자는 모델을 어떻게 설계할지 결정해야 한다. (어떤 아키텍처? 파라미터 몇 개? 등등)
결정의 배경이 되는 요소들을 살펴보자.

## 모델 아키텍처
오늘날 언어 기반 파운데이션 모델에서 가장 널리 쓰이는 아키텍처는 <Attention Is All You Need\>에 기반한 아키텍처다. 이 아키텍처는 이전 모델들의 **여러 한계를 극복했고, 트랜스포머의 성공**을 가져왔다.

### seq2seq
- 기계 번역과 요약에서 성능을 개선한 seq2seq가 텍스트 시퀀스를 다루는 과제에서 널리 쓰였다.
- 인코더-디코더 구조이고 둘 다 RNN을 사용
- 인코더는 입력 토큰을 순차적으로 처리해 입력을 표현하는 최종 은닉 상태를 출력한다.
- 디코더는 이 최종 은닉 상태와 이전에 생성된 토큰을 기반으로 출력 토큰을 순차적으로 생성한다.
![](https://velog.velcdn.com/images/dkan9634/post/5ee2e4fc-4d5f-498c-85b5-975d216aba86/image.png)
- 문제점
    - (1) 디코더는 입력의 최종 은닉 상태만 사용해 출력 토큰을 생성하므로 최종 생성되는 출력물의 품질이 떨어진다.
    - (2) RNN의 인코더와 디코더는 입력 처리와 출력 생성이 순차적으로 이루어져야 하므로 롱 시퀀스를 다룰 때 속도가 느려진다.
    	- 다음으로 넘어가기 전에 각 입력 토큰의 처리가 끝날 때까지 기다려야 한다.


### 트랜스포머 아키텍처
- 트랜스포머 아키텍처는 **어텐션 메커니즘**으로 두 문제를 해결한다.
- 어텐션 메커니즘을 통해 모델은 각 출력 토큰을 생성할 때 서로 다른 입력 토큰의 중요도에 가중치를 둘 수 있다.
> 사실 어텐션 메커니즘은 트랜스포머 논문이 발표되기 3년 전에 이미 등장하였고 트랜스포머 논문에서 RNN 없이 어텐션 메커니즘을 사용할 수 있다는 것을 보여주기 전까지는 AI 전반으로 큰 주목을 받진 못했다.

![](https://velog.velcdn.com/images/dkan9634/post/9d730282-e0be-41ea-a818-7af0f635a9f7/image.png)
- RNN을 사용하지 않고 설계되었고 입력 토큰을 병렬로 처리할 수 있어, 입력 처리 속도가 크게 향상되었다.
- 순차적 입력의 병목 현상을 제거했지만 트랜스포머 기반의 자기회귀 언어 모델은 여전히 **순차적 출력의 병목 현상**이 남아 있다.
    - 트랜스포머 기반의 자기회귀 언어 모델은 텍스트를 한 토큰씩 순차적으로 예측해서 생성하는 방식(auto-regressive)이라, 전체 출력을 한 번에 병렬로 처리할 수 없다는 의미

트랜스포머 기반 언어 모델의 추론은 두 단계
**1. 프리필 단계 (Prefill Phase)**
- 입력 전체를 한 번에 병렬 처리하는 단계
- 첫 번째 출력 토큰을 생성하는 데 필요한 중간 상태를 만든다.
- 이때 각 입력 토큰의 key 벡터와 value 벡터가 중간 상태에 저장된다.

**2. 디코드 단계 (Decode Phase)**
- 모델이 출력 토큰을 한 번에 하나씩 생성한다.
- auto-regressive하기 때문에 병렬 불가능하다.

### 어텐션 메커니즘
- 트랜스포머 아키텍처의 핵심
- 이 책에서는 LLM Autoregressive 디코딩 부분을 설명한다.
- GPT, LLaMA 같은 LLM은 토큰을 하나씩 생성하는 구조다. 이때 매번 전체 입력을 다시 계산하면 매우 비효율적이기 때문에 **KV Cache**를 사용한다.
- **Query(Q)**는 현재 디코딩 단계에서 현재 토큰의 hidden state로부터 계산된다.

- **Key(K)**는 현재 시점 이전까지의 모든 토큰에서 계산된 값이다.
    - Encoder에서는 전체 입력을 모두 보므로 Key = 전체 토큰
    - Decoder(LLM)에서는 Masked self-attention 때문에 Key = 이전 토큰만
- **Value(V)**는 이전 토큰에서 선형변환(Wᵥ)을 통해 만들어진 정보 벡터이며,
attention weight이 Value에 곱해져 최종 출력이 생성된다.
![](https://velog.velcdn.com/images/dkan9634/post/37f2f50c-2f9a-4258-8df0-ef11262389c4/image.png)

- 어텐션 메커니즘은 대부분 멀티헤드로 구현된다. 멀티헤드를 사용하면 모델이 서로 다른 이전 토큰 그룹들을 동시에 주목할 수 있다.
- 멀티헤드 어텐션은 쿼리, 키, 값 벡터가 더 작은 벡터들로 나뉘어 각각 하나의 어텐션 헤드에 할당된다.
ex) 라마 2-7B는 32개의 어텐션 헤드가 있고 모델의 은닉 차원 크기가 4096차원이므로 K, V, Q 벡터는 4096 / 32 = 128차원을 가진 32개의 벡터로 나뉜다.
- 모든 어텐션 헤드의 출력은 이어 붙여진다. 

$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^{T}}{\sqrt{d}}\right) V$


### 트랜스포머 블록
- 트랜스포머 아키텍처는 여러 개의 트랜스포머 블록으로 구성된다. 블록의 정확한 내용은 모델마다 다르다.
- 일반적으로 각 트랜스포머 블록은** 어텐션 모듈과 다층 퍼셉트론(MLP) 모듈**을 포함한다.
- 트랜스포머 **블록 수** == 해당 모델의 **레이어 수**

![](https://velog.velcdn.com/images/dkan9634/post/262b2bf3-4375-4e4d-9039-86880b90b4d7/image.png)
#### 트랜스포머 블록 이전: 임베딩(Embedding)
- 트랜스포머 입력은 두 가지 임베딩을 합쳐 만든다
1. **토큰 임베딩(Token Embedding)** : 단어의 의미를 벡터로 표현한 것
2. **Positional Encoding** : 단어의 순서를 모델에 알려줌
	-> 토큰 임베딩에 위치에 대한 벡터값을 더해주는 것을 의미하고 같은 토큰이라도 어느 위치인지에 따라 최종 벡터값이 달라지므로 모델 성능에 더욱 긍정적인 영향
    ![](https://velog.velcdn.com/images/dkan9634/post/2afd037b-56f7-44b7-b2eb-fa896b54c41c/image.png "Positional Encoding 예시")
    
둘을 더해 최종 입력 임베딩을 만든 뒤 첫 트랜스포머 블록으로 전달한다.

#### 어텐션 모듈
위에서 설명한 어텐션 메커니즘

#### MLP 모듈(Feed-Forward Network, FFN)
- 어텐션으로 얻은 출력은 다시 MLP(FFN) 모듈로 들어간다.
- MLP는 다음 두 개의 선형 레이어로 구성된다.
1. FF1: 선형 레이어 + 비선형 활성화
2. FF2: 선형 레이어(출력 차원 맞춰줌)


#### 트랜스포머 블록 이후: 출력 레이어(Output Layer)
- 모델의 마지막 출력 벡터(최종 hidden state)를 어휘 확률로 변환하는 단계
- 일반적으로 언어 모델의 unembedding 레이어로 구성된다.


![](https://velog.velcdn.com/images/dkan9634/post/15ba82ac-e83b-460e-9ad9-531c2f3064b0/image.png)

- 차원 값이 커지면 모델 크기도 커진다. 
- 컨텍스트 길이가 늘어나면 모델의 메모리 사용량에 영향을 미치지만, 모델의 총 파라미터 수에는 영향을 미치지 않는다.
![](https://velog.velcdn.com/images/dkan9634/post/3913f39f-91e7-48bf-ac48-e6b3ce0904bc/image.png)

### 다른 모델의 아키텍처
- 트랜스포머 모델이 대세이긴 하지만, 다른 아키텍처도 있다.
- 2014~2018: seq2seq
2014~2019: GAN
2017~ing: Transformer
- **RWKV**
    - 병렬로 학습을 할 수 있는 RNN 기반 모델
    - RNN 특성상 이론적으로 컨텍스트 길이 제한이 없지만 롱 컨텍스트에서 좋은 성능이 보장되지는 않는다.
- **SSM**
    - 롱 시퀀스를 모델링하는 것은 LLM을 개발할 때 핵심적인 과제로 남아 있다. 장거리 의존성 모델링에 큰 잠재력을 보여준 아키텍처는 
    - 2021년 이 아키텍처가 소개된 이후, 더 효율적이고 롱 시퀀스 처리를 잘하며 더 큰 모델로 확장할 수 있게 해주는 여러 후속 연구가 등장했다.

![](https://velog.velcdn.com/images/dkan9634/post/7f57dbcb-78f4-4650-a567-761c49d4924b/image.png)

## 모델 크기
- 최근 몇 년간의 AI 발전은 모델 크기가 커진 덕분이다.
- 파운데이션 모델을 설명할 때 파라미터 수가 중요한데, 보통 모델 이름 뒤에 표시된다.
- 라마-13B는 130억 개의 파라미터를 가진 버전
- 모델의 파라미터 수가 늘어나면 학습 용량이 커져서 성능이 향상된다.(반대의 경우도 있음)
- 파라미터 수는 모델을 학습하고 실행하는 데 필요한 컴퓨팅 자원을 추정하는 데 도움이 된다.
    - 모델 파라미터 = 70억 개 (7B)
    - 각 파라미터 = 2바이트(= 16bit)
    - 최소 필요한 메모리: 70억x2byte = 140억byte(=14GB)
- 그런데 파라미터 수만 보면 희소성(sparsity) 때문에 착각할 수 있다.
    - 모델이 희소하다는 건 파라미터 중 0인 값이 높은 비율을 차지한다는 것
     
    
> 파라미터 수는 모델의 기본 크기를 가늠하는 지표지만, 모델이 희소(Sparse) 구조라면 실제 저장 공간과 연산량은 훨씬 줄어든다. 
**따라서 동일한 파라미터 수라도 희소 모델이 밀집(Dense) 모델보다 더 적은 메모리와 연산으로 동작할 수 있다.**

- 최근 몇 년간 인기를 얻은 희소 모델의 종류 중 하나는 **전문가 혼합**(mixture-of-experts(MoE)
    - 여러 파라미터 그룹으로 나누고, 각 그룹이 특정 작업에 특화된 전문가 역할을 담당한다.
    - 전체 파라미터는 매우 크지만, 한 번의 입력에서 실제 사용하는 파라미터는 그중 일부만한다.
    - ex) 믹스트랄 8x7B
    	- 전문가는 8개로 구성, 각 전문가는 7B 파라미터
       - 8개 모두 완전 독립이면 총 파라미터는 8*7B = 56B
        - 하지만 실제는 일부 파라미터를 공유해서 46.7B 파라미터
        - 그런데 각 입력을 처리할 땐 2명만 활성화되어 467억 개의 파라미터를 가지고 있지만, 비용과 속도는 마치 129억 파라미터 모델을 사용하는 것처럼 효율적이다.
- 더 큰 모델이라도 충분한 데이터로 학습하지 않으면 더 작은 모델보다 성능이 떨어질 수 있다.
- 모델 크기를 논할 땐 학습에 사용된 데이터의 크기도 고려하는 게 중요한데, 대부분의 모델은 데이터셋 크기를 학습 데이터 개수로 측정한다.
    - 그렇다고 단순히 데이터의 개수로 데이터셋 크기를 측정하는 것은 적절하지 않다. 대신 데이터셋에 포함된 토큰의 개수를 세는 것이 더 좋은 측정 방법이다.
    - 근데 또 같은 데이터셋이라도 모델마다 토큰화 방식이 달라서 토큰 수가 다르게 나올 수 있기 때문에 토큰 수가 완벽한 기준은 아니다....
- 데이터셋의 토큰 수와 학습 토큰 수는 다르다. 
    - 학습 토큰 수: 모델이 실제로 학습한 토큰의 총량을 의미
    - 예를 들어, 1조 개의 토큰을 가진 데이터셋으로 2번 반복 학습(2epoch)을 한다면 학습 토큰 수는 2조가 된다.
    - 데이터셋 토큰 수: 데이터셋에 포함된 토큰 수
    ![](https://velog.velcdn.com/images/dkan9634/post/4ffb0a1e-c77b-4db4-8ba1-e98799f4ac59/image.png)
    
모델의 컴퓨팅 요구사항을 나타내는 더 표준화된 단위는 **부동소수점 연산**(floating point operation, FLOP)

#### FLOP
- 특정 작업을 수행하는 데 필요한 부동소수점 연산의 개수
- 모델을 학습시키는 데 필요한 연산 횟수(계산량)의 총합

#### FLOPs
- 작업에 필요한 전체 연산량을 측정

#### FLOP/s
- FLOP per second
- 초당 몇 개의 FLOP(연산)를 수행할 수 있는가?
- 기계의 최대 처리 성능 측정
- GPU의 성능(연산속도)

>FLOP/s 는 연산 속도
FLOP 또는 FLOPs 는 연산량

- 활용률은 최대 컴퓨팅 성능 대비 실제로 사용할 수 있는 비율
- **총 비용=(GPU 대당 비용/시간)×(GPU 대수)×(총 학습 시간)÷(GPU 활용률)**

> **모델의 규모를 나타내는 세 가지 숫자**
1. **파라미터 수**: 모델의 학습 용량을 나타냄
2. **모델이 학습한 토큰의 수**: 모델이 얼마나 많이 학습했는지
3. **FLOPs의 수**: 학습 비용을 나타냄


### 스케일링의 법칙: 컴퓨팅 자원 최적 모델 만들기
- 고정된 컴퓨팅 예산 내에서 최고 성능을 달성할 수 있는 모델을 **컴퓨팅-최적 모델**(compute-optimal model)
- 컴퓨팅 예산이 주어졌을 때 최적의 모델 크기와 데이터셋 크기를 계산하는 규칙을 **친칠라 스케일링 법칙**
- 컴퓨팅 최적 학습을 위해서는 **학습 토큰 수가 모델 크기의 약 20배**여야 한다는 것을 발견했다. ex) 30억 개의 파라미터를 가진 모델은 약 600억 개의 학습 토큰이 필요하다
- 스케일링 법칙은 컴퓨팅 예산이 주어졌을 때 모델 품질을 최적화한다. (하지만 실제 제품은 모델 품질이 전부가 아니라는 점을 기억하자.)
    - 라마 개발자들은 주어진 컴퓨팅 예산으로 더 나은 성능을 보이는 큰 모델을 선택할 수도 있었지만, 더 작은 모델을 선택했다. 그 결과로 모델이 다루기 쉽고 추론 비용이 적게 들어 널리 사용될 수 있었다.
    => 2023년 사르다나 연구는 이런 추론 수요를 고려해 최적의 LLM 파라미터 수와 사전 학습 데이터 크기를 계산하도록 친칠라 스케일링 법칙을 수정했다.
- **특정 모델 성능을 달성하는 비용이 점점 감소하고 있다.**
    - 같은 모델 성능에 드는 비용은 감소하고 있지만, 모델 성능 향상에 드는 비용은 여전히 높다.
    
### 스케일링 외삽
- 모델 성능은 하이퍼파라미터 값에 크게 좌우된다.
- ** 대규모 모델에서 어떤 하이퍼파라미터가 최상의 성능을 낼지 예측하려는 연구 분야**로 등장
- 현재 접근 방식은 다양한 크기의 모델에서 하이퍼파라미터가 미치는 영향을 연구하고, 이 하이퍼파라미터가 목표 모델 크기에서 어떻게 작동할지 외삽하는 것이다.

### 스케일링 병목 현상
- AI 모델이 필요로 하는 데이터의 양이, 인간이 실제 인터넷에 새로 만들어 내는 데이터 양보다 훨씬 빠르게 늘고 있다.
- 따라서 이젠 인터넷에 무언가를 올렸다면, 동의 여부와 관계없이 이미 일부 언어 모델의 학습 데이터에 포함됐거나 포함될 것이라고 가정해야 한다.
![](https://velog.velcdn.com/images/dkan9634/post/5907bc53-2002-4c1d-99d5-5b9a7726a270/image.png)
- 많은 기업이 자사 데이터를 다른 회사가 AI 모델 학습용으로 스크래핑 하는 것을 막기 위해 데이터 이용 약관을 변경하는 건 어찌 보면 당연한 결과이다.
- 다른 시급한 문제는 전기이다.

---
# 사후 학습
- 사전 학습된 모델에서 시작
- 현재 사용되는 사전 학습 방식 때문에, 사전 학습된 모델은 일반적으로 두 가지 문제가 있다. 
    - (1) 자기 지도 학습은 모델을 대화가 아닌 텍스트 완성을 잘하도록 학습한다.
    - (2) 인터넷에서 무차별적으로 수집한 데이터로 모델을 사전 학습하면, 그 출력물이 인종 차별적이거나 성차별적이거나, 무례하거나, 그냥 틀린 답일 수 있다. 
- 사후 학습 목표: 이런 두 가지 문제를 모두 해결하는 것
- 모든 모델의 사후 학습은 조금씩 다르지만, 일반적으로 **두 단계**로 구성된다.
1. **지도 파인튜닝(SFT)**
- 완성이 아닌 대화를 위해 모델을 최적화하기 위해 고품질 지시 데이터로 사전 학습된 모델을 파인튜닝
- 사람이 만든 정답 예시로 지도 학습
2. **선호도 파인튜닝**
- 사람의 선호도에 맞는 응답을 출력하도록 모델을 더 파인튜닝
- 선호 파인튜닝은 일반적으로 강화 학습으로 수행된다. 
- 기법
    - 사람 피드백 기반 강화학습(RHLF) : GPT-3.5, 라마 2가 사용
    - 직접 선호도 최적화(DPO) : 라마 3이 사용
    - AI 피드백 기반 강화 학습(RLAIF) : 클로드가 사용하는 것으로 보임
    
- **사후 학습**은 사전 학습에 비해 **적은 양의 자원을 소비**하므로 사전 학습된 모델이 이미 가지고 있지만, 단순한 프롬프트로는 제대로 **활용하기 어려운 능력을 끌어내는 과정**
![](https://velog.velcdn.com/images/dkan9634/post/b5265a07-5ccd-4f2a-8bd2-c3681cf481ab/image.png)


## 지도 파인튜닝
- 사전 학습된 모델은 대화보다 완성에 최적화되어 있을 가능성이 높다.
- **시연 데이터(demonstration data)** : 모델이 적절한 응답을 생성하도록 유도하기위해 적절한 응답의 예시도 보여줌. (프롬프트, 응답) 형식
![](https://velog.velcdn.com/images/dkan9634/post/11e5d390-5eff-4a09-8080-3e7588299afd/image.png)

## 선호도 파인튜닝
- 목표 : AI 모델이 사람의 선호도에 따라 행동하도록 만드는 것 (난이도가 높음)
- 선호도 파인튜닝 분야에서 처음으로 성공을 거두고 지금도 널리 쓰이는 알고리즘은 **RLHF**이다.
    - 1) 파운데이션 모델의 출력에 점수를 매기는 보상 모델을 학습한다.
    - 2) 보상 모델이 최대 점수를 줄 응답을 생성하도록 파운데이션 모델을 최적화한다.
- DPO 같은 새로운 접근 방식이 인기를 얻고 있다.(메타는 라마2의 RLHF에서 라마3의 DPO로 전환)

### 보상 모델
- RLHF는 보상 모델을 기반으로 작동한다.
- 보상 모델은 (프롬프트, 응답) 쌍이 주어지면 그 응답이 얼마나 좋은지 점수를 매긴다.
- **포인트와이즈 평가(pointwise evaluation)** : 각 샘플을 독립적으로 평가하는 것

### 보상 모델을 사용한 파인튜닝
- **학습된 보상 모델을 가지고 SFT 모델을 추가로 학습시켜 보상 모델이 높은 점수를 줄 수 있는 응답을 생성하도록 만든다.**
- 이 과정에서 기존 사용자가 입력한 프롬프트 등 다양한 프롬프트 집합에서 무작위로 프롬프트를 선택한다. 이런 프롬프트를 모델에 입력하면 보상 모델이 그 응답의 점수를 매긴다. 이 학습 과정은 주로 오픈 AI가 2017년에 공개한 강화 학습 알고리즘인 PPO를 수행한다.
- SFT와 선호도 파인튜닝은 모두 사전 학습에 사용된 데이터의 품질이 낮아서 생긴 문제를 해결하기 위한 단계이다. (언젠가 더 좋은 사전 학습 데이터나 파운데이션 모델 학습 기법이 개발되면 SFT와 선호도 기반 학습이 아예 필요하지 않을 수도 있다.)
- 일부 기업은 강화 학습을 완전히 건너뛰어도 괜찮다고 생각한다. ex) 스티치 픽스와 그랩은 자사의 활용 사례에서 보상 모델만으로도 충분하다는 것을 확인했다.
- Best of N 전략 : 모델이 생성한 샘플들 중에서 가장 좋은 것을 선택해 성능을 높인다.

---
# 샘플링
모델은 샘플링 과정을 통해 출력을 생성한다.
이 절에서는 온도(temperature), top-k, top-p를 포함한 다양한 샘플링 전략과 변수를 살펴본다. 그 다음 여러 출력을 샘플링해서 모델의 성능을 향상시키는 방법을 알아본다. 또한 특정 형식과 제약 조건을 따르는 응답을 생성하도록 샘플링 과정을 수정하는 방법도 알아볼 예정이다.


## 샘플링의 기초
- 입력이 주어지면 신경망은 잠재적인 결과들의 확률을 먼저 계산해 출력을 생성한다. 분류모델의 잠재적인 출력은 모델이 분류할 수 있는 클래스다.
- 그리디 샘플링 : 가장 높은 확률을 가진 결과를 선택하는 것

- 다음 토큰으로 가장 가능성이 높은 것만 고르는 대신, 모델은 가능한 모든 값에 대한 확률 분포에 따라 다음 토큰을 샘플링할 수 있다.
![](https://velog.velcdn.com/images/dkan9634/post/9503b7cf-0ccf-4dd3-906f-5c8c9d51458b/image.png)
- 모델은 이런 확률을 어떻게 계산할까? 입력이 주어지면 신경망은 로짓 벡터를 출력한다.(언어 모델에선 각 로짓은 모델 어휘집에 있는 하나의 토큰을 나타낸다. 
로짓 벡터 크기 == 어휘집의 크기)
![](https://velog.velcdn.com/images/dkan9634/post/3a48322a-b996-42c9-94bd-bffb3cd0d252/image.png)
더 큰 로짓이 더 높은 확률에 대응하지만, 로짓은 확률을 나타내지 않는다. 즉, 로짓의 합은 1이 되지 않고 음수가 될 수도 있다. 로짓을 확률로 변환하기 위해 **소프트맥스**를 많이 쓴다.

## 샘플링 전략
- 적절한 샘플링 전략을 사용하면 모델이 애플리케이션에 더 적합한 응답을 생성하도록 만들 수 있다.(한 샘플링 전략은 모델이 더 창의적인 응답을 생성하도록, 다른 전략은 더 예측 가능한 응답을 하도록)

자주 쓰이는 샘플링 전략이 어떻게 작동하는지 알아보자
### 온도 
- 가능한 값들의 확률을 재분배하기 위해, 온도를 사용해 샘플링
- **직관적으로, 더 높은 온도는 흔한 토큰의 확률을 줄임으로써, 결과적으로 희귀한 토큰의 확률을 증가시킨다.** -> 모델은 더 창의적인 응답 만들 수 있다.
- 온도: 소프트맥스 변환 전에 로짓을 조정하는 데 사용되는 **상수**
- 주어진 온도 T에 대해, i번째 토큰의 조정된 로짓 = $\frac{x_i}{T}$
- 다양한 온도에서 토큰 A와 B의 소프트맥스 확률을 보여준다. 온도가 0에 가까워질수록, 모델이 토큰 B를 선택할 확률이 1에 가까워진다.
![](https://velog.velcdn.com/images/dkan9634/post/6dcd8fa1-5fe1-4010-8c85-805a8078d71b/image.png)
- 많은 모델 제공업체는 모델이 생성한 확률을 로그프롭(logprob)으로 제공한다. log probability의 줄임말로, 로그 스케일로 표현된 확률이다. 신경망의 확률을 다룰 땐 언더플로 문제를 줄이는데 도움이 되기 때문에 로그 스케일이 선호된다.
![](https://velog.velcdn.com/images/dkan9634/post/16452ce3-43c5-4b03-a05c-e1c7e75410d3/image.png)

### top-k
- 모델의 응답 다양성을 너무 희생하지 않으면서 계산 작업량을 줄이기 위한 샘플링
- 소프트맥스는 모든 후보 값에 대해 연산을 두 번 수행하는데 큰 어휘를 가진 언어 모델의 경우, 이 과정은 계산 비용이 매우 높다.
- 이 문제를 피하기 위해 모델이 **로짓을 계산한 후, 상위 k개의 로짓을 선택하고 이 상위 k개의 로짓에 대해서만 소프트맥스를 수행**한다.
- k값이 작아지면, 모델이 선택할 수 있는 단어의 수가 줄어들기 때문에 텍스트는 더 예측 가능하지만 결과적으로 문장이 지나치게 예측 가능해지고 반복적이며, 창의성과 다양성이 사라질 수 있다.

### top-p
- top-k 샘플링은 고려되는 값의 수를 k로 고정한다. 하지만 이 숫자는 상황에 따라 변해야 한다. (yes or no 질문일 땐 k=2여야 하고 서술형일 땐 더 많아야 한다.)
- **뉴클리어스 샘플링(nucleus sampling)이라고 알려진 top-p는 샘플링할 값을 더 동적으로 선택**한다.
- 가장 가능성이 높은 다음 값의 확률을 내림차순으로 합산하고 p에 도달하면 중단한다. 이렇게 계산된 누적 확률에 포함된 단어들만 다음 단어 후보로 고려된다.
- [그림 2-18]과 같이 모든 토큰의 확률이 있다고 하자. top-p가 90%라면 “yes”와 “maybe”만 고려될 것이다.(누적 확률이 90%보다 크기 때문이다.) top-p가 99%라면 “yes”, “maybe”, “no”가 고려된다.

![](https://velog.velcdn.com/images/dkan9634/post/24c484ef-1197-48d3-9577-d4aa2f605b3f/image.png)
- 하지만 top-k와 달리 top-p는 소프트맥스 계산 부하를 반드시 줄여주지는 않는다. top-p의 이점은 컨텍스트와 관련이 높은 단어들만 후보로 삼기 때문에, 컨텍스트에 적절한 문장을 생성할 수 있다.
- 관련 샘플링 전략 min-p: 샘플링 중 고려 대상이 될 토큰의 최소 확률 설정

### 중단 조건
- 자기회귀 언어 모델은 토큰을 하나씩 생성하면서 토큰 시퀀스를 생성한다.
- 모델이 시퀀스 생성을 중단하는 조건을 설정할 수 있다.(너무 오래 걸리면 화나기 때문)
- 가장 간단한 방법: **모델이 일정 개수의 토큰을 생성한 후에 자동으로 멈추도록 설정하기**
    - 단점: 출력이 문장 중간에 잘릴 가능성이 있다.
- **중단 토큰이나 중단 단어를 사용하기**
    - ex) 시퀀스 종료 토큰을 만나면 생성 중단하도록 모델에 요청 => 지연 시간과 비용을 낮게 유지하는데 도움
    - 단점: 모델이 특정 형식으로 출력을 생성하길 원하는 경우, 조기 중단으로 출력 형식이 잘못될 수 있다. ex) JSON 생성을 요청하면 조기 중단으로 인해 출력 JSON의 닫는 괄호 같은 것이 누락될 수 있다


## 테스트 시점 연산
모델이 전체 출력을 어떻게 샘플링할 수 있는지 논의하자.

- 모델의 응답 품질을 향상시키는 가장 간단한 방법은 테스트 시점 연산을 사용하는 것이다.
- 질의당 하나의 응답만 생성하는 대신, 좋은 응답이 나올 확률을 높이기 위해 여러 응답을 생성한다. 
- **테스트 시점 연산**을 수행하는 한 가지 방법은 Best of N기법
    - 무작위로 여러 출력을 생성하고, 가장 적합한 출력을 선택한다.
    - 단점: 모든 출력을 무작위로 생성하면 가능성이 낮은 후보들이 많이 포함될 수 있다.
    - 대신 **빔 검색**을 사용하면, 시퀀스 생성 단계마다 가장 가능성이 높은 후보(빔)들을 정해진 개수만큼 생성할 수 있다.
- 테스트 시점 연산 효과를 높이는 간단한 전략: 출력의 다양성을 높이는 것
    - WHY? 더 다양한 옵션 집합이 더 나은 후보를 산출할 가능성이 높기 때문
- 동일한 모델을 사용해 다른 옵션을 생성한다면, 출력의 다양성을 높이기 위해 모델의 샘플링 변수를 변경하는 것이 좋은 방법이다.
- 딥마인드는 테스트 시점 연산을 확장 하는 것(추론 중 더 많은 출력을 생성하기 위해 더 많은 계산하는 것)이 모델 파라미터를 확장하는 것보다 더 효율적일 수 있다고 주장한다.

![](https://velog.velcdn.com/images/dkan9634/post/e30bc10c-ef12-4944-88e5-1d1e9e914343/image.png)
> 문제 1개당 답을 여러 개 생성해서 그중 최선을 고르는 방식은 성능을 올려주지만,
400개 이상 생성하면 오히려 성능이 떨어진다.

    

## 구조화된 출력
- 실제 서비스에서는 모델이 특정 형식을 따르는 출력을 생성하는 경우도 있다. 
AI 모델이 특정 형식(정규식, JSON 등) 을 따라 출력해야 하는 상황에서 사용하는 기술들

1) 구조화된 출력이 필요한 경우
자연어 → 기계가 읽을 수 있는 형식으로 변환해야 할 때
예) Text-to-SQL, 정규식 생성, 포맷된 날짜 만들기 등
출력이 반드시 유효한 형식이어야 함(예: 올바른 SQL, 올바른 Regex)

2) 다운스트림 애플리케이션에서 사용되는 경우
다른 시스템에서 바로 쓰기 위한 형식 필요
예) JSON 형태: { "title": "...", "body": "..." }

3) 구조화된 출력 지원 도구
guidance, outlines, instructor, llama.cpp 등
OpenAI API의 JSON 모드 → 형태는 보장하지만 내용까지 보장하진 않음

4) 가이던스를 사용한 제약 생성
미리 선택지 제한: select(['red','blue','green'])
정규식 제한: gen(regex='\d+')
→ 모델이 반드시 해당 형식에 맞춰 출력하도록 강제

테스트 시점 연산은 예상된 형식에 맞는 출력이 나올 때까지 계속 생성하는 방식이다. 이번 절에서는 나머지 네 가지 접근 방식에 초점을 맞춘다.

### 프롬프팅
- 구조화된 출력을 위한 조치
- 모델에게 어떤 형식으로든 출력을 생성하도록 지시할 수 있지만 모델이 이 지시를 따를 수 있는지는 모델의 지시 수행 능력과 지시의 명확성에 달려 있다. 항상 지시를 따른다는 보장은 없다.
- 유효한 출력의 비율을 높이기 위해 일부 사람들은 우너래 프롬프트의 출력을 검증하거나 수정하기 위해 AI를 사용한다. 이는 출력마다 최소 두 번의 모델 질의가 필요하다는 뜻: 
(1) 출력 생성 (2) 검증
- 추가된 검증 단계가 출력의 유효성을 크게 향상시킬 순 있지만, 추가 요청으로 비용과 지연 시간이 늘어난다.

### 후처리
- 간단하고 비용이 적게 들지만, 큰 효과
- 모델은 여러 질의에서 비슷한 실수를 반복하는 경향이 있다..
- 따라서 모델이 자주 하는 실수를 파악하면, 이를 수정하는 스크립트를 작성해 오류를 고칠 수 있다. ex) JSON 객체에 닫는 괄호 빠져있으면 괄호를 직접 추가
- 단, 후처리는 수정하기 쉬운 실수에만 효과가 있다.

### 제약 샘플링(constraint sampling)
- 특정 제약 조건에 맞게 텍스트 생성을 유도하는 기법
- 주로 구조화된 출력 도구와 함께 사용된다.
- 큰 틀에서 보면, 제약 조건을 만족하는 값들 중에서 토큰을 샘플링하는 방식이다. 토큰을 생성할 때 모델은 먼저 로짓 벡터를 출력하는데, 각 로짓은 가능한 토큰 하나와 대응된다.
- 제약 샘플링은 이 로짓 벡터에서 제약 조건을 만족하는 토큰만 걸러내고, 걸러진 토큰들 중 하나를 뽑는다.
![](https://velog.velcdn.com/images/dkan9634/post/1779c0fc-20a4-49ba-aa7f-183d6c069117/image.png)

### 파인튜닝
- 원하는 형식에 맞는 예시로 모델을 학습시키는 것이고 이 형식대로 출력을 생성하게 만드는 가장 효과적이고 일반적인 방법
- 어떤 형식이든 적용해 볼 수는 있지만, 단순 파인튜닝으로는 모델이 항상 기대한 형식을 지켜서 출력한다고 보장할 순 없다. 
- 그래도 프롬프트만 쓰는 것보다는 훨씬 더 믿을 만하다.
- **특성 기반 전이** : 사전학습된 모델(pretrained model)이 이미 추출해 놓은 ‘특성(feature)’을 그대로 가져와서, 그 위에 새로운 분류기(head)만 붙여서 학습하는 전이 방식
![](https://velog.velcdn.com/images/dkan9634/post/4b9396d0-09d7-46a8-93be-8dc5dce4fdaf/image.png)

- 파인튜닝할 땐 모델 전체를 처음~끝 재학습할 수도 있고, 분류기 헤드처럼 일부만 학습할 수 있다.

## AI의 확률적 특성
- AI 모델이 응답을 샘플링하는 방식은 확률적이다. ex) 친구에게 좋아하는 음식을 2번 물어보면 답변이 똑같지만 AI 모델한테 물어보면 응답이 달라질 수 있다. 
- 이런 확률적 특성 때문에 일관성이 떨어지고 환각 현상이 생긴다. 
    - 일관성이 떨어진다는 건 같거나 비슷한 프롬프트에 대해 다른 응답을 내놓는다는 뜻이다. 
    - 환각은 사실에 근거하지 않은 응답을 내놓는 현상이다.
- 확률적 특성 덕분에 AI는 창의적인 작업에서 빛을 발한다.

### 비일관성
- 모델의 **비일관성은 두 가지 상황**에서 나타난다.
1. **같은 입력, 다른 출력**: 모델에게 똑같은 프롬프트를 두 번 주면 다른 응답이 나온다.
2. **살짝 다른 입력, 완전히 다른 출력**: 실수로 글자를 대문자로 쓰는 것처럼 프롬프트를 아주 조금만 바꿔도 매우 다른 출력이 나올 수 있다.
![](https://velog.velcdn.com/images/dkan9634/post/1d1918da-2081-4c2f-af5c-50ef9f987da1/image.png)

### 환각
- 중요한 작업에서 환각은 치명적이다.
- 환각은 LLM의 등장과 함께 주목받기 시작했지만, 사실 파운데이션 모델이나 트랜스포머가 나오기 전부터 생성 모델에서는 흔한 현상이었다.
    - 텍스트 생성 컨텍스트의 환각은 이미 2016년에 언급
- 비일관성이 샘플링 과정의 무작위성에서 발생한다면, 환각의 원인은 미묘하다. 샘플링 과정만으로는 충분히 설명되지 않는다.
- 현재 언어 모델이 환각을 일으키는 이유에 대한 두 가지 가설
1. 2021년 딥마인드의 오르테가 처음 제시
- 언어 모델이 주어진 데이터와 자신이 생성한 데이터를 구분하지 못해서 일어난다.
- 일종의 자기 기만(self-delusion)
![](https://velog.velcdn.com/images/dkan9634/post/04e82c9d-9a61-4c95-b61b-381ffd992281/image.png)
![](https://velog.velcdn.com/images/dkan9634/post/4f06d5e3-95c0-4f1e-bf01-4176fedb41fb/image.png)
- 딥마인드 논문에서 환각을 줄일 수 있는 두 가지 방법 제시
    - 1) 강화학습을 사용해서, 모델이 사용자가 제공한 프롬프트와 모델이 생성한 토큰을 구분하도록 만듦
    - 2) 지도 학습을 활용하는 것으로, 학습 데이터에 사실과 반사실적 데이터를 포함시킴
2.  오픈AI 연구원인 레오 가오가 처음 주장
- 모델의 내부 지식과 레이블러의 내부 지식이 일치하지 않아서 환각이 일어난다는 것이다.
- 지도 파인 튜닝(SFT) 과정에서 모델은 레이블러가 작성한 응답을 모방하도록 학습된다. 만약, 이런 응답들이 레이블러는 가지고 있지만 모델은 가지고 있지 않은 지식을 사용한다면, 우리는 사실상 모델에게 환각을 일으키도록 학습시키고 있는 것이다.

오픈AI 공동 창립자인 존 슐만이 제시한 두 가지 해결책

1. 검증
- 각 응답에 대해 모델에게 그 응답의 근거가 되는 출처를 검색 요청
2. 강화 학습을 사용
- 보상 모델은 A가 더 나은 이유에 대한 설명 없이 오직 비교만을 사용해 학습
- 모델이 환각 현상을 보일 때 더 큰 불이익을 주는 방식

---
https://velog.io/@dkan9634/AI-Engineering-Chap-2.-%ED%8C%8C%EC%9A%B4%EB%8D%B0%EC%9D%B4%EC%85%98-%EB%AA%A8%EB%8D%B8-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0
